{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium[atari] --quiet\n!pip install gymnasium --quiet\n!pip install -U gymnasium[atari] --quiet\n!pip install imageio_ffmpeg --quiet\n!pip install npy_append_array --quiet\n!pip install pyTelegramBotAPI --quiet\n!pip install gymnasium[accept-rom-license] --quiet\n!pip install gymnasium[box2d] --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-26T10:49:41.685275Z","iopub.execute_input":"2023-03-26T10:49:41.685818Z","iopub.status.idle":"2023-03-26T10:51:22.287625Z","shell.execute_reply.started":"2023-03-26T10:49:41.685787Z","shell.execute_reply":"2023-03-26T10:51:22.286315Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n\nclass ExperienceReplayBuffer: \n    def __init__(self, max_memory, input_shape, batch_size, n_actions, cer=False): \n        self.mem_size = max_memory\n        self.mem_counter = 0\n        self.state_memory = np.zeros((self.mem_size, *input_shape),\n                                     dtype=np.float32)\n        self.next_state_memory = np.zeros((self.mem_size, *input_shape),\n                                         dtype=np.float32)\n\n        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=np.float32)\n        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n        self.batch_size = batch_size\n        self.cer = cer\n\n    def store_experience(self, state, action, reward, next_state, done): \n        index = self.mem_counter % self.mem_size \n\n        self.state_memory[index] = state\n        self.next_state_memory[index] = next_state\n        self.reward_memory[index] = reward\n        self.action_memory[index] = action\n        self.terminal_memory[index] = done\n        self.mem_counter += 1\n\n    def sample_experience(self, batch_size):\n        # used to get the last transition\n        offset = 1 if self.cer else 0\n\n        max_mem = min(self.mem_counter, self.mem_size) - offset\n        batch_index = np.random.choice(max_mem, batch_size - offset, replace=False)\n\n        states = self.state_memory[batch_index]\n        next_states = self.next_state_memory[batch_index]\n        rewards = self.reward_memory[batch_index]\n        actions = self.action_memory[batch_index]\n        terminals = self.terminal_memory[batch_index]\n\n        if self.cer: \n            last_index = self.mem_counter % self.mem_size - 1\n            last_state = self.state_memory[last_index]\n            last_action = self.action_memory[last_index]\n            last_terminal = self.terminal_memory[last_index]\n            last_next_state = self.next_state_memory[last_index]\n            last_reward = self.reward_memory[last_index]\n\n            # for 2d and 3d use vstack to append, for 1d array use append() to append the data\n            states = np.vstack((self.state_memory[batch_index], last_state))\n            next_states = np.vstack((self.next_state_memory[batch_index], last_next_state))\n\n            actions = np.append(actions, last_action)\n            terminals = np.append(terminals, last_terminal)\n            rewards = np.append(rewards, last_reward)\n    \n        return states, actions, rewards, next_states, terminals\n    \n    \n    def is_sufficient(self): \n        return self.mem_counter > self.batch_size\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:22.290261Z","iopub.execute_input":"2023-03-26T10:51:22.290573Z","iopub.status.idle":"2023-03-26T10:51:22.307612Z","shell.execute_reply.started":"2023-03-26T10:51:22.290541Z","shell.execute_reply":"2023-03-26T10:51:22.306443Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras.layers import Dense, Conv2D, Input, Lambda, concatenate\n \nclass ActorNetwork(tf.keras.Model):\n    def __init__(self, input_dims, action_bound, action_dims, name):\n        super(ActorNetwork, self).__init__()\n        self.model_name = name\n        self.fc1 = Dense(64, activation=\"relu\", input_shape=input_dims, kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.fc3 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        \n        self.out = Dense(action_dims, activation='tanh')\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.out(x)\n        return x \n\n\nclass CriticNetwork(tf.keras.Model):\n    def __init__(self, input_dims, action_dims, name): \n        super(CriticNetwork, self).__init__()\n        self.model_name = name\n        self.fc1 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.fc3 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.out = Dense(1, activation='linear')\n\n    def call(self, state, action):\n        x = self.fc1(tf.concat([state, action], axis=1))\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.out(x)\n        return x ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:22.309431Z","iopub.execute_input":"2023-03-26T10:51:22.309818Z","iopub.status.idle":"2023-03-26T10:51:22.325701Z","shell.execute_reply.started":"2023-03-26T10:51:22.309781Z","shell.execute_reply":"2023-03-26T10:51:22.324680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nimport tensorflow as tf \nimport tensorflow_probability as tfp\nimport numpy as np\n\nclass DDPGAgent:\n  \n    def __init__(self, input_dims, n_actions, gamma, alpha, beta, \n                                batch_size, mem_size, soft_update, \n                                tau, min_action, max_action, noise): \n        self.gamma = gamma \n        self.noise = noise\n        self.n_actions = n_actions\n        self.soft_update = soft_update\n        self.tau = tau\n        self.fname = \"models/ddpg/\"\n        self.min_action = min_action\n        self.max_action = max_action\n        self.batch_size = batch_size\n\n        self.memory = ExperienceReplayBuffer(mem_size, input_dims, batch_size, n_actions, cer=False)\n        self.actor = ActorNetwork(input_dims, max_action, n_actions, \"actor\")\n        self.target_actor = ActorNetwork(input_dims, max_action, n_actions, \"target_actor\")\n        self.critic = CriticNetwork(input_dims, 1, \"critic\")\n        self.target_critic = CriticNetwork(input_dims, 1, \"target_critic_\")\n        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n        self.critic.compile(optimizer=Adam(learning_rate=beta))\n        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n        self.bg_noise = np.zeros(n_actions)\n\n        self.update_target_networks()\n        \n    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n        return x + rho * (mu-x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n        \n    def get_action(self, state, evaluate): \n        # adding noise, makes us to do the exploration,\n        state = tf.convert_to_tensor([state], dtype=tf.float32)\n        actions = self.actor(state)\n        noise = self.ou_noise(self.bg_noise, dim=self.n_actions) \n        if not evaluate:\n            actions = actions + noise\n\n        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n        self.bg_noise = noise\n        return actions[0]\n\n    def store_experience(self, state, action, reward, state_, done):\n        self.memory.store_experience(state, action, reward, state_, done)\n\n    def sample_experience(self):\n        state, action, reward, new_state, done = \\\n                                  self.memory.sample_experience(self.batch_size)\n        states = tf.convert_to_tensor(state)\n        rewards = tf.convert_to_tensor(reward)\n        dones = tf.convert_to_tensor(done)\n        actions = tf.convert_to_tensor(action)\n        states_ = tf.convert_to_tensor(new_state)\n        return states, actions, rewards, states_, dones\n \n    def save_models(self):\n        self.actor.save(self.fname + \"ddpg_actor_network\")\n        self.target_actor.save(self.fname + \"ddpg_target_actor_network\")\n        self.critic.save(self.fname  + \"ddpg_critic_network\")\n        self.target_critic.save(self.fname  + \"ddpg_target_critic_network\")\n        print(\"[+] Saving the models\") \n\n    def load_models(self):\n        self.actor = tf.keras.models.load_model(self.fname + \"_\" + \"ddpg_actor_network\") \n        self.target_actor = tf.keras.models.load_model(self.fname + \"_\" + \"ddpg_target-actor_network\") \n        self.critic = tf.keras.models.load_model(self.fname + \"_\" + \"ddpg_critic_network\") \n        self.target_critic = tf.keras.models.load_model(self.fname + \"_\" + \"ddpg_target_critic_network\") \n        print(\"[+] Loading the models\")\n  \n    def learn(self): \n        if not self.memory.is_sufficient():\n            return\n        states, actions, rewards, next_states, dones = self.sample_experience()\n        states = tf.convert_to_tensor(states, dtype=tf.float32)\n        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n        rewards = tf.convert_to_tensor(rewards, tf.float32)\n        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n        \n        with tf.GradientTape() as tape:\n            target_actions = self.target_actor(next_states)\n            one_step_lookahead_vals = tf.squeeze(self.target_critic(next_states, target_actions), 1)\n            q_vals = tf.squeeze(self.critic(states, actions), 1)\n            target_q_vals = rewards + self.gamma * one_step_lookahead_vals * ([1 - int(d) for d in dones])\n            critic_loss = self.critic_loss(q_vals, target_q_vals)\n      \n        critic_params = self.critic.trainable_variables\n        critic_grads = tape.gradient(critic_loss, critic_params)\n        self.critic.optimizer.apply_gradients(zip(critic_grads, critic_params))\n\n        with tf.GradientTape() as tape:\n            pred_actions = self.actor(states)\n            q_vals = -self.critic(states, pred_actions)\n            actor_loss = self.actor_loss(q_vals)\n\n        actor_params = self.actor.trainable_variables\n        actor_grads = tape.gradient(actor_loss, actor_params)\n        self.actor.optimizer.apply_gradients(zip(actor_grads, actor_params))\n\n        self.update_target_networks()\n    \n    def critic_loss(self, q_vals, target_q_vals): \n        loss = tf.keras.losses.MSE(q_vals, target_q_vals)\n        return loss\n    \n    def actor_loss(self, q_vals):\n        return tf.math.reduce_mean(q_vals)\n\n    def update_target_networks(self):\n        actor_weights = self.actor.get_weights()\n        t_actor_weights = self.target_actor.get_weights()\n        critic_weights = self.critic.get_weights()\n        t_critic_weights = self.target_critic.get_weights()\n        if self.soft_update: \n            for i in range(len(actor_weights)):\n                t_actor_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * t_actor_weights[i]\n\n            for i in range(len(critic_weights)):\n                t_critic_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * t_critic_weights[i]\n\n            self.target_actor.set_weights(t_actor_weights)\n            self.target_critic.set_weights(t_critic_weights)\n            \n        else: \n            self.target_actor.set_weights(t_actor_weights)\n            self.target_critic.set_weights(t_critic_weights)\n  ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:22.328756Z","iopub.execute_input":"2023-03-26T10:51:22.329142Z","iopub.status.idle":"2023-03-26T10:51:22.356770Z","shell.execute_reply.started":"2023-03-26T10:51:22.329106Z","shell.execute_reply":"2023-03-26T10:51:22.355641Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom telebot import TeleBot\nimport datetime\nimport telebot\n\ntoken = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\nchat_id = \"1055055706\"\nbot = TeleBot(token=token) \n\ndef telegram_send(message, bot):\n    chat_id = \"1055055706\"\n    bot.send_message(chat_id=chat_id, text=message)\n\ndef welcome_msg(multi_step, double_dqn, dueling):\n    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n    telegram_send(st, bot)\n    \ndef info_msg(episode, max_episode, reward, best_score, loss): \n    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n    telegram_send(st, bot)\n\ndef end_msg(learning_time):\n    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n    telegram_send(st, bot)\n    print(st)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:22.358500Z","iopub.execute_input":"2023-03-26T10:51:22.358948Z","iopub.status.idle":"2023-03-26T10:51:22.389344Z","shell.execute_reply.started":"2023-03-26T10:51:22.358909Z","shell.execute_reply":"2023-03-26T10:51:22.388411Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gymnasium as gym\nimport tensorflow as tf\nfrom gymnasium.wrappers import *\n\n\ndef manage_memory():\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\n\ndef plot_learning_curve(x, scores, figure_file):\n    running_avg = np.zeros(len(scores))\n    for i in range(len(running_avg)):\n        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n    plt.plot(x, running_avg)\n    plt.title('Running average of previous 100 scores')\n    plt.savefig(figure_file)\n\n\ndef make_env(env_name, video_file_name, episode_freq_fo_video): \n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    \n    if len(env.observation_space.shape) >= 3: \n        #env = AtariPreprocessing(env, 10, 4, 84, False, True)\n        env = ResizeObservation(env, 84)\n        env = GrayScaleObservation(env, keep_dim=False)\n        env = FrameStack(env, 4, lz4_compress=False)\n        env = NormalizeObservation(env)\n\n    return env","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:22.390607Z","iopub.execute_input":"2023-03-26T10:51:22.391139Z","iopub.status.idle":"2023-03-26T10:51:23.937719Z","shell.execute_reply.started":"2023-03-26T10:51:22.391105Z","shell.execute_reply":"2023-03-26T10:51:23.936671Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Writer:\n    def __init__(self, fname): \n        self.fname = fname \n\n    def write_to_file(self, content): \n        with open(self.fname, \"a\") as file: \n            file.write(content + \"\\n\")\n\n    def read_file(self, fname):\n        with open(fname, \"r\") as file: \n            return file.read()\n            ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T10:51:23.939086Z","iopub.execute_input":"2023-03-26T10:51:23.939560Z","iopub.status.idle":"2023-03-26T10:51:23.947334Z","shell.execute_reply.started":"2023-03-26T10:51:23.939523Z","shell.execute_reply":"2023-03-26T10:51:23.945376Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport imageio\n\n\nclass RecordVideo: \n    \n    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n        self.prefix_fname = prefix_fname\n        self.out_directory = out_directory\n        self.fps = fps\n        self.images = []\n        \n    def add_image(self, image): \n        self.images.append(image)\n    \n    def save(self, episode_no): \n        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n        self.images = []","metadata":{"execution":{"iopub.status.busy":"2023-03-26T12:49:23.277411Z","iopub.execute_input":"2023-03-26T12:49:23.278377Z","iopub.status.idle":"2023-03-26T12:49:23.286757Z","shell.execute_reply.started":"2023-03-26T12:49:23.278324Z","shell.execute_reply":"2023-03-26T12:49:23.285551Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from npy_append_array import NpyAppendArray\nimport numpy as np\n\nclass Trainer:   \n    def __init__(self, env, gamma, alpha, beta, batch_size, \n                                     tau, sot_update, noe, max_steps, \n                                     is_tg, tg_bot_freq_epi, record, \n                                     mem_size, noise): \n       \n        self.env = env \n        self.target_score = 80\n        self.noe = noe\n        self.max_steps = max_steps\n        self.is_tg = is_tg\n        self.tg_bot_freq_epi = tg_bot_freq_epi\n        self.record = record \n        self.writer = Writer(\"model_training_results.txt\")\n        self.recorder = RecordVideo(\"ddpg\", \"videos/\", 20)\n        self.agent = DDPGAgent(env.observation_space.shape, \n                               env.action_space.shape[0], gamma, alpha,\n                               beta, batch_size, mem_size,\n                               soft_update, tau, env.action_space.low[0],\n                               env.action_space.high[0], noise\n                            )\n\n    def train_rl_model(self): \n        avg_rewards = []\n        best_reward = float(\"-inf\")\n        episode_rewards = []\n        for episode in range(self.noe): \n            n_steps = 0 \n            state = self.env.reset()\n            reward = 0 \n            \n            if record and episode % 50 == 0:\n                img = self.env.render()\n                self.recorder.add_image(img)\n\n            for step in range(self.max_steps): \n\n                if type(state) == tuple: \n                    state = state[0]\n                state = state\n\n                action = self.agent.get_action(state, evaluate=False)\n\n                next_info = self.env.step(action)\n                next_state, reward_prob, terminated, truncated, _ = next_info\n                done = truncated or terminated\n                reward += reward_prob\n\n                self.agent.store_experience(state, action, reward_prob, next_state, done)\n                self.agent.learn()\n\n                state = next_state\n                n_steps += 1   \n                \n                # record\n                if record and episode % 50 == 0:\n                    img = self.env.render()\n                    self.recorder.add_image(img)\n                \n                if done: \n                    break\n            \n            episode_rewards.append(reward)\n            avg_reward = np.mean(episode_rewards[-100:])\n            avg_rewards.append(avg_reward)\n\n            result = f\"Episode: {episode}, Steps: {n_steps}, Reward: {reward}, Best reward: {best_reward}, Avg reward: {avg_reward}\"\n            self.writer.write_to_file(result)\n            print(result)\n            \n            # Recording.\n            if record and episode % 50 == 0:\n                self.recorder.save(episode)\n                \n            # Saving Best Model\n            if reward > best_reward: \n                best_reward = reward\n                self.agent.save_models()\n                \n            # Telegram bot\n            if self.is_tg and episode % self.tg_bot_freq_epi == 0: \n                info_msg(episode+1, self.noe, reward, best_reward, \"d\")\n                \n            # Eatly Stopping\n            if episode > 100 and np.mean(episode_rewards[-20:]) >= self.target_score: \n                break\n                \n        return episode_rewards, avg_rewards, best_reward\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T12:51:18.443093Z","iopub.execute_input":"2023-03-26T12:51:18.443562Z","iopub.status.idle":"2023-03-26T12:51:18.460439Z","shell.execute_reply.started":"2023-03-26T12:51:18.443524Z","shell.execute_reply":"2023-03-26T12:51:18.458795Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nimport time\nimport signal\nimport time\nimport sys\nimport pickle\nimport os \n\nenv = make_env(\"MountainCarContinuous-v0\", \"videos/\", 50)\nrecord = False\ngamma = 0.9\nalpha = 0.001\nbeta = 0.002\nbatch_size = 64\ntau = 0.01 \nsoft_update = True \nnoe = 500\nmax_steps = 100000\nis_tg = True \ntg_bot_freq_epi = 20\nrecord = True \nmem_size = 5000000\nnoise = 0.5\n    \n    \nif not os.path.exists(\"videos\"): \n    os.mkdir(\"videos\")\n\nif not os.path.exists(\"test_videos\"):\n    os.mkdir(\"test_videos\")\n\n\nif __name__ == \"__main__\": \n    \n    try: \n        manage_memory()\n        trainer = Trainer(env, gamma, alpha, beta, batch_size, tau,\n                          soft_update, noe, max_steps, is_tg,\n                          tg_bot_freq_epi, record, mem_size, noise\n                )\n        episode_rewards, avg_rewards, best_reward = trainer.train_rl_model()\n        \n        with open(\"ddpg_episode_rewards.obj\", \"wb\") as f: \n            pickle.dump(episode_rewards, f)\n        \n        with open(\"ddpg_avg_rewards.obj\", \"wb\") as f: \n            pickle.dump(avg_rewards, f)\n        \n        x = [i+1 for i in range(len(episode_rewards))]\n        plot_learning_curve(x, episode_rewards, \"ddpg_con_mountain_car\")\n\n       # model_path = \"models/lunarlander_DQN_q_value/\"\n\n        #evaluator = Eval(env, action_space, model_path, \"vanilla_dqn_lunarlander\", 10)\n        #evaluator.test()\n        \n    except Exception as error: \n        raise error\n        \n   # eval_model(env, \"keras model\", \"videos/\", fps=10)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T12:54:00.647056Z","iopub.execute_input":"2023-03-26T12:54:00.647452Z","iopub.status.idle":"2023-03-26T13:44:38.355937Z","shell.execute_reply.started":"2023-03-26T12:54:00.647415Z","shell.execute_reply":"2023-03-26T13:44:38.353422Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Episode: 0, Steps: 999, Reward: -12.125138246653938, Best reward: -inf, Avg reward: -12.125138246653938\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x5cfc600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"[+] Saving the models\nEpisode: 1, Steps: 550, Reward: 93.66087795283859, Best reward: -12.125138246653938, Avg reward: 40.76786985309233\n[+] Saving the models\nEpisode: 2, Steps: 770, Reward: 46.61673010312367, Best reward: 93.66087795283859, Avg reward: 42.71748993643612\nEpisode: 3, Steps: 832, Reward: 59.94406130045898, Best reward: 93.66087795283859, Avg reward: 47.024132777441835\nEpisode: 4, Steps: 841, Reward: 86.55764883156344, Best reward: 93.66087795283859, Avg reward: 54.93083598826615\nEpisode: 5, Steps: 999, Reward: -17.24570222451307, Best reward: 93.66087795283859, Avg reward: 42.90141295280295\nEpisode: 6, Steps: 999, Reward: -26.837695696351382, Best reward: 93.66087795283859, Avg reward: 32.9386831457809\nEpisode: 7, Steps: 784, Reward: 88.91155665519042, Best reward: 93.66087795283859, Avg reward: 39.93529233445709\nEpisode: 8, Steps: 496, Reward: 90.31761411655832, Best reward: 93.66087795283859, Avg reward: 45.53332808802389\nEpisode: 9, Steps: 999, Reward: -19.050727085467763, Best reward: 93.66087795283859, Avg reward: 39.07492257067473\nEpisode: 10, Steps: 810, Reward: 88.34584192045881, Best reward: 93.66087795283859, Avg reward: 43.55409705701873\nEpisode: 11, Steps: 794, Reward: 87.89775252288081, Best reward: 93.66087795283859, Avg reward: 47.2494016791739\nEpisode: 12, Steps: 838, Reward: 75.18325015881543, Best reward: 93.66087795283859, Avg reward: 49.39815925453094\nEpisode: 13, Steps: 568, Reward: 86.51156888810159, Best reward: 93.66087795283859, Avg reward: 52.04911708550027\nEpisode: 14, Steps: 566, Reward: 83.59043988687564, Best reward: 93.66087795283859, Avg reward: 54.1518719389253\nEpisode: 15, Steps: 304, Reward: 93.51269561203064, Best reward: 93.66087795283859, Avg reward: 56.611923418494385\nEpisode: 16, Steps: 196, Reward: 95.87919906302025, Best reward: 93.66087795283859, Avg reward: 58.92176316229002\n[+] Saving the models\nEpisode: 17, Steps: 497, Reward: 93.74044854451556, Best reward: 95.87919906302025, Avg reward: 60.856134572413666\nEpisode: 18, Steps: 285, Reward: 91.46174999082054, Best reward: 95.87919906302025, Avg reward: 62.46695643654035\nEpisode: 19, Steps: 193, Reward: 93.96764551758046, Best reward: 95.87919906302025, Avg reward: 64.04199089059236\nEpisode: 20, Steps: 136, Reward: 89.8527819939183, Best reward: 95.87919906302025, Avg reward: 65.27107618122693\nEpisode: 21, Steps: 456, Reward: 79.46014899274974, Best reward: 95.87919906302025, Avg reward: 65.91603403629614\nEpisode: 22, Steps: 128, Reward: 95.88142071964938, Best reward: 95.87919906302025, Avg reward: 67.21887693557237\n[+] Saving the models\nEpisode: 23, Steps: 221, Reward: 94.96793298858807, Best reward: 95.88142071964938, Avg reward: 68.37508760444803\nEpisode: 24, Steps: 237, Reward: 96.37481976821519, Best reward: 95.88142071964938, Avg reward: 69.49507689099872\n[+] Saving the models\nEpisode: 25, Steps: 162, Reward: 94.57340342936041, Best reward: 96.37481976821519, Avg reward: 70.45962791170494\nEpisode: 26, Steps: 104, Reward: 95.61917838152215, Best reward: 96.37481976821519, Avg reward: 71.39146311429076\nEpisode: 27, Steps: 127, Reward: 94.85767036707821, Best reward: 96.37481976821519, Avg reward: 72.22954194474745\nEpisode: 28, Steps: 103, Reward: 95.25935067217, Best reward: 96.37481976821519, Avg reward: 73.02367328017581\nEpisode: 29, Steps: 231, Reward: 90.33943418735123, Best reward: 96.37481976821519, Avg reward: 73.600865310415\nEpisode: 30, Steps: 183, Reward: 96.47370473475448, Best reward: 96.37481976821519, Avg reward: 74.33869884023241\n[+] Saving the models\nEpisode: 31, Steps: 161, Reward: 94.87301888755957, Best reward: 96.47370473475448, Avg reward: 74.98039634171136\nEpisode: 32, Steps: 151, Reward: 95.65349322604816, Best reward: 96.47370473475448, Avg reward: 75.6068538230549\nEpisode: 33, Steps: 272, Reward: 95.06872830063504, Best reward: 96.47370473475448, Avg reward: 76.17926189592491\nEpisode: 34, Steps: 83, Reward: 95.66800108826641, Best reward: 96.47370473475448, Avg reward: 76.73608301570609\nEpisode: 35, Steps: 109, Reward: 95.15804519207789, Best reward: 96.47370473475448, Avg reward: 77.24780418727197\nEpisode: 36, Steps: 136, Reward: 90.35404800593474, Best reward: 96.47370473475448, Avg reward: 77.60202699318177\nEpisode: 37, Steps: 137, Reward: 95.34214631549743, Best reward: 96.47370473475448, Avg reward: 78.06887223850586\nEpisode: 38, Steps: 151, Reward: 95.54712768459602, Best reward: 96.47370473475448, Avg reward: 78.51703263455946\nEpisode: 39, Steps: 135, Reward: 94.87861104779708, Best reward: 96.47370473475448, Avg reward: 78.92607209489042\nEpisode: 40, Steps: 175, Reward: 95.39124962922331, Best reward: 96.47370473475448, Avg reward: 79.32766179084975\nEpisode: 41, Steps: 140, Reward: 94.32382685098811, Best reward: 96.47370473475448, Avg reward: 79.68471333990067\nEpisode: 42, Steps: 168, Reward: 94.00233846597793, Best reward: 96.47370473475448, Avg reward: 80.01768136608851\nEpisode: 43, Steps: 165, Reward: 94.6491009458723, Best reward: 96.47370473475448, Avg reward: 80.35021362926541\nEpisode: 44, Steps: 69, Reward: 94.65097599071119, Best reward: 96.47370473475448, Avg reward: 80.66800834840865\nEpisode: 45, Steps: 138, Reward: 94.97322973921364, Best reward: 96.47370473475448, Avg reward: 80.97899142212181\nEpisode: 46, Steps: 197, Reward: 95.49704908839082, Best reward: 96.47370473475448, Avg reward: 81.28788626608498\nEpisode: 47, Steps: 183, Reward: 94.46361148730526, Best reward: 96.47370473475448, Avg reward: 81.56238054152706\nEpisode: 48, Steps: 198, Reward: 93.69577442657678, Best reward: 96.47370473475448, Avg reward: 81.81000082489543\nEpisode: 49, Steps: 305, Reward: 93.30701761484862, Best reward: 96.47370473475448, Avg reward: 82.03994116069448\nEpisode: 50, Steps: 180, Reward: 94.055362142511, Best reward: 96.47370473475448, Avg reward: 82.27553765053402\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x72b1600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 51, Steps: 173, Reward: 95.6393746658143, Best reward: 96.47370473475448, Avg reward: 82.53253451621248\nEpisode: 52, Steps: 217, Reward: 91.73833795227033, Best reward: 96.47370473475448, Avg reward: 82.70622892066642\nEpisode: 53, Steps: 108, Reward: 95.8160809266881, Best reward: 96.47370473475448, Avg reward: 82.94900395781497\nEpisode: 54, Steps: 135, Reward: 95.17415824418387, Best reward: 96.47370473475448, Avg reward: 83.1712794902944\nEpisode: 55, Steps: 241, Reward: 91.41021270791163, Best reward: 96.47370473475448, Avg reward: 83.31840329775186\nEpisode: 56, Steps: 225, Reward: 90.6412008321305, Best reward: 96.47370473475448, Avg reward: 83.44687342993394\nEpisode: 57, Steps: 159, Reward: 91.73118929569323, Best reward: 96.47370473475448, Avg reward: 83.5897064621022\nEpisode: 58, Steps: 180, Reward: 92.20595827414212, Best reward: 96.47370473475448, Avg reward: 83.73574462840796\nEpisode: 59, Steps: 144, Reward: 92.49753754852382, Best reward: 96.47370473475448, Avg reward: 83.8817745104099\nEpisode: 60, Steps: 261, Reward: 94.65253011013877, Best reward: 96.47370473475448, Avg reward: 84.05834427433989\nEpisode: 61, Steps: 326, Reward: 94.15198926598443, Best reward: 96.47370473475448, Avg reward: 84.22114500001156\nEpisode: 62, Steps: 341, Reward: 93.36256066797898, Best reward: 96.47370473475448, Avg reward: 84.36624683601104\nEpisode: 63, Steps: 155, Reward: 95.0628498941971, Best reward: 96.47370473475448, Avg reward: 84.5333812587952\nEpisode: 64, Steps: 239, Reward: 88.39610332042857, Best reward: 96.47370473475448, Avg reward: 84.5928077520511\nEpisode: 65, Steps: 121, Reward: 92.40321039364393, Best reward: 96.47370473475448, Avg reward: 84.71114718601463\nEpisode: 66, Steps: 547, Reward: 89.43309299003973, Best reward: 96.47370473475448, Avg reward: 84.78162398905978\nEpisode: 67, Steps: 409, Reward: 90.81946153102689, Best reward: 96.47370473475448, Avg reward: 84.87041571761813\nEpisode: 68, Steps: 307, Reward: 88.61349401070628, Best reward: 96.47370473475448, Avg reward: 84.92466322911214\nEpisode: 69, Steps: 141, Reward: 95.19207300857288, Best reward: 96.47370473475448, Avg reward: 85.07134051167587\nEpisode: 70, Steps: 393, Reward: 92.42631577895783, Best reward: 96.47370473475448, Avg reward: 85.17493171262352\nEpisode: 71, Steps: 374, Reward: 87.97607190249182, Best reward: 96.47370473475448, Avg reward: 85.2138364374828\nEpisode: 72, Steps: 274, Reward: 93.18652320369337, Best reward: 96.47370473475448, Avg reward: 85.32305132469116\nEpisode: 73, Steps: 197, Reward: 94.6113952711092, Best reward: 96.47370473475448, Avg reward: 85.44856948612924\nEpisode: 74, Steps: 283, Reward: 93.4438847491229, Best reward: 96.47370473475448, Avg reward: 85.55517368963582\nEpisode: 75, Steps: 292, Reward: 94.36559158073634, Best reward: 96.47370473475448, Avg reward: 85.67110024083452\nEpisode: 76, Steps: 474, Reward: 89.10509408058462, Best reward: 96.47370473475448, Avg reward: 85.71569756342868\nEpisode: 77, Steps: 145, Reward: 94.16384759593997, Best reward: 96.47370473475448, Avg reward: 85.8240071792301\nEpisode: 78, Steps: 306, Reward: 89.90827879972149, Best reward: 96.47370473475448, Avg reward: 85.87570681999581\nEpisode: 79, Steps: 337, Reward: 95.43375200026522, Best reward: 96.47370473475448, Avg reward: 85.99518238474917\nEpisode: 80, Steps: 490, Reward: 89.84721531922045, Best reward: 96.47370473475448, Avg reward: 86.04273834690314\nEpisode: 81, Steps: 156, Reward: 95.30558935298393, Best reward: 96.47370473475448, Avg reward: 86.15569994453827\nEpisode: 82, Steps: 248, Reward: 93.18591708753532, Best reward: 96.47370473475448, Avg reward: 86.24040135589968\nEpisode: 83, Steps: 134, Reward: 94.70172981109424, Best reward: 96.47370473475448, Avg reward: 86.34113145655675\nEpisode: 84, Steps: 342, Reward: 95.05929219290829, Best reward: 96.47370473475448, Avg reward: 86.44369805345501\nEpisode: 85, Steps: 230, Reward: 95.49559244734425, Best reward: 96.47370473475448, Avg reward: 86.54895263943045\nEpisode: 86, Steps: 449, Reward: 93.61629006565468, Best reward: 96.47370473475448, Avg reward: 86.63018640295027\nEpisode: 87, Steps: 357, Reward: 84.80788452993119, Best reward: 96.47370473475448, Avg reward: 86.60947842712052\nEpisode: 88, Steps: 217, Reward: 93.99928921132334, Best reward: 96.47370473475448, Avg reward: 86.6925100089655\nEpisode: 89, Steps: 147, Reward: 95.67963944707849, Best reward: 96.47370473475448, Avg reward: 86.79236700272232\nEpisode: 90, Steps: 486, Reward: 91.98589959689436, Best reward: 96.47370473475448, Avg reward: 86.84943878947145\nEpisode: 91, Steps: 359, Reward: 92.44587915917363, Best reward: 96.47370473475448, Avg reward: 86.91026966305517\nEpisode: 92, Steps: 350, Reward: 95.6974599626362, Best reward: 96.47370473475448, Avg reward: 87.00475558025497\nEpisode: 93, Steps: 144, Reward: 95.85554942308207, Best reward: 96.47370473475448, Avg reward: 87.09891296156164\nEpisode: 94, Steps: 127, Reward: 93.92349936399376, Best reward: 96.47370473475448, Avg reward: 87.17075071316619\nEpisode: 95, Steps: 253, Reward: 94.05754302134433, Best reward: 96.47370473475448, Avg reward: 87.24248813304304\nEpisode: 96, Steps: 396, Reward: 91.93864316904742, Best reward: 96.47370473475448, Avg reward: 87.29090210248638\nEpisode: 97, Steps: 222, Reward: 92.1816230686159, Best reward: 96.47370473475448, Avg reward: 87.3408074184673\nEpisode: 98, Steps: 216, Reward: 94.78442096660063, Best reward: 96.47370473475448, Avg reward: 87.41599543410501\nEpisode: 99, Steps: 218, Reward: 94.71895485493893, Best reward: 96.47370473475448, Avg reward: 87.48902502831335\nEpisode: 100, Steps: 155, Reward: 95.06711745474156, Best reward: 96.47370473475448, Avg reward: 88.56094758532733\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x6d3e600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 101, Steps: 209, Reward: 91.419612068983, Best reward: 96.47370473475448, Avg reward: 88.53853492648875\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/3973858724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m    \u001b[0;31m# eval_model(env, \"keras model\", \"videos/\", fps=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/3973858724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ddpg_con_mountain_car\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m        \u001b[0;31m# model_path = \"models/lunarlander_DQN_q_value/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/2141901049.py\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(x, scores, figure_file)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrunning_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running average of previous 100 scores'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m     return gca().plot(\n\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2769\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (500,) and (102,)"],"ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (500,) and (102,)","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}