{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium[atari] --quiet\n!pip install gymnasium --quiet\n!pip install -U gymnasium[atari] --quiet\n!pip install imageio_ffmpeg --quiet\n!pip install npy_append_array --quiet\n!pip install pyTelegramBotAPI --quiet\n!pip install gymnasium[accept-rom-license] --quiet\n!pip install gymnasium[box2d] --quiet\n!pip install  mujoco-py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T08:10:43.646087Z","iopub.execute_input":"2023-04-17T08:10:43.646438Z","iopub.status.idle":"2023-04-17T08:12:39.844215Z","shell.execute_reply.started":"2023-04-17T08:10:43.646396Z","shell.execute_reply":"2023-04-17T08:12:39.842953Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting mujoco-py\n  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cffi>=1.10 in /opt/conda/lib/python3.7/site-packages (from mujoco-py) (1.15.1)\nRequirement already satisfied: fasteners~=0.15 in /opt/conda/lib/python3.7/site-packages (from mujoco-py) (0.18)\nRequirement already satisfied: Cython>=0.27.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py) (0.29.34)\nRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from mujoco-py) (1.21.6)\nRequirement already satisfied: imageio>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py) (2.25.0)\nCollecting glfw>=1.4.0\n  Downloading glfw-2.5.9-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10->mujoco-py) (2.21)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio>=2.1.2->mujoco-py) (9.4.0)\nInstalling collected packages: glfw, mujoco-py\nSuccessfully installed glfw-2.5.9 mujoco-py-2.1.2.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \n\nclass ExperienceReplayBuffer: \n    def __init__(self, max_memory, input_shape, n_actions, batch_size, cer=False): \n        self.mem_size = max_memory\n        self.mem_counter = 0\n        self.state_memory = np.zeros((self.mem_size, *input_shape),\n                                     dtype=np.float32)\n        self.next_state_memory = np.zeros((self.mem_size, *input_shape),\n                                         dtype=np.float32)\n\n        self.action_memory = np.zeros((self.mem_size, n_actions))\n        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int64)\n        self.batch_size = batch_size\n        self.cer = cer\n\n    def store_experience(self, state, action, reward, next_state, done): \n        index = self.mem_counter % self.mem_size \n\n        self.state_memory[index] = state\n        self.next_state_memory[index] = next_state\n        self.reward_memory[index] = reward\n        self.action_memory[index] = action\n        self.terminal_memory[index] = done\n        self.mem_counter += 1\n\n    def sample_experience(self, batch_size):\n        # used to get the last transition\n        offset = 1 if self.cer else 0\n\n        max_mem = min(self.mem_counter, self.mem_size) - offset\n        batch_index = np.random.choice(max_mem, batch_size - offset, replace=False)\n\n        states = self.state_memory[batch_index]\n        next_states = self.next_state_memory[batch_index]\n        rewards = self.reward_memory[batch_index]\n        actions = self.action_memory[batch_index]\n        terminals = self.terminal_memory[batch_index]\n        \n        if self.cer: \n            last_index = self.mem_counter % self.mem_size - 1\n            last_state = self.state_memory[last_index]\n            last_action = self.action_memory[last_index]\n            last_terminal = self.terminal_memory[last_index]\n            last_next_state = self.next_state_memory[last_index]\n            last_reward = self.reward_memory[last_index]\n\n            # for 2d and 3d use vstack to append, for 1d array use append() to append the data\n            states = np.vstack((self.state_memory[batch_index], last_state))\n            next_states = np.vstack((self.next_state_memory[batch_index], last_next_state))\n\n            actions = np.append(actions, last_action)\n            terminals = np.append(terminals, last_terminal)\n            rewards = np.append(rewards, last_reward)\n    \n        return states, actions, rewards, next_states, terminals\n    \n    \n    def is_sufficient(self): \n        return self.mem_counter > self.batch_size","metadata":{"execution":{"iopub.status.busy":"2023-04-17T08:51:12.549764Z","iopub.execute_input":"2023-04-17T08:51:12.550524Z","iopub.status.idle":"2023-04-17T08:51:12.567871Z","shell.execute_reply.started":"2023-04-17T08:51:12.550473Z","shell.execute_reply":"2023-04-17T08:51:12.566750Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D\nimport tensorflow as tf \nimport tensorflow.keras as keras \n\nclass CriticNetwork(tf.keras.Model):\n    def __init__(self):\n        super(CriticNetwork, self).__init__()\n        self.conv1 = Conv2D(64, 3, activation='relu', kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.conv2 = Conv2D(32, 3, activation=\"relu\", kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.fc1 = Dense(64, activation=\"relu\",  kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.fc3 = Dense(1, activation=None)\n        self.flatten = Flatten()\n        \n    def call(self, x):\n        state = x[0]\n        action = x[0]\n      #  x = self.conv1(tf.concat([state, action], axis=1))\n       # x = self.conv2(x)\n        #x = self.flatten(x)\n        #x = self.fc1(x)\n        #x = self.fc3(x)\n        x = self.fc1(tf.concat([state, action], axis=1))\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x                                                                                                                                                        ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:07.453245Z","iopub.execute_input":"2023-04-17T12:00:07.454438Z","iopub.status.idle":"2023-04-17T12:00:07.465624Z","shell.execute_reply.started":"2023-04-17T12:00:07.454393Z","shell.execute_reply":"2023-04-17T12:00:07.464369Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class ActorNetwork(keras.Model):\n    def __init__(self, noise, n_actions):\n        super(ActorNetwork, self).__init__()\n        self.noise = noise\n        self.conv1 = Conv2D(64, 3, activation='relu', kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.conv2 = Conv2D(32, 3, activation=\"relu\", kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.fc1 = Dense(64, activation='relu')\n        self.fc2 = Dense(32, activation='relu')\n        self.mu = Dense(n_actions, activation=None)\n        self.sigma = Dense(n_actions, activation=None)\n        self.flatten = Flatten()\n\n    def call(self, state):\n      #  x = self.conv1(state)\n       # x = self.conv2(x)\n        #x = self.flatten(x)\n        #x = self.fc1(x)\n        prob = self.fc1(state)\n        prob = self.fc2(prob)\n        mu = self.mu(prob)\n        sigma = self.sigma(prob)\n        \n        sigma = tf.clip_by_value(sigma, self.noise, 1)\n        return mu, sigma","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:30.357815Z","iopub.execute_input":"2023-04-17T12:00:30.358186Z","iopub.status.idle":"2023-04-17T12:00:30.366687Z","shell.execute_reply.started":"2023-04-17T12:00:30.358154Z","shell.execute_reply":"2023-04-17T12:00:30.365613Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class ValueNetwork(keras.Model):\n    def __init__(self):\n        super(ValueNetwork, self).__init__()\n        self.conv1 = Conv2D(64, 3, activation='relu', kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.conv2 = Conv2D(32, 3, activation=\"relu\", kernel_initializer=\"he_uniform\", data_format=\"channels_first\")\n        self.fc1 = Dense(64, activation='relu', kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(32, activation='relu', kernel_initializer=\"he_uniform\")\n        self.v = Dense(1, activation=\"linear\")\n        self.flatten = Flatten()\n        \n    def call(self, state):\n     #   x = self.conv1(state)\n      #  x = self.conv2(x)\n       # x = self.flatten(x)\n       # x = self.fc1(x)\n        state_value = self.fc1(state)\n        state_value = self.fc2(state_value)\n        v = self.v(state_value)\n        return v","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:42.837644Z","iopub.execute_input":"2023-04-17T12:00:42.838372Z","iopub.status.idle":"2023-04-17T12:00:42.846371Z","shell.execute_reply.started":"2023-04-17T12:00:42.838336Z","shell.execute_reply":"2023-04-17T12:00:42.845209Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nimport tensorflow as tf \nimport tensorflow_probability as tfp\nimport numpy as np\n\nclass SACAgent:\n  \n    def __init__(self, input_dims, out_dims, gamma, alpha, beta,\n                                            batch_size, noise, tau, mem_size,\n                                            min_action, max_action, chkpt=\"models/\"): \n        self.gamma = gamma \n        self.batch_size = batch_size \n        self.chkpt = chkpt\n        self.min_action = min_action \n        self.max_action = max_action \n        self.scale = 5\n        self.tau = tau\n        \n        self.memory = ExperienceReplayBuffer(mem_size, input_dims, out_dims, batch_size, False)\n        self.actor = ActorNetwork(noise, out_dims)\n        self.target_actor = ActorNetwork(noise, out_dims)\n        self.critic_1 = CriticNetwork()\n        self.critic_2 = CriticNetwork()\n        self.target_critic = CriticNetwork()\n        self.value_network = ValueNetwork()\n        self.target_value_network = ValueNetwork()\n        \n        self.critic_1.compile(optimizer=Adam(learning_rate=beta))\n        self.critic_2.compile(optimizer=Adam(learning_rate=beta))\n        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n        self.value_network.compile(optimizer=Adam(learning_rate=beta))\n        self.target_value_network.compile(optimizer=Adam(learning_rate=beta))\n    \n    def sample_normal(self, state): \n        mu, std = self.actor(state)\n        probs = tfp.distributions.Normal(mu, std)\n        actions = probs.sample()\n            \n        action = tf.math.tanh(actions)*self.max_action\n        log_probs = probs.log_prob(actions)\n        log_probs -= tf.math.log(1-tf.math.pow(action, 2)+self.actor.noise)\n        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n        return actions, log_probs  \n    \n    def get_action(self, state): \n      #  state = tf.reshape(tf.convert_to_tensor(state), (1, *state.shape))\n        state = tf.convert_to_tensor([state])\n        actions, _ = self.sample_normal(state)\n        return actions[0].numpy()\n\n    def save_models(self):\n        self.actor.save(self.chkpt + \"_\" + \"actor_network\")\n        self.critic_1.save(self.chkpt + \"_\" + \"critic_1_network\")\n        self.critic_2.save(self.chkpt + \"_\" + \"critic_2_network\")\n        self.target_value_network.save(self.chkpt + \"_\" + \"target_value_network\")\n        self.value_network.save(self.chkpt + \"_\" + \"value_network\")\n        print(\"[+] Saving the model\")\n\n\n    def load_models(self):\n        self.actor = tf.keras.models.load_model(self.chkpt + \"_\" + \"actor_network\") \n        self.critic_1 = tf.keras.models.load_model(self.chkpt + \"_\" + \"critic_1_network\") \n        self.critic_2 = tf.keras.models.load_model(self.chkpt + \"_\" + \"critic_2_network\") \n        self.value_network = tf.keras.models.load_model(self.chkpt + \"_\" + \"value_network\") \n        self.target_value_network = tf.keras.models.load_model(self.chkpt + \"_\" + \"target_value_network\") \n        \n        print(\"[+] Loading the model\")\n        \n    def store_experience(self, state, action, reward, state_, done):\n        self.memory.store_experience(state, action, reward, state_, done)\n\n    def sample_experience(self):\n        state, action, reward, new_state, done = \\\n                                  self.memory.sample_experience(self.batch_size)\n        states = tf.convert_to_tensor(state)\n        rewards = tf.convert_to_tensor(reward)\n        dones = tf.convert_to_tensor(done)\n        actions = tf.convert_to_tensor(action, dtype=tf.int32)\n        states_ = tf.convert_to_tensor(new_state)\n        return states, actions, rewards, states_, dones\n    \n    def update_network_parameters(self): \n        value_weights = self.value_network.get_weights()\n        target_value_weights = self.target_value_network.get_weights()\n        \n        for i in range(len(value_weights)): \n            target_value_weights[i] = self.tau * value_weights[i] + (1 - self.tau) * target_value_weights[i]\n            \n        self.target_value_network.set_weights(target_value_weights)\n        #weights = []\n        #targets = self.target_value_network.weights\n        #for i, weight in enumerate(self.value_network.weights):\n         #   weights.append(weight * tau + targets[i]*(1-tau))\n\n        #self.target_value_network.set_weights(weights)\n  \n    def learn(self): \n        if not self.memory.is_sufficient(): \n            return \n        \n        states, actions, rewards, states_, done = self.sample_experience()\n        done = tf.cast(done, tf.float32)\n        \n        with tf.GradientTape() as tape:\n            value = tf.squeeze(self.value_network(states), 1)\n            current_policy_actions, log_probs = self.sample_normal(states)\n            \n            log_probs = tf.squeeze(log_probs, 1)\n            q1_new_pi = self.critic_1((states, current_policy_actions))\n            q2_new_pi = self.critic_2((states, current_policy_actions))\n            critic_value = tf.squeeze(\n                                tf.math.minimum(q1_new_pi, q2_new_pi), 1)\n\n            value_target = critic_value - log_probs\n            value_loss = 0.5 * keras.losses.MSE(value, value_target)\n        params = self.value_network.trainable_variables\n        grads = tape.gradient(value_loss, params)\n        self.value_network.optimizer.apply_gradients(zip(grads, params))\n\n        with tf.GradientTape() as tape:\n            new_policy_actions, log_probs = self.sample_normal(states)\n\n            log_probs = tf.squeeze(log_probs, 1)\n            q1_new_policy = self.critic_1((states, new_policy_actions))\n            q2_new_policy = self.critic_2((states, new_policy_actions))\n            critic_value = tf.squeeze(tf.math.minimum(\n                                        q1_new_policy, q2_new_policy), 1)\n            actor_loss = self.actor_loss(log_probs, critic_value)\n        params = self.actor.trainable_variables\n        grads = tape.gradient(actor_loss, params)\n        self.actor.optimizer.apply_gradients(zip(grads, params))\n\n        with tf.GradientTape(persistent=True) as tape:\n            value_ = tf.squeeze(self.target_value_network(states_), 1)            \n            target_q_val = self.scale*rewards + self.gamma*value_*(1-done)\n            q_val = tf.squeeze(self.critic_1((states, actions)), 1)\n            critic_1_loss = self.critic_loss(q_val, target_q_val)\n           \n        params_1 = self.critic_1.trainable_variables\n        grads_1 = tape.gradient(critic_1_loss, params_1)\n        self.critic_1.optimizer.apply_gradients(zip(grads_1, params_1))\n       \n        \n        with tf.GradientTape(persistent=True) as tape:\n            value_ = tf.squeeze(self.target_value_network(states_), 1)\n            target_q_val = self.scale*rewards + self.gamma*value_*(1-done)\n            q_val = tf.squeeze(self.critic_2((states, actions)), 1)\n            critic_2_loss = self.critic_loss(q_val, target_q_val)\n            \n        params_2 = self.critic_2.trainable_variables\n        grads_2 = tape.gradient(critic_2_loss, params_2)\n        self.critic_2.optimizer.apply_gradients(zip(grads_2, params_2))\n\n        self.update_network_parameters()\n\n    def actor_loss(self, log_probs, q_vals): \n        actor_loss = log_probs - q_vals\n        actor_loss = tf.math.reduce_mean(actor_loss)\n        return actor_loss\n  \n    def critic_loss(self, q_val, target_q_val): \n        return 0.5 * keras.losses.MSE(q_val, target_q_val)\n                                ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:46.130959Z","iopub.execute_input":"2023-04-17T12:00:46.131401Z","iopub.status.idle":"2023-04-17T12:00:46.166141Z","shell.execute_reply.started":"2023-04-17T12:00:46.131359Z","shell.execute_reply":"2023-04-17T12:00:46.165204Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import time\nfrom telebot import TeleBot\nimport datetime\nimport telebot\n\ntoken = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\nchat_id = \"1055055706\"\nbot = TeleBot(token=token) \n\ndef telegram_send(message, bot):\n    chat_id = \"1055055706\"\n    bot.send_message(chat_id=chat_id, text=message)\n\ndef welcome_msg(multi_step, double_dqn, dueling):\n    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n    telegram_send(st, bot)\n    \ndef info_msg(episode, max_episode, reward, best_score, loss): \n    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n    telegram_send(st, bot)\n\ndef end_msg(learning_time):\n    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n    telegram_send(st, bot)\n    print(st)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:46.641686Z","iopub.execute_input":"2023-04-17T12:00:46.642049Z","iopub.status.idle":"2023-04-17T12:00:46.653440Z","shell.execute_reply.started":"2023-04-17T12:00:46.642017Z","shell.execute_reply":"2023-04-17T12:00:46.652302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import collections\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gymnasium as gym\nimport tensorflow as tf\nfrom gymnasium.wrappers import *\n\n\ndef manage_memory():\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\n\ndef plot_learning_curve(scores, epsilons, filename, lines=None):\n    x = [_ for _ in range(len(scores))]\n    fig=plt.figure()\n    ax=fig.add_subplot(111, label=\"1\")\n    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n\n    ax.plot(x, epsilons, color=\"C0\")\n    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n    ax.tick_params(axis='x', colors=\"C0\")\n    ax.tick_params(axis='y', colors=\"C0\")\n\n    N = len(scores)\n    running_avg = np.empty(N)\n    for t in range(N):\n\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n\n    ax2.scatter(x, running_avg, color=\"C1\")\n    ax2.axes.get_xaxis().set_visible(False)\n    ax2.yaxis.tick_right()\n    ax2.set_ylabel('Score', color=\"C1\")\n    ax2.yaxis.set_label_position('right')\n    ax2.tick_params(axis='y', colors=\"C1\")\n\n    if lines is not None:\n        for line in lines:\n            plt.axvline(x=line)\n\n    plt.savefig(filename)\n\n\ndef make_env(env_name, video_file_name, episode_freq_fo_video): \n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    \n    if len(env.observation_space.shape) >= 3: \n        #env = AtariPreprocessing(env, 10, 4, 84, False, True)\n        env = ResizeObservation(env, 84)\n        env = GrayScaleObservation(env, keep_dim=False)\n        env = FrameStack(env, 4, lz4_compress=False)\n        env = NormalizeObservation(env)\n\n    return env","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:47.001785Z","iopub.execute_input":"2023-04-17T12:00:47.002534Z","iopub.status.idle":"2023-04-17T12:00:47.016077Z","shell.execute_reply.started":"2023-04-17T12:00:47.002500Z","shell.execute_reply":"2023-04-17T12:00:47.014935Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class Writer:\n    def __init__(self, fname): \n        self.fname = fname \n\n    def write_to_file(self, content): \n        with open(self.fname, \"a\") as file: \n            file.write(content + \"\\n\")\n\n    def read_file(self, fname):\n        with open(fname, \"r\") as file: \n            return file.read()\n            ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:47.431898Z","iopub.execute_input":"2023-04-17T12:00:47.432804Z","iopub.status.idle":"2023-04-17T12:00:47.439665Z","shell.execute_reply.started":"2023-04-17T12:00:47.432755Z","shell.execute_reply":"2023-04-17T12:00:47.438438Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport imageio\n\n\nclass RecordVideo: \n    \n    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n        self.prefix_fname = prefix_fname\n        self.out_directory = out_directory\n        self.fps = fps\n        self.images = []\n        \n    def add_image(self, image): \n        self.images.append(image)\n    \n    def save(self, episode_no): \n        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n        self.images = []","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:47.831561Z","iopub.execute_input":"2023-04-17T12:00:47.832749Z","iopub.status.idle":"2023-04-17T12:00:47.841104Z","shell.execute_reply.started":"2023-04-17T12:00:47.832702Z","shell.execute_reply":"2023-04-17T12:00:47.840044Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from npy_append_array import NpyAppendArray\nimport numpy as np\n\nclass Trainer:   \n    def __init__(self, env, gamma, alpha, beta, batch_size, tau, noe, max_steps, \n                                                 is_tg, tg_bot_freq_epi, record, mem_size, noise, chkpt): \n       \n        self.env = env \n        self.target_score = 0\n        self.noe = noe\n        self.max_steps = max_steps\n        self.is_tg = is_tg\n        self.tg_bot_freq_epi = tg_bot_freq_epi\n        self.record = record \n        self.writer = Writer(\"model_training_results.txt\")\n        self.recorder = RecordVideo(\"sac\", \"videos/\", 20)\n        self.agent = SACAgent(env.observation_space.shape, env.action_space.shape[0], gamma, alpha, \n                                                beta, batch_size, noise, tau, mem_size, env.action_space.low[0],\n                                                env.action_space.high[0], chkpt\n                                        )\n\n    def train_rl_model(self): \n        avg_rewards = []\n        best_reward = float(\"-inf\")\n        episode_rewards = []\n\n        for episode in range(self.noe): \n            n_steps = 0 \n            state, _ = self.env.reset()\n            reward = 0 \n\n            if record and episode % 50 == 0:\n                img = self.env.render()\n                self.recorder.add_image(img)\n\n            for step in range(self.max_steps): \n                \n                action = self.agent.get_action(state)\n                next_info = self.env.step(action)\n                next_state, reward_prob, terminated, truncated, _ = next_info\n                done = truncated or terminated\n                reward += reward_prob\n\n                self.agent.store_experience(state, action, reward_prob, next_state, done)\n                self.agent.learn()\n\n                # record\n                if record and episode % 50 == 0:\n                    img = self.env.render()\n                    self.recorder.add_image(img)\n\n                # next state\n                state = next_state\n                n_steps += 1        \n                if done: \n                    break\n            \n            episode_rewards.append(reward)\n            avg_reward = np.mean(episode_rewards[-100:])\n            avg_rewards.append(avg_reward)\n\n            result = f\"Episode: {episode}, Steps: {n_steps}, Reward: {reward}, Best reward: {best_reward}, Avg reward: {avg_reward}\"\n            self.writer.write_to_file(result)\n            print(result)\n\n            # Recording.\n            if record and episode % 50 == 0:\n                self.recorder.save(episode)\n            \n            # Saving Best Model\n            if reward > best_reward and episode!=0: \n                best_reward = reward\n                self.agent.save_models()\n                \n            # Telegram bot\n            if self.is_tg and episode % self.tg_bot_freq_epi == 0: \n                info_msg(episode+1, self.noe, reward, best_reward, \"d\")\n                \n            # Eatly Stopping\n            if episode > 100 and np.mean(episode_rewards[-50:]) >= self.target_score: \n                break\n                \n                \n        return episode_rewards, avg_rewards, best_reward\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:48.393456Z","iopub.execute_input":"2023-04-17T12:00:48.393836Z","iopub.status.idle":"2023-04-17T12:00:48.411138Z","shell.execute_reply.started":"2023-04-17T12:00:48.393803Z","shell.execute_reply":"2023-04-17T12:00:48.408864Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nimport time\nimport signal\nimport time\nimport sys\nimport pickle\nimport os \n\nenv = make_env('LunarLanderContinuous-v2', None, None)\nprint(env)\nrecord = True\ngamma = 0.95\nalpha = 0.0001# actor lr\nbeta = 0.001# critic lr \nbatch_size = 64\ntau = 0.05\nnoe = 500\nmax_steps = int(1e7)\nis_tg = True \ntg_bot_freq_epi = 20\nrecord = True \nmem_size = 25000\nnoise = 0.1\nchkpt = \"models/sac/\"\n  \nif not os.path.exists(\"videos\"): \n    os.mkdir(\"videos\")\n\nif not os.path.exists(\"test_videos\"):\n    os.mkdir(\"test_videos\")\n\n\nif __name__ == \"__main__\": \n    \n    try: \n        manage_memory()\n        trainer = Trainer(env, gamma, alpha, beta, batch_size, tau, noe,\n                                          max_steps, is_tg, tg_bot_freq_epi, record, mem_size, noise, chkpt)\n        episode_rewards, avg_rewards, best_reward = trainer.train_rl_model()\n        \n        with open(\"sac_episode_rewards.obj\", \"wb\") as f: \n            pickle.dump(episode_rewards, f)\n        \n        with open(\"sac_avg_rewards.obj\", \"wb\") as f: \n            pickle.dump(avg_rewards, f)\n            \n        x = [i+1 for i in range(noe)]\n        plot_learning_curve(x, episode_rewards, \"sac_con_mountain_car\")\n\n       # model_path = \"models/lunarlander_DQN_q_value/\"\n\n        #evaluator = Eval(env, action_space, model_path, \"vanilla_dqn_lunarlander\", 10)\n        #evaluator.test()\n        \n    except Exception as error: \n        raise error","metadata":{"execution":{"iopub.status.busy":"2023-04-17T12:00:58.472861Z","iopub.execute_input":"2023-04-17T12:00:58.473822Z","iopub.status.idle":"2023-04-17T12:55:22.015451Z","shell.execute_reply.started":"2023-04-17T12:00:58.473771Z","shell.execute_reply":"2023-04-17T12:55:22.013628Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"<TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLanderContinuous-v2>>>>>\nEpisode: 0, Steps: 79, Reward: -545.9136988131911, Best reward: -inf, Avg reward: -545.9136988131911\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x5bda600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 1, Steps: 128, Reward: -742.1603003576906, Best reward: -inf, Avg reward: -644.0369995854409\n[+] Saving the model\nEpisode: 2, Steps: 68, Reward: -286.8447580403556, Best reward: -742.1603003576906, Avg reward: -524.9729190704124\n[+] Saving the model\nEpisode: 3, Steps: 119, Reward: -320.75902756918424, Best reward: -286.8447580403556, Avg reward: -473.91944619510537\nEpisode: 4, Steps: 102, Reward: -163.458229018368, Best reward: -286.8447580403556, Avg reward: -411.8272027597579\n[+] Saving the model\nEpisode: 5, Steps: 101, Reward: -295.1655839535804, Best reward: -163.458229018368, Avg reward: -392.383599625395\nEpisode: 6, Steps: 66, Reward: -199.15670951172348, Best reward: -163.458229018368, Avg reward: -364.7797581805848\nEpisode: 7, Steps: 110, Reward: -133.0645324930437, Best reward: -163.458229018368, Avg reward: -335.81535496964216\n[+] Saving the model\nEpisode: 8, Steps: 131, Reward: -329.6269743009766, Best reward: -133.0645324930437, Avg reward: -335.1277571175682\nEpisode: 9, Steps: 94, Reward: -299.3812148747032, Best reward: -133.0645324930437, Avg reward: -331.55310289328173\nEpisode: 10, Steps: 124, Reward: -391.7305833985876, Best reward: -133.0645324930437, Avg reward: -337.0237829392186\nEpisode: 11, Steps: 145, Reward: -517.0756252805613, Best reward: -133.0645324930437, Avg reward: -352.0281031343305\nEpisode: 12, Steps: 114, Reward: -382.6923639455444, Best reward: -133.0645324930437, Avg reward: -354.3868924275008\nEpisode: 13, Steps: 99, Reward: -389.6415855833689, Best reward: -133.0645324930437, Avg reward: -356.9050847957771\nEpisode: 14, Steps: 124, Reward: -117.6132320755681, Best reward: -133.0645324930437, Avg reward: -340.95229461442983\n[+] Saving the model\nEpisode: 15, Steps: 104, Reward: -374.8399696546793, Best reward: -117.6132320755681, Avg reward: -343.07027430444543\nEpisode: 16, Steps: 77, Reward: -86.18463522772734, Best reward: -117.6132320755681, Avg reward: -327.9593543587561\n[+] Saving the model\nEpisode: 17, Steps: 110, Reward: -373.8568424820053, Best reward: -86.18463522772734, Avg reward: -330.50921481004775\nEpisode: 18, Steps: 119, Reward: -506.81032491754127, Best reward: -86.18463522772734, Avg reward: -339.78822060517894\nEpisode: 19, Steps: 86, Reward: -356.78233637983334, Best reward: -86.18463522772734, Avg reward: -340.6379263939117\nEpisode: 20, Steps: 136, Reward: -427.6734194323951, Best reward: -86.18463522772734, Avg reward: -344.7824736814585\nEpisode: 21, Steps: 96, Reward: -83.83646344462636, Best reward: -86.18463522772734, Avg reward: -332.9212913979661\n[+] Saving the model\nEpisode: 22, Steps: 94, Reward: -272.7317357681379, Best reward: -83.83646344462636, Avg reward: -330.30435419666924\nEpisode: 23, Steps: 76, Reward: -5.95642635962642, Best reward: -83.83646344462636, Avg reward: -316.7898572034591\n[+] Saving the model\nEpisode: 24, Steps: 96, Reward: -439.45004451461574, Best reward: -5.95642635962642, Avg reward: -321.6962646959054\nEpisode: 25, Steps: 131, Reward: -76.58493615724902, Best reward: -5.95642635962642, Avg reward: -312.26890590595707\nEpisode: 26, Steps: 117, Reward: -198.38629706984202, Best reward: -5.95642635962642, Avg reward: -308.0510315046195\nEpisode: 27, Steps: 110, Reward: -268.1560272683844, Best reward: -5.95642635962642, Avg reward: -306.62620992475394\nEpisode: 28, Steps: 92, Reward: -482.8319162324986, Best reward: -5.95642635962642, Avg reward: -312.70226876295203\nEpisode: 29, Steps: 103, Reward: -158.86108551998197, Best reward: -5.95642635962642, Avg reward: -307.57422932151974\nEpisode: 30, Steps: 136, Reward: -299.81053918412545, Best reward: -5.95642635962642, Avg reward: -307.32378770418444\nEpisode: 31, Steps: 75, Reward: -65.21027958410922, Best reward: -5.95642635962642, Avg reward: -299.7577405754321\nEpisode: 32, Steps: 87, Reward: -326.14605158528593, Best reward: -5.95642635962642, Avg reward: -300.55738636360945\nEpisode: 33, Steps: 89, Reward: -241.7059778242059, Best reward: -5.95642635962642, Avg reward: -298.82646258303873\nEpisode: 34, Steps: 168, Reward: -184.5697647591872, Best reward: -5.95642635962642, Avg reward: -295.5619855023573\nEpisode: 35, Steps: 76, Reward: -72.46179035669766, Best reward: -5.95642635962642, Avg reward: -289.3647578594223\nEpisode: 36, Steps: 95, Reward: 19.530170739125268, Best reward: -5.95642635962642, Avg reward: -281.0162462756778\n[+] Saving the model\nEpisode: 37, Steps: 123, Reward: -48.253708067407516, Best reward: 19.530170739125268, Avg reward: -274.8909163228286\nEpisode: 38, Steps: 110, Reward: -305.1005956923235, Best reward: 19.530170739125268, Avg reward: -275.66552348614897\nEpisode: 39, Steps: 103, Reward: -305.84302222795736, Best reward: 19.530170739125268, Avg reward: -276.41996095469415\nEpisode: 40, Steps: 93, Reward: -30.425182659483255, Best reward: 19.530170739125268, Avg reward: -270.4200883133475\nEpisode: 41, Steps: 78, Reward: -130.72416750756057, Best reward: 19.530170739125268, Avg reward: -267.09399496082875\nEpisode: 42, Steps: 106, Reward: -119.2789143760007, Best reward: 19.530170739125268, Avg reward: -263.6564349472281\nEpisode: 43, Steps: 152, Reward: -476.341209160102, Best reward: 19.530170739125268, Avg reward: -268.4901798157025\nEpisode: 44, Steps: 89, Reward: -436.93891693754415, Best reward: 19.530170739125268, Avg reward: -272.2334850850768\nEpisode: 45, Steps: 119, Reward: -411.81912942904523, Best reward: 19.530170739125268, Avg reward: -275.26795561429344\nEpisode: 46, Steps: 1000, Reward: 19.209927742198346, Best reward: 19.530170739125268, Avg reward: -269.0024687343681\nEpisode: 47, Steps: 136, Reward: -222.92232376221256, Best reward: 19.530170739125268, Avg reward: -268.04246571411494\nEpisode: 48, Steps: 99, Reward: -553.6847375271477, Best reward: 19.530170739125268, Avg reward: -273.8718998327482\nEpisode: 49, Steps: 81, Reward: -358.37720634652106, Best reward: 19.530170739125268, Avg reward: -275.5620059630237\nEpisode: 50, Steps: 109, Reward: -283.6763359508725, Best reward: 19.530170739125268, Avg reward: -275.7211104725894\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x66fc600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 51, Steps: 163, Reward: -86.13370852287889, Best reward: 19.530170739125268, Avg reward: -272.0751988966334\nEpisode: 52, Steps: 101, Reward: -112.14820523251643, Best reward: 19.530170739125268, Avg reward: -269.05770845014064\nEpisode: 53, Steps: 78, Reward: -291.4682514907656, Best reward: 19.530170739125268, Avg reward: -269.47271850644853\nEpisode: 54, Steps: 91, Reward: -344.08749550944333, Best reward: 19.530170739125268, Avg reward: -270.8293508155939\nEpisode: 55, Steps: 78, Reward: -79.38704062125464, Best reward: 19.530170739125268, Avg reward: -267.41073813355206\nEpisode: 56, Steps: 108, Reward: -289.37565598101395, Best reward: 19.530170739125268, Avg reward: -267.79608756947243\nEpisode: 57, Steps: 79, Reward: -194.68888775121357, Best reward: 19.530170739125268, Avg reward: -266.5356186070887\nEpisode: 58, Steps: 106, Reward: -235.10124029677888, Best reward: 19.530170739125268, Avg reward: -266.00283253403256\nEpisode: 59, Steps: 79, Reward: -82.17088048414634, Best reward: 19.530170739125268, Avg reward: -262.93896666653444\nEpisode: 60, Steps: 187, Reward: -357.5265564993293, Best reward: 19.530170739125268, Avg reward: -264.4895828933016\nEpisode: 61, Steps: 145, Reward: -295.0287460781258, Best reward: 19.530170739125268, Avg reward: -264.9821500414439\nEpisode: 62, Steps: 79, Reward: -321.0488923878774, Best reward: 19.530170739125268, Avg reward: -265.8720983326571\nEpisode: 63, Steps: 155, Reward: -147.74656760030712, Best reward: 19.530170739125268, Avg reward: -264.0263869149642\nEpisode: 64, Steps: 92, Reward: -293.04691218666136, Best reward: 19.530170739125268, Avg reward: -264.4728565345288\nEpisode: 65, Steps: 132, Reward: -410.3776016004487, Best reward: 19.530170739125268, Avg reward: -266.683534490073\nEpisode: 66, Steps: 114, Reward: -346.7156996555886, Best reward: 19.530170739125268, Avg reward: -267.87804441791656\nEpisode: 67, Steps: 92, Reward: -299.61596331730556, Best reward: 19.530170739125268, Avg reward: -268.3447785193781\nEpisode: 68, Steps: 87, Reward: -85.09664595545851, Best reward: 19.530170739125268, Avg reward: -265.6890084822199\nEpisode: 69, Steps: 103, Reward: -177.38940225872773, Best reward: 19.530170739125268, Avg reward: -264.42758553616994\nEpisode: 70, Steps: 105, Reward: -164.88887633054998, Best reward: 19.530170739125268, Avg reward: -263.0256318853866\nEpisode: 71, Steps: 108, Reward: -281.22592442993226, Best reward: 19.530170739125268, Avg reward: -263.2784137262831\nEpisode: 72, Steps: 79, Reward: -81.85910306945196, Best reward: 19.530170739125268, Avg reward: -260.7932176898881\nEpisode: 73, Steps: 103, Reward: -115.8610570975533, Best reward: 19.530170739125268, Avg reward: -258.8346749791809\nEpisode: 74, Steps: 103, Reward: -18.673994388869446, Best reward: 19.530170739125268, Avg reward: -255.63253257131007\nEpisode: 75, Steps: 94, Reward: -280.42023486620246, Best reward: 19.530170739125268, Avg reward: -255.95868654887445\nEpisode: 76, Steps: 125, Reward: -34.01967730781617, Best reward: 19.530170739125268, Avg reward: -253.07636175353602\nEpisode: 77, Steps: 121, Reward: -163.07742763896323, Best reward: 19.530170739125268, Avg reward: -251.9225292648877\nEpisode: 78, Steps: 141, Reward: -102.01391800226224, Best reward: 19.530170739125268, Avg reward: -250.02495190713293\nEpisode: 79, Steps: 86, Reward: -180.3445178657886, Best reward: 19.530170739125268, Avg reward: -249.1539464816161\nEpisode: 80, Steps: 92, Reward: -28.218514988424417, Best reward: 19.530170739125268, Avg reward: -246.42634856194707\nEpisode: 81, Steps: 75, Reward: -203.0241847382964, Best reward: 19.530170739125268, Avg reward: -245.89705388117085\nEpisode: 82, Steps: 76, Reward: -55.9223994617996, Best reward: 19.530170739125268, Avg reward: -243.60820262310614\nEpisode: 83, Steps: 183, Reward: -41.87656443352638, Best reward: 19.530170739125268, Avg reward: -241.2066355018016\nEpisode: 84, Steps: 112, Reward: -367.93254860499536, Best reward: 19.530170739125268, Avg reward: -242.6975285971333\nEpisode: 85, Steps: 71, Reward: -341.9862872558702, Best reward: 19.530170739125268, Avg reward: -243.8520490466535\nEpisode: 86, Steps: 69, Reward: -32.53602180024939, Best reward: 19.530170739125268, Avg reward: -241.42312919324655\nEpisode: 87, Steps: 158, Reward: -149.0529791011077, Best reward: 19.530170739125268, Avg reward: -240.37346839674498\nEpisode: 88, Steps: 84, Reward: -365.0146563640207, Best reward: 19.530170739125268, Avg reward: -241.77393118289413\nEpisode: 89, Steps: 127, Reward: -387.61057412181833, Best reward: 19.530170739125268, Avg reward: -243.39433832665995\nEpisode: 90, Steps: 119, Reward: -223.28730899604898, Best reward: 19.530170739125268, Avg reward: -243.1733819603895\nEpisode: 91, Steps: 100, Reward: -95.83647039909856, Best reward: 19.530170739125268, Avg reward: -241.57189379124503\nEpisode: 92, Steps: 126, Reward: -215.1411062454677, Best reward: 19.530170739125268, Avg reward: -241.2876917746238\nEpisode: 93, Steps: 181, Reward: -85.41362574134598, Best reward: 19.530170739125268, Avg reward: -239.6294570295889\nEpisode: 94, Steps: 97, Reward: -90.52367394471645, Best reward: 19.530170739125268, Avg reward: -238.05992247080079\nEpisode: 95, Steps: 103, Reward: -288.1536765675439, Best reward: 19.530170739125268, Avg reward: -238.58173240930853\nEpisode: 96, Steps: 91, Reward: -202.19390135332173, Best reward: 19.530170739125268, Avg reward: -238.20660013038085\nEpisode: 97, Steps: 93, Reward: -249.50168134256904, Best reward: 19.530170739125268, Avg reward: -238.32185606111744\nEpisode: 98, Steps: 91, Reward: -101.7047607553305, Best reward: 19.530170739125268, Avg reward: -236.94188540146305\nEpisode: 99, Steps: 90, Reward: -379.1309363155333, Best reward: 19.530170739125268, Avg reward: -238.36377591060375\nEpisode: 100, Steps: 94, Reward: -291.63978074940417, Best reward: 19.530170739125268, Avg reward: -235.82103672996584\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x5999600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 101, Steps: 112, Reward: -50.03836668693256, Best reward: 19.530170739125268, Avg reward: -228.89981739325827\nEpisode: 102, Steps: 97, Reward: -101.46069612082917, Best reward: 19.530170739125268, Avg reward: -227.04597677406298\nEpisode: 103, Steps: 85, Reward: -219.147495235421, Best reward: 19.530170739125268, Avg reward: -226.02986145072538\nEpisode: 104, Steps: 196, Reward: -535.3645091823139, Best reward: 19.530170739125268, Avg reward: -229.74892425236482\nEpisode: 105, Steps: 95, Reward: -335.37669801358095, Best reward: 19.530170739125268, Avg reward: -230.15103539296484\nEpisode: 106, Steps: 85, Reward: -102.72401212842507, Best reward: 19.530170739125268, Avg reward: -229.18670841913186\nEpisode: 107, Steps: 81, Reward: -68.2052856699024, Best reward: 19.530170739125268, Avg reward: -228.53811595090042\nEpisode: 108, Steps: 85, Reward: -70.66796557588768, Best reward: 19.530170739125268, Avg reward: -225.94852586364956\nEpisode: 109, Steps: 202, Reward: -288.607931627457, Best reward: 19.530170739125268, Avg reward: -225.84079303117713\nEpisode: 110, Steps: 166, Reward: -306.6277202764866, Best reward: 19.530170739125268, Avg reward: -224.98976439995607\nEpisode: 111, Steps: 90, Reward: -426.82950460006987, Best reward: 19.530170739125268, Avg reward: -224.08730319315123\nEpisode: 112, Steps: 131, Reward: -252.17331112274255, Best reward: 19.530170739125268, Avg reward: -222.78211266492318\nEpisode: 113, Steps: 76, Reward: -67.93193604910019, Best reward: 19.530170739125268, Avg reward: -219.56501616958053\nEpisode: 114, Steps: 83, Reward: -66.55902196449134, Best reward: 19.530170739125268, Avg reward: -219.05447406846972\nEpisode: 115, Steps: 92, Reward: -85.20403268709181, Best reward: 19.530170739125268, Avg reward: -216.15811469879384\nEpisode: 116, Steps: 74, Reward: -73.21071280830401, Best reward: 19.530170739125268, Avg reward: -216.02837547459959\nEpisode: 117, Steps: 143, Reward: -323.7383412606649, Best reward: 19.530170739125268, Avg reward: -215.52719046238622\nEpisode: 118, Steps: 104, Reward: -167.24775615261422, Best reward: 19.530170739125268, Avg reward: -212.13156477473692\nEpisode: 119, Steps: 122, Reward: -278.56196832368676, Best reward: 19.530170739125268, Avg reward: -211.34936109417546\nEpisode: 120, Steps: 104, Reward: -394.0595597659606, Best reward: 19.530170739125268, Avg reward: -211.0132224975111\nEpisode: 121, Steps: 148, Reward: -427.5600846099579, Best reward: 19.530170739125268, Avg reward: -214.45045870916445\nEpisode: 122, Steps: 117, Reward: -263.4366847979302, Best reward: 19.530170739125268, Avg reward: -214.35750819946236\nEpisode: 123, Steps: 110, Reward: -304.42170637848324, Best reward: 19.530170739125268, Avg reward: -217.3421609996509\nEpisode: 124, Steps: 148, Reward: -294.2703055735835, Best reward: 19.530170739125268, Avg reward: -215.8903636102406\nEpisode: 125, Steps: 143, Reward: -431.5415284860235, Best reward: 19.530170739125268, Avg reward: -219.43992953352833\nEpisode: 126, Steps: 100, Reward: -492.0436708997316, Best reward: 19.530170739125268, Avg reward: -222.37650327182723\nEpisode: 127, Steps: 125, Reward: -536.3947504692148, Best reward: 19.530170739125268, Avg reward: -225.05889050383558\nEpisode: 128, Steps: 104, Reward: -174.79929362461243, Best reward: 19.530170739125268, Avg reward: -221.9785642777567\nEpisode: 129, Steps: 88, Reward: -376.5206650092224, Best reward: 19.530170739125268, Avg reward: -224.1551600726491\nEpisode: 130, Steps: 132, Reward: -376.75573285513855, Best reward: 19.530170739125268, Avg reward: -224.92461200935918\nEpisode: 131, Steps: 124, Reward: -214.38896149894214, Best reward: 19.530170739125268, Avg reward: -226.4163988285075\nEpisode: 132, Steps: 169, Reward: -111.0181648701679, Best reward: 19.530170739125268, Avg reward: -224.26511996135636\nEpisode: 133, Steps: 67, Reward: -229.9628313764034, Best reward: 19.530170739125268, Avg reward: -224.14768849687832\nEpisode: 134, Steps: 81, Reward: -92.09070854822033, Best reward: 19.530170739125268, Avg reward: -223.2228979347687\nEpisode: 135, Steps: 86, Reward: -472.2149159233972, Best reward: 19.530170739125268, Avg reward: -227.22042919043568\nEpisode: 136, Steps: 78, Reward: -162.19441293256116, Best reward: 19.530170739125268, Avg reward: -229.03767502715252\nEpisode: 137, Steps: 100, Reward: -503.24525663931496, Best reward: 19.530170739125268, Avg reward: -233.58759051287157\nEpisode: 138, Steps: 102, Reward: -33.75572247502137, Best reward: 19.530170739125268, Avg reward: -230.87414178069855\nEpisode: 139, Steps: 101, Reward: -383.69998151624344, Best reward: 19.530170739125268, Avg reward: -231.6527113735814\nEpisode: 140, Steps: 85, Reward: -299.13283430711743, Best reward: 19.530170739125268, Avg reward: -234.33978789005778\nEpisode: 141, Steps: 87, Reward: -490.06524870775655, Best reward: 19.530170739125268, Avg reward: -237.93319870205974\nEpisode: 142, Steps: 159, Reward: -102.34140644276741, Best reward: 19.530170739125268, Avg reward: -237.7638236227274\nEpisode: 143, Steps: 95, Reward: -252.0377992727261, Best reward: 19.530170739125268, Avg reward: -235.52078952385367\nEpisode: 144, Steps: 95, Reward: -218.90905063136685, Best reward: 19.530170739125268, Avg reward: -233.34049086079187\nEpisode: 145, Steps: 144, Reward: -11.108705450828978, Best reward: 19.530170739125268, Avg reward: -229.33338662100968\nEpisode: 146, Steps: 93, Reward: -313.17035649202813, Best reward: 19.530170739125268, Avg reward: -232.65718946335198\nEpisode: 147, Steps: 84, Reward: -93.90757405432117, Best reward: 19.530170739125268, Avg reward: -231.36704196627304\nEpisode: 148, Steps: 117, Reward: -258.76128361585353, Best reward: 19.530170739125268, Avg reward: -228.4178074271601\nEpisode: 149, Steps: 81, Reward: -85.00501886150579, Best reward: 19.530170739125268, Avg reward: -225.68408555231\nEpisode: 150, Steps: 210, Reward: -341.21375765758904, Best reward: 19.530170739125268, Avg reward: -226.25945976937714\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x6627600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 151, Steps: 156, Reward: -39.64237234893703, Best reward: 19.530170739125268, Avg reward: -225.79454640763777\nEpisode: 152, Steps: 75, Reward: -35.14954980002669, Best reward: 19.530170739125268, Avg reward: -225.0245598533128\nEpisode: 153, Steps: 145, Reward: -324.4745219325579, Best reward: 19.530170739125268, Avg reward: -225.35462255773075\nEpisode: 154, Steps: 116, Reward: -445.59755353585655, Best reward: 19.530170739125268, Avg reward: -226.36972313799487\nEpisode: 155, Steps: 77, Reward: -218.57189186531315, Best reward: 19.530170739125268, Avg reward: -227.76157165043549\nEpisode: 156, Steps: 148, Reward: -281.2429725552033, Best reward: 19.530170739125268, Avg reward: -227.6802448161774\nEpisode: 157, Steps: 68, Reward: -32.164907058218134, Best reward: 19.530170739125268, Avg reward: -226.0550050092474\nEpisode: 158, Steps: 88, Reward: -47.13068032773004, Best reward: 19.530170739125268, Avg reward: -224.1752994095569\nEpisode: 159, Steps: 99, Reward: -328.88899884366276, Best reward: 19.530170739125268, Avg reward: -226.64248059315207\nEpisode: 160, Steps: 125, Reward: -265.50926837355007, Best reward: 19.530170739125268, Avg reward: -225.72230771189427\nEpisode: 161, Steps: 103, Reward: -230.62554486923975, Best reward: 19.530170739125268, Avg reward: -225.07827569980543\nEpisode: 162, Steps: 130, Reward: -508.6199162770609, Best reward: 19.530170739125268, Avg reward: -226.95398593869726\nEpisode: 163, Steps: 111, Reward: -153.881338973846, Best reward: 19.530170739125268, Avg reward: -227.01533365243264\nEpisode: 164, Steps: 173, Reward: -96.27598243990771, Best reward: 19.530170739125268, Avg reward: -225.0476243549651\nEpisode: 165, Steps: 139, Reward: -284.8792928571685, Best reward: 19.530170739125268, Avg reward: -223.7926412675323\nEpisode: 166, Steps: 79, Reward: -298.5740318786928, Best reward: 19.530170739125268, Avg reward: -223.31122458976333\nEpisode: 167, Steps: 88, Reward: -231.67007680874298, Best reward: 19.530170739125268, Avg reward: -222.6317657246777\nEpisode: 168, Steps: 106, Reward: -229.2569237035667, Best reward: 19.530170739125268, Avg reward: -224.07336850215881\nEpisode: 169, Steps: 124, Reward: -90.28074536522807, Best reward: 19.530170739125268, Avg reward: -223.2022819332238\nEpisode: 170, Steps: 112, Reward: -314.47386049207944, Best reward: 19.530170739125268, Avg reward: -224.69813177483914\nEpisode: 171, Steps: 172, Reward: -413.8300410383005, Best reward: 19.530170739125268, Avg reward: -226.0241729409228\nEpisode: 172, Steps: 176, Reward: -60.19119869123587, Best reward: 19.530170739125268, Avg reward: -225.80749389714063\nEpisode: 173, Steps: 98, Reward: -426.0509255357953, Best reward: 19.530170739125268, Avg reward: -228.90939258152304\nEpisode: 174, Steps: 136, Reward: -53.565325156911314, Best reward: 19.530170739125268, Avg reward: -229.25830588920348\nEpisode: 175, Steps: 105, Reward: 38.78506531892563, Best reward: 19.530170739125268, Avg reward: -226.06625288735216\n[+] Saving the model\nEpisode: 176, Steps: 88, Reward: -93.14576340279886, Best reward: 38.78506531892563, Avg reward: -226.65751374830197\nEpisode: 177, Steps: 101, Reward: -267.16761284768893, Best reward: 38.78506531892563, Avg reward: -227.69841560038927\nEpisode: 178, Steps: 110, Reward: -153.51955176741913, Best reward: 38.78506531892563, Avg reward: -228.2134719380408\nEpisode: 179, Steps: 80, Reward: -513.4332744591097, Best reward: 38.78506531892563, Avg reward: -231.54435950397405\nEpisode: 180, Steps: 84, Reward: -413.7836384770906, Best reward: 38.78506531892563, Avg reward: -235.40001073886066\nEpisode: 181, Steps: 147, Reward: -320.17057433780695, Best reward: 38.78506531892563, Avg reward: -236.5714746348558\nEpisode: 182, Steps: 105, Reward: -46.35103568096177, Best reward: 38.78506531892563, Avg reward: -236.47576099704736\nEpisode: 183, Steps: 113, Reward: 11.093001341396103, Best reward: 38.78506531892563, Avg reward: -235.94606533929817\nEpisode: 184, Steps: 93, Reward: -289.31873802333826, Best reward: 38.78506531892563, Avg reward: -235.15992723348162\nEpisode: 185, Steps: 145, Reward: -302.74265290893084, Best reward: 38.78506531892563, Avg reward: -234.76749089001223\nEpisode: 186, Steps: 133, Reward: -332.6682439614551, Best reward: 38.78506531892563, Avg reward: -237.7688131116243\nEpisode: 187, Steps: 96, Reward: -68.55757898390587, Best reward: 38.78506531892563, Avg reward: -236.9638591104523\nEpisode: 188, Steps: 119, Reward: -331.79977543664114, Best reward: 38.78506531892563, Avg reward: -236.6317103011785\nEpisode: 189, Steps: 115, Reward: -275.50874925811115, Best reward: 38.78506531892563, Avg reward: -235.51069205254143\nEpisode: 190, Steps: 185, Reward: -274.54421247762355, Best reward: 38.78506531892563, Avg reward: -236.02326108735716\nEpisode: 191, Steps: 106, Reward: -248.23380542130528, Best reward: 38.78506531892563, Avg reward: -237.54723443757922\nEpisode: 192, Steps: 79, Reward: -47.833660137007, Best reward: 38.78506531892563, Avg reward: -235.87415997649464\nEpisode: 193, Steps: 74, Reward: -129.54151235719073, Best reward: 38.78506531892563, Avg reward: -236.31543884265307\nEpisode: 194, Steps: 142, Reward: -79.72993435980187, Best reward: 38.78506531892563, Avg reward: -236.20750144680395\nEpisode: 195, Steps: 143, Reward: -312.2035451780311, Best reward: 38.78506531892563, Avg reward: -236.4480001329088\nEpisode: 196, Steps: 133, Reward: -340.43509931528314, Best reward: 38.78506531892563, Avg reward: -237.83041211252842\nEpisode: 197, Steps: 81, Reward: -214.37131244953667, Best reward: 38.78506531892563, Avg reward: -237.47910842359812\nEpisode: 198, Steps: 108, Reward: -97.64713014848085, Best reward: 38.78506531892563, Avg reward: -237.4385321175296\nEpisode: 199, Steps: 91, Reward: -76.53411475898966, Best reward: 38.78506531892563, Avg reward: -234.41256390196418\nEpisode: 200, Steps: 111, Reward: -81.08800618803781, Best reward: 38.78506531892563, Avg reward: -232.3070461563505\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x6a91600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 201, Steps: 88, Reward: -39.010480355282645, Best reward: 38.78506531892563, Avg reward: -232.196767293034\nEpisode: 202, Steps: 70, Reward: -86.08745038393312, Best reward: 38.78506531892563, Avg reward: -232.04303483566503\nEpisode: 203, Steps: 105, Reward: -128.33312728167562, Best reward: 38.78506531892563, Avg reward: -231.13489115612757\nEpisode: 204, Steps: 93, Reward: -94.91487229500733, Best reward: 38.78506531892563, Avg reward: -226.7303947872545\nEpisode: 205, Steps: 88, Reward: -317.9201645398332, Best reward: 38.78506531892563, Avg reward: -226.55582945251703\nEpisode: 206, Steps: 99, Reward: -361.56242311849525, Best reward: 38.78506531892563, Avg reward: -229.14421356241772\nEpisode: 207, Steps: 128, Reward: -155.92080087944348, Best reward: 38.78506531892563, Avg reward: -230.02136871451316\nEpisode: 208, Steps: 83, Reward: -324.87520050606713, Best reward: 38.78506531892563, Avg reward: -232.5634410638149\nEpisode: 209, Steps: 107, Reward: -418.6863691049844, Best reward: 38.78506531892563, Avg reward: -233.8642254385902\nEpisode: 210, Steps: 121, Reward: -222.69148874520863, Best reward: 38.78506531892563, Avg reward: -233.02486312327744\nEpisode: 211, Steps: 114, Reward: -68.8999217414088, Best reward: 38.78506531892563, Avg reward: -229.44556729469082\nEpisode: 212, Steps: 98, Reward: -496.8743540743569, Best reward: 38.78506531892563, Avg reward: -231.89257772420694\nEpisode: 213, Steps: 101, Reward: -129.557990536064, Best reward: 38.78506531892563, Avg reward: -232.5088382690766\nEpisode: 214, Steps: 79, Reward: -86.77665266731006, Best reward: 38.78506531892563, Avg reward: -232.7110145761048\nEpisode: 215, Steps: 105, Reward: -16.85036917429204, Best reward: 38.78506531892563, Avg reward: -232.02747794097678\nEpisode: 216, Steps: 133, Reward: -324.3939471255389, Best reward: 38.78506531892563, Avg reward: -234.53931028414914\nEpisode: 217, Steps: 114, Reward: -276.17421592374905, Best reward: 38.78506531892563, Avg reward: -234.06366903077998\nEpisode: 218, Steps: 108, Reward: -68.82699957561076, Best reward: 38.78506531892563, Avg reward: -233.07946146500993\nEpisode: 219, Steps: 83, Reward: -297.5704767648605, Best reward: 38.78506531892563, Avg reward: -233.26954654942168\nEpisode: 220, Steps: 108, Reward: -95.145802363477, Best reward: 38.78506531892563, Avg reward: -230.28040897539682\nEpisode: 221, Steps: 75, Reward: -44.5283772410911, Best reward: 38.78506531892563, Avg reward: -226.45009190170813\nEpisode: 222, Steps: 86, Reward: -63.54352698150745, Best reward: 38.78506531892563, Avg reward: -224.45116032354395\nEpisode: 223, Steps: 152, Reward: -550.8512718457653, Best reward: 38.78506531892563, Avg reward: -226.91545597821678\nEpisode: 224, Steps: 74, Reward: -219.99523465567742, Best reward: 38.78506531892563, Avg reward: -226.17270526903772\nEpisode: 225, Steps: 75, Reward: -65.18393330079658, Best reward: 38.78506531892563, Avg reward: -222.5091293171854\nEpisode: 226, Steps: 124, Reward: -356.3735063657968, Best reward: 38.78506531892563, Avg reward: -221.15242767184606\nEpisode: 227, Steps: 84, Reward: -274.618495782419, Best reward: 38.78506531892563, Avg reward: -218.53466512497812\nEpisode: 228, Steps: 103, Reward: -95.5316450859409, Best reward: 38.78506531892563, Avg reward: -217.7419886395914\nEpisode: 229, Steps: 74, Reward: -269.25953681714304, Best reward: 38.78506531892563, Avg reward: -216.6693773576706\nEpisode: 230, Steps: 92, Reward: -356.42326392971285, Best reward: 38.78506531892563, Avg reward: -216.46605266841635\nEpisode: 231, Steps: 80, Reward: -75.88148041529739, Best reward: 38.78506531892563, Avg reward: -215.08097785757988\nEpisode: 232, Steps: 167, Reward: -115.64943806625405, Best reward: 38.78506531892563, Avg reward: -215.1272905895408\nEpisode: 233, Steps: 113, Reward: -308.5259795146411, Best reward: 38.78506531892563, Avg reward: -215.9129220709232\nEpisode: 234, Steps: 117, Reward: -211.8858736152386, Best reward: 38.78506531892563, Avg reward: -217.11087372159338\nEpisode: 235, Steps: 85, Reward: -169.53097813849843, Best reward: 38.78506531892563, Avg reward: -214.08403434374438\nEpisode: 236, Steps: 215, Reward: 1.342836171885338, Best reward: 38.78506531892563, Avg reward: -212.44866185269987\nEpisode: 237, Steps: 123, Reward: -238.75681275120678, Best reward: 38.78506531892563, Avg reward: -209.80377741381878\nEpisode: 238, Steps: 100, Reward: -420.5313789760066, Best reward: 38.78506531892563, Avg reward: -213.67153397882862\nEpisode: 239, Steps: 157, Reward: -263.31103081879155, Best reward: 38.78506531892563, Avg reward: -212.46764447185416\nEpisode: 240, Steps: 67, Reward: -79.5078869962193, Best reward: 38.78506531892563, Avg reward: -210.2713949987452\nEpisode: 241, Steps: 123, Reward: -444.9717557487823, Best reward: 38.78506531892563, Avg reward: -209.82046006915542\nEpisode: 242, Steps: 116, Reward: -366.3138828705298, Best reward: 38.78506531892563, Avg reward: -212.4601848334331\nEpisode: 243, Steps: 100, Reward: -306.42772190157115, Best reward: 38.78506531892563, Avg reward: -213.0040840597215\nEpisode: 244, Steps: 182, Reward: -32.59642716953513, Best reward: 38.78506531892563, Avg reward: -211.1409578251032\nEpisode: 245, Steps: 88, Reward: -398.66752835946346, Best reward: 38.78506531892563, Avg reward: -215.01654605418952\nEpisode: 246, Steps: 101, Reward: -67.36770051306665, Best reward: 38.78506531892563, Avg reward: -212.55851949439992\nEpisode: 247, Steps: 92, Reward: -257.0102461098213, Best reward: 38.78506531892563, Avg reward: -214.18954621495493\nEpisode: 248, Steps: 85, Reward: -228.55994406462574, Best reward: 38.78506531892563, Avg reward: -213.88753281944267\nEpisode: 249, Steps: 97, Reward: -127.75452105019383, Best reward: 38.78506531892563, Avg reward: -214.3150278413295\nEpisode: 250, Steps: 114, Reward: -100.5946126409811, Best reward: 38.78506531892563, Avg reward: -211.90883639116342\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x6e39600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"name":"stdout","text":"Episode: 251, Steps: 83, Reward: -56.73204346719436, Best reward: 38.78506531892563, Avg reward: -212.07973310234595\nEpisode: 252, Steps: 102, Reward: -246.58018324939079, Best reward: 38.78506531892563, Avg reward: -214.19403943683966\nEpisode: 253, Steps: 104, Reward: -181.55819921097168, Best reward: 38.78506531892563, Avg reward: -212.76487620962382\nEpisode: 254, Steps: 97, Reward: -261.3545053138582, Best reward: 38.78506531892563, Avg reward: -210.92244572740387\nEpisode: 255, Steps: 103, Reward: -517.8692454415619, Best reward: 38.78506531892563, Avg reward: -213.91541926316634\nEpisode: 256, Steps: 116, Reward: -77.83400017800273, Best reward: 38.78506531892563, Avg reward: -211.88132953939427\nEpisode: 257, Steps: 83, Reward: -55.4321342480217, Best reward: 38.78506531892563, Avg reward: -212.11400181129233\nEpisode: 258, Steps: 106, Reward: -392.2021850441712, Best reward: 38.78506531892563, Avg reward: -215.5647168584567\nEpisode: 259, Steps: 96, Reward: -60.79140289572746, Best reward: 38.78506531892563, Avg reward: -212.88374089897738\nEpisode: 260, Steps: 104, Reward: -343.4253091531856, Best reward: 38.78506531892563, Avg reward: -213.66290130677373\nEpisode: 261, Steps: 77, Reward: -181.12134476201214, Best reward: 38.78506531892563, Avg reward: -213.16785930570148\nEpisode: 262, Steps: 139, Reward: -524.5647188925755, Best reward: 38.78506531892563, Avg reward: -213.32730733185662\nEpisode: 263, Steps: 74, Reward: -245.79958335752647, Best reward: 38.78506531892563, Avg reward: -214.24648977569345\nEpisode: 264, Steps: 140, Reward: -222.34886577137513, Best reward: 38.78506531892563, Avg reward: -215.50721860900813\nEpisode: 265, Steps: 139, Reward: -265.2140993337842, Best reward: 38.78506531892563, Avg reward: -215.31056667377428\nEpisode: 266, Steps: 93, Reward: -421.30520927928615, Best reward: 38.78506531892563, Avg reward: -216.53787844778017\nEpisode: 267, Steps: 179, Reward: -185.8774910784932, Best reward: 38.78506531892563, Avg reward: -216.07995259047766\nEpisode: 268, Steps: 111, Reward: -290.6625988701519, Best reward: 38.78506531892563, Avg reward: -216.69400934214354\nEpisode: 269, Steps: 82, Reward: -351.4851875468921, Best reward: 38.78506531892563, Avg reward: -219.3060537639602\nEpisode: 270, Steps: 68, Reward: -124.58632252629857, Best reward: 38.78506531892563, Avg reward: -217.40717838430234\nEpisode: 271, Steps: 97, Reward: -78.16859418984848, Best reward: 38.78506531892563, Avg reward: -214.0505639158178\nEpisode: 272, Steps: 89, Reward: -316.08996888921956, Best reward: 38.78506531892563, Avg reward: -216.60955161779765\nEpisode: 273, Steps: 109, Reward: -386.17213296780653, Best reward: 38.78506531892563, Avg reward: -216.2107636921178\nEpisode: 274, Steps: 107, Reward: -127.77008509688727, Best reward: 38.78506531892563, Avg reward: -216.95281129151755\nEpisode: 275, Steps: 92, Reward: -171.137504931234, Best reward: 38.78506531892563, Avg reward: -219.05203699401915\nEpisode: 276, Steps: 129, Reward: -227.17602903220737, Best reward: 38.78506531892563, Avg reward: -220.39233965031326\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/594937315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         trainer = Trainer(env, gamma, alpha, beta, batch_size, tau, noe,\n\u001b[1;32m     38\u001b[0m                                           max_steps, is_tg, tg_bot_freq_epi, record, mem_size, noise, chkpt)\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_rl_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sac_episode_rewards.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/4219281391.py\u001b[0m in \u001b[0;36mtrain_rl_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1143894710.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mq1_new_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_policy_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mq2_new_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_policy_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             critic_value = tf.squeeze(\n\u001b[1;32m    112\u001b[0m                                 tf.math.minimum(q1_new_pi, q2_new_pi), 1)\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 ):\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3200085250.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                         \u001b[0m_name_scope_unnester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope_on_declaration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                     )\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0mnamescope_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6954\u001b[0m   \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_exit_fns\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6956\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6957\u001b[0m     \"\"\"Initialize the context manager.\n\u001b[1;32m   6958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":" \n    \n    \n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}