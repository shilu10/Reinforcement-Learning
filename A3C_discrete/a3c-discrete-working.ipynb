{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "bETUFZq-wePe"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari] --quiet\n",
    "!pip install gymnasium --quiet\n",
    "!pip install -U gymnasium[atari] --quiet\n",
    "!pip install imageio_ffmpeg --quiet\n",
    "!pip install npy_append_array --quiet\n",
    "!pip install pyTelegramBotAPI --quiet\n",
    "!pip install gymnasium[accept-rom-license] --quiet\n",
    "!!pip install gymnasium[box2d] --quiet\n",
    "!pip install gym-super-mario-bros --quiet\n",
    "!pip install minigrid --quiet\n",
    "!pip install miniworld --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:44:59.590197Z",
     "iopub.status.busy": "2023-04-11T03:44:59.589754Z",
     "iopub.status.idle": "2023-04-11T03:45:02.448831Z",
     "shell.execute_reply": "2023-04-11T03:45:02.447688Z",
     "shell.execute_reply.started": "2023-04-11T03:44:59.590154Z"
    },
    "id": "SYJzThpJx_8e"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:45:02.731013Z",
     "iopub.status.busy": "2023-04-11T03:45:02.729547Z",
     "iopub.status.idle": "2023-04-11T03:45:02.745288Z",
     "shell.execute_reply": "2023-04-11T03:45:02.744075Z",
     "shell.execute_reply.started": "2023-04-11T03:45:02.730964Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class ExperienceReplayBuffer: \n",
    "    def __init__(self, max_memory, input_shape, batch_size, cer=False): \n",
    "        self.mem_size = max_memory\n",
    "        self.mem_counter = 0\n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.cer = cer\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done): \n",
    "        index = self.mem_counter % self.mem_size \n",
    "\n",
    "        self.state_memory.append(state)\n",
    "        self.next_state_memory.append(next_state)\n",
    "        self.reward_memory.append(reward)\n",
    "        self.action_memory.append(action)\n",
    "        self.terminal_memory.append(done)\n",
    "   #     self.action_probs_memory[index] = action_probs\n",
    "        self.mem_counter += 1\n",
    "\n",
    "    def sample_experience(self, batch_size):\n",
    "        # used to get the last transition\n",
    "        offset = 1 if self.cer else 0\n",
    "\n",
    "        max_mem = min(self.mem_counter, self.mem_size) - offset\n",
    "        batch_index = np.random.choice(max_mem, batch_size - offset, replace=False)\n",
    "\n",
    "        states = self.state_memory[: ]\n",
    "        next_states = self.next_state_memory[: ]\n",
    "        rewards = self.reward_memory[: ]\n",
    "        actions = self.action_memory[: ]\n",
    "        terminals = self.terminal_memory[: ]\n",
    "    #    action_probs = self.action_probs_memory[batch_index]\n",
    "\n",
    "        if self.cer: \n",
    "            last_index = self.mem_counter % self.mem_size - 1\n",
    "            last_state = self.state_memory[last_index]\n",
    "            last_action = self.action_memory[last_index]\n",
    "            last_terminal = self.terminal_memory[last_index]\n",
    "            last_next_state = self.next_state_memory[last_index]\n",
    "            last_reward = self.reward_memory[last_index]\n",
    "\n",
    "            # for 2d and 3d use vstack to append, for 1d array use append() to append the data\n",
    "            states = np.vstack((self.state_memory[batch_index], last_state))\n",
    "            next_states = np.vstack((self.next_state_memory[batch_index], last_next_state))\n",
    "\n",
    "            actions = np.append(actions, last_action)\n",
    "            terminals = np.append(terminals, last_terminal)\n",
    "            rewards = np.append(rewards, last_reward)\n",
    "    \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminals)\n",
    "    \n",
    "    \n",
    "    def is_sufficient(self): \n",
    "        return self.mem_counter > self.batch_size\n",
    "    \n",
    "    def make_empty(self): \n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPM5Ptfjw799"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Flatten\n",
    " \n",
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dims, action_dim=1):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "     #   self.conv1 = Conv2D(64, 3, activation=\"relu\", input_shape=input_dims)\n",
    "      #  self.conv2 = Conv2D(64, 3, activation=\"relu\")\n",
    "       # self.flatten = Flatten()\n",
    "        self.fc1 = Dense(64, activation=\"relu\")\n",
    "        self.fc2 = Dense(32, activation=\"relu\")\n",
    "        self.fc3 = Dense(action_dim, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dims, action_dim=1):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "       # self.conv1 = Conv2D(64, 3, activation=\"relu\", kernel_initializer=\"he_uniform\", input_shape=input_dims)\n",
    "       # self.conv2 = Conv2D(64, 3, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "       # self.flatten = Flatten()\n",
    "        self.fc1 = Dense(64, activation=\"relu\",  kernel_initializer=\"he_uniform\")\n",
    "        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.fc3 = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goC4wfIYw8eI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:54:43.752926Z",
     "iopub.status.busy": "2023-04-11T03:54:43.751949Z",
     "iopub.status.idle": "2023-04-11T03:54:43.768351Z",
     "shell.execute_reply": "2023-04-11T03:54:43.767086Z",
     "shell.execute_reply.started": "2023-04-11T03:54:43.752872Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Lambda\n",
    "import numpy as np \n",
    "from tensorflow.keras.optimizers import Adam\n",
    " \n",
    "class Actor:\n",
    "    def __init__(self, input_dims, action_dim, actor_lr): \n",
    "        self.input_dims = input_dims \n",
    "        self.action_dim = action_dim\n",
    "        self.actor_lr = actor_lr \n",
    "        self.model = None\n",
    "        \n",
    "    def build_network(self): \n",
    "        input_ = tf.keras.layers.Input(self.input_dims)\n",
    "        x = Dense(64, activation=\"relu\")(input_)\n",
    "        x = Dense(64, activation=\"relu\")(input_)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        output_ = Dense(self.action_dim, activation=None)(x)\n",
    "        model = keras.models.Model(input_, output_)\n",
    "        model.compile(Adam(learning_rate=self.actor_lr))\n",
    "        \n",
    "        self.model = model \n",
    "        \n",
    "    def get_action(self, state): \n",
    "        state = tf.convert_to_tensor(np.reshape(state, (1, -1)), dtype=tf.float32)\n",
    "        action_probabilities = self.model(state)\n",
    "        action_probabilities = tf.nn.softmax(action_probabilities)\n",
    "        action_probabilities = action_probabilities.numpy()\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs=action_probabilities, dtype=tf.float32\n",
    "        )\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "\n",
    "    def learn(self, states, action, td_target, baselines):   \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape: \n",
    "            \n",
    "            logits = self.model(states)\n",
    "            action_probs = tf.nn.softmax(logits)\n",
    "            actor_loss = self.actor_loss(action_probs, action, td_target, baselines)\n",
    "\n",
    "        actor_params = self.model.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "\n",
    "        self.model.optimizer.apply_gradients(zip(actor_grads, actor_params))  \n",
    "        return actor_loss\n",
    "\n",
    "    def actor_loss(self, action_probs, action, td, baselines): \n",
    "        baselines = tf.cast(baselines, dtype=tf.float32)\n",
    "        baselines = tf.squeeze(baselines)\n",
    "        action_probs = tfp.distributions.Categorical(probs=action_probs)\n",
    "        log_prob = action_probs.log_prob(action)\n",
    "        loss = -log_prob * tf.convert_to_tensor(td - baselines, dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    def save_model(self): \n",
    "        self.model.save(\"models/a3c/\" + \"actor_network\")  \n",
    "        print(\"Model Saved Successfully.\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:54:44.522311Z",
     "iopub.status.busy": "2023-04-11T03:54:44.521852Z",
     "iopub.status.idle": "2023-04-11T03:54:44.536144Z",
     "shell.execute_reply": "2023-04-11T03:54:44.534923Z",
     "shell.execute_reply.started": "2023-04-11T03:54:44.522272Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam \n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras \n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, input_dims, action_dim, critic_lr): \n",
    "        self.input_dims = input_dims \n",
    "        self.action_dim = action_dim\n",
    "        self.critic_lr = critic_lr\n",
    "        self.model = None\n",
    "        \n",
    "    def build_network(self): \n",
    "        input_ = tf.keras.layers.Input(self.input_dims)\n",
    "        x = Dense(64, activation=\"relu\")(input_)\n",
    "        x = Dense(64, activation=\"relu\")(input_)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        output_ = Dense(1, activation=None)(x)\n",
    "        model = keras.models.Model(input_, output_)\n",
    "        model.compile(Adam(learning_rate=self.critic_lr))\n",
    "        self.model = model\n",
    "    \n",
    "    def load_model(self): \n",
    "        self.model = tf.keras.models.load_model(self.fname + \"_actor_network\")\n",
    "        print(\"loaded the model\")\n",
    "        \n",
    "    def critic_loss(self, value, reward, next_state_value, gamma, done):\n",
    "        value = tf.cast(value, dtype=tf.float32)\n",
    "        next_state_value = tf.cast(next_state_value, dtype=tf.float32)\n",
    "        next_state_value = tf.squeeze(next_state_value)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        value = tf.squeeze(value)\n",
    "        td_target = reward + gamma * next_state_value * tf.convert_to_tensor([1-int(d) for d in done], dtype=tf.float32)\n",
    "        delta = tf.keras.losses.MSE(td_target, value)\n",
    "        return tf.convert_to_tensor(delta, dtype=tf.float32), tf.convert_to_tensor(td_target, dtype=tf.float32)\n",
    "    \n",
    "    def save_model(self): \n",
    "        self.model.save(\"models/a3c/\" + \"critic_network\") \n",
    "    \n",
    "    def learn(self, state, next_state_v_value, reward, done, gamma): \n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        #next_state = tf.convert_to_tensor(next_state, dtype=tf.float32)  \n",
    "        td_target = None\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            v_value = self.model(state)\n",
    "            v_target = next_state_v_value\n",
    "\n",
    "            critic_loss, td = self.critic_loss(v_value, reward, v_target, gamma, done)\n",
    "            \n",
    "        critic_params = self.model.trainable_variables \n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.model.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "        \n",
    "        td_target = td\n",
    "        return  critic_loss, td_target, v_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:54:44.700170Z",
     "iopub.status.busy": "2023-04-11T03:54:44.699467Z",
     "iopub.status.idle": "2023-04-11T03:54:44.709261Z",
     "shell.execute_reply": "2023-04-11T03:54:44.708078Z",
     "shell.execute_reply.started": "2023-04-11T03:54:44.700131Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalNetwork: \n",
    "    def __init__(self, input_dims, action_dims, actor_lr, critic_lr): \n",
    "        self.actor = Actor(input_dims, action_dims, actor_lr)        \n",
    "        self.critic= Critic(input_dims, action_dims, critic_lr)\n",
    "        \n",
    "    def update_global_params(self, state, next_state_v_value, actions, reward, done, gamma):\n",
    "        critic_l, td_target, v_value = self.critic.learn(state, next_state_v_value, reward, done, gamma)\n",
    "        actor_l = self.actor.learn(state, actions, td_target, baselines=v_value)\n",
    "        \n",
    "        return actor_l , critic_l\n",
    "        \n",
    "    def pull_global_params(self): \n",
    "        actor_params = self.actor.model.get_weights()\n",
    "        critic_params = self.critic.model.get_weights()\n",
    "        \n",
    "        return actor_params, critic_params        \n",
    "    \n",
    "    def save_models(self): \n",
    "        pass \n",
    "    \n",
    "    def load_models(self):\n",
    "        pass \n",
    "\n",
    "class WorkerNetwork: \n",
    "    def __init__(self , input_dims, action_dims, actor_lr, critic_lr): \n",
    "        self.actor = Actor(input_dims, action_dims, actor_lr)     \n",
    "        self.critic = Critic(input_dims, action_dims, critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:56:46.189053Z",
     "iopub.status.busy": "2023-04-11T03:56:46.188663Z",
     "iopub.status.idle": "2023-04-11T03:56:46.204426Z",
     "shell.execute_reply": "2023-04-11T03:56:46.203276Z",
     "shell.execute_reply.started": "2023-04-11T03:56:46.189018Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "# global_networks will be created in the A3C Agent\n",
    "# global_network = GlobalNetwork()\n",
    "\n",
    "CURR_EPISODE = 0\n",
    "BEST_REWARD = float(\"-inf\")\n",
    "EPISODE_REWARDS = []\n",
    "\n",
    "class A3CWorker: \n",
    "    def __init__(self, env, noe, gamma, update_interval, global_network, worker_network, input_dims): \n",
    "        self.env = env \n",
    "        self.noe = noe\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_interval\n",
    "        \n",
    "        \n",
    "        self.global_network = global_network\n",
    "        self.worker_network = worker_network\n",
    "        \n",
    "        actor_weights, critic_weights = self.global_network.pull_global_params()\n",
    "        self.worker_network.actor.model.set_weights(actor_weights)\n",
    "        self.worker_network.critic.model.set_weights(critic_weights)\n",
    "        \n",
    "        self.memory = ExperienceReplayBuffer(self.update_interval, input_dims, self.update_interval, False)\n",
    "        \n",
    "    \n",
    "    def learn(self): \n",
    "        \n",
    "        global CURR_EPISODE\n",
    "        global BEST_REWARD\n",
    "        global EPISODE_REWARDS\n",
    "        \n",
    "        while CURR_EPISODE <= self.noe: \n",
    "            state = self.env.reset()\n",
    "            \n",
    "            done = False \n",
    "            episodic_reward = 0\n",
    "            step = 0\n",
    "            while not done:  \n",
    "                \n",
    "                if type(state) == tuple: \n",
    "                    state = state[0]\n",
    "                    \n",
    "                action = self.worker_network.actor.get_action(state)\n",
    "                next_state_info = self.env.step(action)\n",
    "                    \n",
    "                next_state, reward_prob, terminated, truncated, _ = next_state_info \n",
    "                done = terminated or truncated \n",
    "                    \n",
    "                if (step % self.update_interval == 0 and step!=0) or done:\n",
    "                    states, actions, rewards, next_states, dones = self.memory.sample_experience(self.update_interval)\n",
    "                    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "\n",
    "                    next_state_v_value = self.worker_network.critic.model(next_states)\n",
    "                    action_probs = self.worker_network.actor.model(states)\n",
    "\n",
    "                    actor_l, critic_l = self.global_network.update_global_params(states, next_state_v_value, \n",
    "                                                                                 actions, rewards, dones, self.gamma)\n",
    "                                 \n",
    "                    actor_weights, critic_weights = self.global_network.pull_global_params()\n",
    "                    self.worker_network.actor.model.set_weights(actor_weights)\n",
    "                    self.worker_network.critic.model.set_weights(critic_weights)\n",
    "                        \n",
    "                    self.memory.make_empty()\n",
    "                \n",
    "                self.memory.store_experience(state, action, reward_prob, next_state, done)\n",
    "                step += 1\n",
    "                episodic_reward += reward_prob\n",
    "                state  = next_state\n",
    "            \n",
    "            EPISODE_REWARDS.append(episodic_reward)\n",
    "            avg_reward = np.mean(EPISODE_REWARDS[-100: ])\n",
    "            \n",
    "            if episodic_reward > BEST_REWARD: \n",
    "                self.global_network.actor.save_model()\n",
    "                self.global_network.critic.save_model()\n",
    "                BEST_REWARD = episodic_reward\n",
    "                \n",
    "            print(f\"Episode: {CURR_EPISODE}, Reward: {episodic_reward} Average Reward: {avg_reward} Best Reward: {BEST_REWARD}\")\n",
    "            \n",
    "            CURR_EPISODE += 1 \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:56:46.607231Z",
     "iopub.status.busy": "2023-04-11T03:56:46.606077Z",
     "iopub.status.idle": "2023-04-11T03:56:46.633948Z",
     "shell.execute_reply": "2023-04-11T03:56:46.632776Z",
     "shell.execute_reply.started": "2023-04-11T03:56:46.607187Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread \n",
    "from multiprocessing import cpu_count \n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, actor_lr, critic_lr, gamma, update_interval, noe, worker_count=cpu_count()):\n",
    "        self.input_dims = env.observation_space.shape\n",
    "        action_space = [_ for _ in range((env.action_space.n))]\n",
    "        self.out_dims = len(action_space)\n",
    "        self.worker_count = worker_count\n",
    "        self.noe = noe\n",
    "        self.update_interval = update_interval\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_lr \n",
    "        self.critic_lr = critic_lr\n",
    "        self.global_network = GlobalNetwork(self.input_dims, self.out_dims, actor_lr, critic_lr)\n",
    "        self.worker_network = WorkerNetwork(self.input_dims, self.out_dims, actor_lr, critic_lr)\n",
    "        self.global_network.actor.build_network()\n",
    "        self.global_network.critic.build_network()\n",
    "        self.worker_network.actor.build_network()\n",
    "        self.worker_network.critic.build_network()\n",
    "        \n",
    "    def learn(self): \n",
    "        workers = []\n",
    "        for _ in range(self.worker_count): \n",
    "            a3c_worker = A3CWorker(self.env, self.noe, self.gamma, \n",
    "                                           self.update_interval, self.global_network, self.worker_network, self.input_dims)\n",
    "            workers.append(Thread(target=a3c_worker.learn()))\n",
    "        \n",
    "        for worker in workers: \n",
    "            worker.start()\n",
    "        \n",
    "        for worker in workers: \n",
    "            worker.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T03:56:47.140939Z",
     "iopub.status.busy": "2023-04-11T03:56:47.139755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully.\n",
      "Episode: 0, Reward: 24.0 Average Reward: 24.0 Best Reward: 24.0\n",
      "Episode: 1, Reward: 20.0 Average Reward: 22.0 Best Reward: 24.0\n",
      "Episode: 2, Reward: 11.0 Average Reward: 18.333333333333332 Best Reward: 24.0\n",
      "Episode: 3, Reward: 9.0 Average Reward: 16.0 Best Reward: 24.0\n",
      "Episode: 4, Reward: 17.0 Average Reward: 16.2 Best Reward: 24.0\n",
      "Episode: 5, Reward: 9.0 Average Reward: 15.0 Best Reward: 24.0\n",
      "Episode: 6, Reward: 8.0 Average Reward: 14.0 Best Reward: 24.0\n",
      "Episode: 7, Reward: 12.0 Average Reward: 13.75 Best Reward: 24.0\n",
      "Episode: 8, Reward: 9.0 Average Reward: 13.222222222222221 Best Reward: 24.0\n",
      "Episode: 9, Reward: 10.0 Average Reward: 12.9 Best Reward: 24.0\n",
      "Episode: 10, Reward: 12.0 Average Reward: 12.818181818181818 Best Reward: 24.0\n",
      "Episode: 11, Reward: 10.0 Average Reward: 12.583333333333334 Best Reward: 24.0\n",
      "Episode: 12, Reward: 12.0 Average Reward: 12.538461538461538 Best Reward: 24.0\n",
      "Episode: 13, Reward: 8.0 Average Reward: 12.214285714285714 Best Reward: 24.0\n",
      "Episode: 14, Reward: 10.0 Average Reward: 12.066666666666666 Best Reward: 24.0\n",
      "Episode: 15, Reward: 10.0 Average Reward: 11.9375 Best Reward: 24.0\n",
      "Episode: 16, Reward: 9.0 Average Reward: 11.764705882352942 Best Reward: 24.0\n",
      "Episode: 17, Reward: 10.0 Average Reward: 11.666666666666666 Best Reward: 24.0\n",
      "Episode: 18, Reward: 8.0 Average Reward: 11.473684210526315 Best Reward: 24.0\n",
      "Episode: 19, Reward: 9.0 Average Reward: 11.35 Best Reward: 24.0\n",
      "Episode: 20, Reward: 8.0 Average Reward: 11.19047619047619 Best Reward: 24.0\n",
      "Episode: 21, Reward: 9.0 Average Reward: 11.090909090909092 Best Reward: 24.0\n",
      "Episode: 22, Reward: 9.0 Average Reward: 11.0 Best Reward: 24.0\n",
      "Episode: 23, Reward: 9.0 Average Reward: 10.916666666666666 Best Reward: 24.0\n",
      "Episode: 24, Reward: 8.0 Average Reward: 10.8 Best Reward: 24.0\n",
      "Episode: 25, Reward: 10.0 Average Reward: 10.76923076923077 Best Reward: 24.0\n",
      "Episode: 26, Reward: 10.0 Average Reward: 10.74074074074074 Best Reward: 24.0\n",
      "Episode: 27, Reward: 9.0 Average Reward: 10.678571428571429 Best Reward: 24.0\n",
      "Episode: 28, Reward: 10.0 Average Reward: 10.655172413793103 Best Reward: 24.0\n",
      "Episode: 29, Reward: 10.0 Average Reward: 10.633333333333333 Best Reward: 24.0\n",
      "Episode: 30, Reward: 9.0 Average Reward: 10.580645161290322 Best Reward: 24.0\n",
      "Episode: 31, Reward: 10.0 Average Reward: 10.5625 Best Reward: 24.0\n",
      "Episode: 32, Reward: 8.0 Average Reward: 10.484848484848484 Best Reward: 24.0\n",
      "Episode: 33, Reward: 9.0 Average Reward: 10.441176470588236 Best Reward: 24.0\n",
      "Episode: 34, Reward: 9.0 Average Reward: 10.4 Best Reward: 24.0\n",
      "Episode: 35, Reward: 9.0 Average Reward: 10.36111111111111 Best Reward: 24.0\n",
      "Episode: 36, Reward: 10.0 Average Reward: 10.35135135135135 Best Reward: 24.0\n",
      "Episode: 37, Reward: 8.0 Average Reward: 10.289473684210526 Best Reward: 24.0\n",
      "Episode: 38, Reward: 10.0 Average Reward: 10.282051282051283 Best Reward: 24.0\n",
      "Episode: 39, Reward: 9.0 Average Reward: 10.25 Best Reward: 24.0\n",
      "Episode: 40, Reward: 11.0 Average Reward: 10.268292682926829 Best Reward: 24.0\n",
      "Episode: 41, Reward: 8.0 Average Reward: 10.214285714285714 Best Reward: 24.0\n",
      "Episode: 42, Reward: 10.0 Average Reward: 10.209302325581396 Best Reward: 24.0\n",
      "Episode: 43, Reward: 10.0 Average Reward: 10.204545454545455 Best Reward: 24.0\n",
      "Episode: 44, Reward: 9.0 Average Reward: 10.177777777777777 Best Reward: 24.0\n",
      "Episode: 45, Reward: 10.0 Average Reward: 10.173913043478262 Best Reward: 24.0\n",
      "Episode: 46, Reward: 9.0 Average Reward: 10.148936170212766 Best Reward: 24.0\n",
      "Episode: 47, Reward: 12.0 Average Reward: 10.1875 Best Reward: 24.0\n",
      "Episode: 48, Reward: 12.0 Average Reward: 10.224489795918368 Best Reward: 24.0\n",
      "Episode: 49, Reward: 10.0 Average Reward: 10.22 Best Reward: 24.0\n",
      "Episode: 50, Reward: 9.0 Average Reward: 10.196078431372548 Best Reward: 24.0\n",
      "Episode: 51, Reward: 10.0 Average Reward: 10.192307692307692 Best Reward: 24.0\n",
      "Episode: 52, Reward: 10.0 Average Reward: 10.18867924528302 Best Reward: 24.0\n",
      "Episode: 53, Reward: 9.0 Average Reward: 10.166666666666666 Best Reward: 24.0\n",
      "Episode: 54, Reward: 21.0 Average Reward: 10.363636363636363 Best Reward: 24.0\n",
      "Episode: 55, Reward: 17.0 Average Reward: 10.482142857142858 Best Reward: 24.0\n",
      "Episode: 56, Reward: 12.0 Average Reward: 10.508771929824562 Best Reward: 24.0\n",
      "Episode: 57, Reward: 24.0 Average Reward: 10.741379310344827 Best Reward: 24.0\n",
      "Episode: 58, Reward: 15.0 Average Reward: 10.813559322033898 Best Reward: 24.0\n",
      "Episode: 59, Reward: 9.0 Average Reward: 10.783333333333333 Best Reward: 24.0\n",
      "Episode: 60, Reward: 10.0 Average Reward: 10.770491803278688 Best Reward: 24.0\n",
      "Episode: 61, Reward: 12.0 Average Reward: 10.790322580645162 Best Reward: 24.0\n",
      "Episode: 62, Reward: 9.0 Average Reward: 10.761904761904763 Best Reward: 24.0\n",
      "Episode: 63, Reward: 9.0 Average Reward: 10.734375 Best Reward: 24.0\n",
      "Episode: 64, Reward: 8.0 Average Reward: 10.692307692307692 Best Reward: 24.0\n",
      "Episode: 65, Reward: 11.0 Average Reward: 10.696969696969697 Best Reward: 24.0\n",
      "Episode: 66, Reward: 9.0 Average Reward: 10.671641791044776 Best Reward: 24.0\n",
      "Episode: 67, Reward: 8.0 Average Reward: 10.632352941176471 Best Reward: 24.0\n",
      "Episode: 68, Reward: 10.0 Average Reward: 10.623188405797102 Best Reward: 24.0\n",
      "Episode: 69, Reward: 8.0 Average Reward: 10.585714285714285 Best Reward: 24.0\n",
      "Episode: 70, Reward: 12.0 Average Reward: 10.605633802816902 Best Reward: 24.0\n",
      "Episode: 71, Reward: 10.0 Average Reward: 10.597222222222221 Best Reward: 24.0\n",
      "Episode: 72, Reward: 10.0 Average Reward: 10.58904109589041 Best Reward: 24.0\n",
      "Episode: 73, Reward: 9.0 Average Reward: 10.567567567567568 Best Reward: 24.0\n",
      "Episode: 74, Reward: 9.0 Average Reward: 10.546666666666667 Best Reward: 24.0\n",
      "Episode: 75, Reward: 8.0 Average Reward: 10.513157894736842 Best Reward: 24.0\n",
      "Episode: 76, Reward: 10.0 Average Reward: 10.506493506493506 Best Reward: 24.0\n",
      "Episode: 77, Reward: 10.0 Average Reward: 10.5 Best Reward: 24.0\n",
      "Episode: 78, Reward: 9.0 Average Reward: 10.481012658227849 Best Reward: 24.0\n",
      "Episode: 79, Reward: 10.0 Average Reward: 10.475 Best Reward: 24.0\n",
      "Episode: 80, Reward: 10.0 Average Reward: 10.469135802469136 Best Reward: 24.0\n",
      "Episode: 81, Reward: 8.0 Average Reward: 10.439024390243903 Best Reward: 24.0\n",
      "Episode: 82, Reward: 9.0 Average Reward: 10.421686746987952 Best Reward: 24.0\n",
      "Episode: 83, Reward: 9.0 Average Reward: 10.404761904761905 Best Reward: 24.0\n",
      "Episode: 84, Reward: 11.0 Average Reward: 10.411764705882353 Best Reward: 24.0\n",
      "Episode: 85, Reward: 8.0 Average Reward: 10.383720930232558 Best Reward: 24.0\n",
      "Episode: 86, Reward: 8.0 Average Reward: 10.35632183908046 Best Reward: 24.0\n",
      "Episode: 87, Reward: 10.0 Average Reward: 10.352272727272727 Best Reward: 24.0\n",
      "Episode: 88, Reward: 10.0 Average Reward: 10.348314606741573 Best Reward: 24.0\n",
      "Episode: 89, Reward: 9.0 Average Reward: 10.333333333333334 Best Reward: 24.0\n",
      "Episode: 90, Reward: 10.0 Average Reward: 10.32967032967033 Best Reward: 24.0\n",
      "Episode: 91, Reward: 10.0 Average Reward: 10.326086956521738 Best Reward: 24.0\n",
      "Episode: 92, Reward: 8.0 Average Reward: 10.301075268817204 Best Reward: 24.0\n",
      "Episode: 93, Reward: 9.0 Average Reward: 10.287234042553191 Best Reward: 24.0\n",
      "Episode: 94, Reward: 10.0 Average Reward: 10.284210526315789 Best Reward: 24.0\n",
      "Episode: 95, Reward: 9.0 Average Reward: 10.270833333333334 Best Reward: 24.0\n",
      "Episode: 96, Reward: 12.0 Average Reward: 10.288659793814434 Best Reward: 24.0\n",
      "Episode: 97, Reward: 12.0 Average Reward: 10.306122448979592 Best Reward: 24.0\n",
      "Episode: 98, Reward: 9.0 Average Reward: 10.292929292929292 Best Reward: 24.0\n",
      "Episode: 99, Reward: 13.0 Average Reward: 10.32 Best Reward: 24.0\n",
      "Episode: 100, Reward: 13.0 Average Reward: 10.21 Best Reward: 24.0\n",
      "Episode: 101, Reward: 16.0 Average Reward: 10.17 Best Reward: 24.0\n",
      "Model Saved Successfully.\n",
      "Episode: 102, Reward: 27.0 Average Reward: 10.33 Best Reward: 27.0\n",
      "Episode: 103, Reward: 21.0 Average Reward: 10.45 Best Reward: 27.0\n",
      "Episode: 104, Reward: 20.0 Average Reward: 10.48 Best Reward: 27.0\n",
      "Model Saved Successfully.\n",
      "Episode: 105, Reward: 36.0 Average Reward: 10.75 Best Reward: 36.0\n",
      "Episode: 106, Reward: 27.0 Average Reward: 10.94 Best Reward: 36.0\n",
      "Episode: 107, Reward: 17.0 Average Reward: 10.99 Best Reward: 36.0\n",
      "Episode: 108, Reward: 23.0 Average Reward: 11.13 Best Reward: 36.0\n",
      "Episode: 109, Reward: 10.0 Average Reward: 11.13 Best Reward: 36.0\n",
      "Model Saved Successfully.\n",
      "Episode: 110, Reward: 38.0 Average Reward: 11.39 Best Reward: 38.0\n",
      "Episode: 111, Reward: 19.0 Average Reward: 11.48 Best Reward: 38.0\n",
      "Episode: 112, Reward: 16.0 Average Reward: 11.52 Best Reward: 38.0\n",
      "Episode: 113, Reward: 18.0 Average Reward: 11.62 Best Reward: 38.0\n",
      "Episode: 114, Reward: 11.0 Average Reward: 11.63 Best Reward: 38.0\n",
      "Episode: 115, Reward: 34.0 Average Reward: 11.87 Best Reward: 38.0\n",
      "Episode: 116, Reward: 36.0 Average Reward: 12.14 Best Reward: 38.0\n",
      "Episode: 117, Reward: 9.0 Average Reward: 12.13 Best Reward: 38.0\n",
      "Episode: 118, Reward: 13.0 Average Reward: 12.18 Best Reward: 38.0\n",
      "Episode: 119, Reward: 16.0 Average Reward: 12.25 Best Reward: 38.0\n",
      "Episode: 120, Reward: 21.0 Average Reward: 12.38 Best Reward: 38.0\n",
      "Episode: 121, Reward: 15.0 Average Reward: 12.44 Best Reward: 38.0\n",
      "Episode: 122, Reward: 36.0 Average Reward: 12.71 Best Reward: 38.0\n",
      "Model Saved Successfully.\n",
      "Episode: 123, Reward: 46.0 Average Reward: 13.08 Best Reward: 46.0\n",
      "Episode: 124, Reward: 36.0 Average Reward: 13.36 Best Reward: 46.0\n",
      "Episode: 125, Reward: 23.0 Average Reward: 13.49 Best Reward: 46.0\n",
      "Episode: 126, Reward: 21.0 Average Reward: 13.6 Best Reward: 46.0\n",
      "Episode: 127, Reward: 22.0 Average Reward: 13.73 Best Reward: 46.0\n",
      "Model Saved Successfully.\n",
      "Episode: 128, Reward: 52.0 Average Reward: 14.15 Best Reward: 52.0\n",
      "Model Saved Successfully.\n",
      "Episode: 129, Reward: 88.0 Average Reward: 14.93 Best Reward: 88.0\n",
      "Episode: 130, Reward: 52.0 Average Reward: 15.36 Best Reward: 88.0\n",
      "Episode: 131, Reward: 31.0 Average Reward: 15.57 Best Reward: 88.0\n",
      "Episode: 132, Reward: 60.0 Average Reward: 16.09 Best Reward: 88.0\n",
      "Episode: 133, Reward: 84.0 Average Reward: 16.84 Best Reward: 88.0\n",
      "Episode: 134, Reward: 63.0 Average Reward: 17.38 Best Reward: 88.0\n",
      "Episode: 135, Reward: 28.0 Average Reward: 17.57 Best Reward: 88.0\n",
      "Episode: 136, Reward: 36.0 Average Reward: 17.83 Best Reward: 88.0\n",
      "Episode: 137, Reward: 37.0 Average Reward: 18.12 Best Reward: 88.0\n",
      "Episode: 138, Reward: 27.0 Average Reward: 18.29 Best Reward: 88.0\n",
      "Episode: 139, Reward: 22.0 Average Reward: 18.42 Best Reward: 88.0\n",
      "Episode: 140, Reward: 31.0 Average Reward: 18.62 Best Reward: 88.0\n",
      "Episode: 141, Reward: 63.0 Average Reward: 19.17 Best Reward: 88.0\n",
      "Episode: 142, Reward: 53.0 Average Reward: 19.6 Best Reward: 88.0\n",
      "Episode: 143, Reward: 56.0 Average Reward: 20.06 Best Reward: 88.0\n",
      "Episode: 144, Reward: 46.0 Average Reward: 20.43 Best Reward: 88.0\n",
      "Episode: 145, Reward: 30.0 Average Reward: 20.63 Best Reward: 88.0\n",
      "Episode: 146, Reward: 38.0 Average Reward: 20.92 Best Reward: 88.0\n",
      "Episode: 147, Reward: 25.0 Average Reward: 21.05 Best Reward: 88.0\n",
      "Episode: 148, Reward: 27.0 Average Reward: 21.2 Best Reward: 88.0\n",
      "Episode: 149, Reward: 22.0 Average Reward: 21.32 Best Reward: 88.0\n",
      "Episode: 150, Reward: 31.0 Average Reward: 21.54 Best Reward: 88.0\n",
      "Episode: 151, Reward: 33.0 Average Reward: 21.77 Best Reward: 88.0\n",
      "Episode: 152, Reward: 32.0 Average Reward: 21.99 Best Reward: 88.0\n",
      "Episode: 153, Reward: 76.0 Average Reward: 22.66 Best Reward: 88.0\n",
      "Episode: 154, Reward: 46.0 Average Reward: 22.91 Best Reward: 88.0\n",
      "Episode: 155, Reward: 40.0 Average Reward: 23.14 Best Reward: 88.0\n",
      "Episode: 156, Reward: 65.0 Average Reward: 23.67 Best Reward: 88.0\n",
      "Episode: 157, Reward: 62.0 Average Reward: 24.05 Best Reward: 88.0\n",
      "Model Saved Successfully.\n",
      "Episode: 158, Reward: 97.0 Average Reward: 24.87 Best Reward: 97.0\n",
      "Episode: 159, Reward: 43.0 Average Reward: 25.21 Best Reward: 97.0\n",
      "Episode: 160, Reward: 33.0 Average Reward: 25.44 Best Reward: 97.0\n",
      "Episode: 161, Reward: 51.0 Average Reward: 25.83 Best Reward: 97.0\n",
      "Episode: 162, Reward: 67.0 Average Reward: 26.41 Best Reward: 97.0\n",
      "Episode: 163, Reward: 35.0 Average Reward: 26.67 Best Reward: 97.0\n",
      "Model Saved Successfully.\n",
      "Episode: 164, Reward: 111.0 Average Reward: 27.7 Best Reward: 111.0\n",
      "Episode: 165, Reward: 46.0 Average Reward: 28.05 Best Reward: 111.0\n",
      "Episode: 166, Reward: 43.0 Average Reward: 28.39 Best Reward: 111.0\n",
      "Episode: 167, Reward: 43.0 Average Reward: 28.74 Best Reward: 111.0\n",
      "Episode: 168, Reward: 61.0 Average Reward: 29.25 Best Reward: 111.0\n",
      "Episode: 169, Reward: 54.0 Average Reward: 29.71 Best Reward: 111.0\n",
      "Episode: 170, Reward: 72.0 Average Reward: 30.31 Best Reward: 111.0\n",
      "Episode: 171, Reward: 40.0 Average Reward: 30.61 Best Reward: 111.0\n",
      "Episode: 172, Reward: 104.0 Average Reward: 31.55 Best Reward: 111.0\n",
      "Episode: 173, Reward: 64.0 Average Reward: 32.1 Best Reward: 111.0\n",
      "Episode: 174, Reward: 50.0 Average Reward: 32.51 Best Reward: 111.0\n",
      "Episode: 175, Reward: 46.0 Average Reward: 32.89 Best Reward: 111.0\n",
      "Episode: 176, Reward: 66.0 Average Reward: 33.45 Best Reward: 111.0\n",
      "Model Saved Successfully.\n",
      "Episode: 177, Reward: 129.0 Average Reward: 34.64 Best Reward: 129.0\n",
      "Episode: 178, Reward: 48.0 Average Reward: 35.03 Best Reward: 129.0\n",
      "Episode: 179, Reward: 54.0 Average Reward: 35.47 Best Reward: 129.0\n",
      "Episode: 180, Reward: 110.0 Average Reward: 36.47 Best Reward: 129.0\n",
      "Episode: 181, Reward: 93.0 Average Reward: 37.32 Best Reward: 129.0\n",
      "Episode: 182, Reward: 67.0 Average Reward: 37.9 Best Reward: 129.0\n",
      "Episode: 183, Reward: 63.0 Average Reward: 38.44 Best Reward: 129.0\n",
      "Model Saved Successfully.\n",
      "Episode: 184, Reward: 201.0 Average Reward: 40.34 Best Reward: 201.0\n",
      "Episode: 185, Reward: 106.0 Average Reward: 41.32 Best Reward: 201.0\n",
      "Episode: 186, Reward: 40.0 Average Reward: 41.64 Best Reward: 201.0\n",
      "Episode: 187, Reward: 43.0 Average Reward: 41.97 Best Reward: 201.0\n",
      "Episode: 188, Reward: 61.0 Average Reward: 42.48 Best Reward: 201.0\n",
      "Episode: 189, Reward: 45.0 Average Reward: 42.84 Best Reward: 201.0\n",
      "Episode: 190, Reward: 51.0 Average Reward: 43.25 Best Reward: 201.0\n",
      "Episode: 191, Reward: 59.0 Average Reward: 43.74 Best Reward: 201.0\n",
      "Episode: 192, Reward: 85.0 Average Reward: 44.51 Best Reward: 201.0\n",
      "Episode: 193, Reward: 60.0 Average Reward: 45.02 Best Reward: 201.0\n",
      "Episode: 194, Reward: 38.0 Average Reward: 45.3 Best Reward: 201.0\n",
      "Episode: 195, Reward: 38.0 Average Reward: 45.59 Best Reward: 201.0\n",
      "Episode: 196, Reward: 49.0 Average Reward: 45.96 Best Reward: 201.0\n",
      "Episode: 197, Reward: 69.0 Average Reward: 46.53 Best Reward: 201.0\n",
      "Episode: 198, Reward: 50.0 Average Reward: 46.94 Best Reward: 201.0\n",
      "Episode: 199, Reward: 60.0 Average Reward: 47.41 Best Reward: 201.0\n",
      "Episode: 200, Reward: 42.0 Average Reward: 47.7 Best Reward: 201.0\n",
      "Episode: 201, Reward: 72.0 Average Reward: 48.26 Best Reward: 201.0\n",
      "Episode: 202, Reward: 51.0 Average Reward: 48.5 Best Reward: 201.0\n",
      "Episode: 203, Reward: 46.0 Average Reward: 48.75 Best Reward: 201.0\n",
      "Episode: 204, Reward: 42.0 Average Reward: 48.97 Best Reward: 201.0\n",
      "Episode: 205, Reward: 42.0 Average Reward: 49.03 Best Reward: 201.0\n",
      "Episode: 206, Reward: 57.0 Average Reward: 49.33 Best Reward: 201.0\n",
      "Episode: 207, Reward: 85.0 Average Reward: 50.01 Best Reward: 201.0\n",
      "Episode: 208, Reward: 37.0 Average Reward: 50.15 Best Reward: 201.0\n",
      "Episode: 209, Reward: 55.0 Average Reward: 50.6 Best Reward: 201.0\n",
      "Episode: 210, Reward: 37.0 Average Reward: 50.59 Best Reward: 201.0\n",
      "Episode: 211, Reward: 41.0 Average Reward: 50.81 Best Reward: 201.0\n",
      "Episode: 212, Reward: 46.0 Average Reward: 51.11 Best Reward: 201.0\n",
      "Episode: 213, Reward: 82.0 Average Reward: 51.75 Best Reward: 201.0\n",
      "Episode: 214, Reward: 51.0 Average Reward: 52.15 Best Reward: 201.0\n",
      "Episode: 215, Reward: 56.0 Average Reward: 52.37 Best Reward: 201.0\n",
      "Episode: 216, Reward: 46.0 Average Reward: 52.47 Best Reward: 201.0\n",
      "Episode: 217, Reward: 51.0 Average Reward: 52.89 Best Reward: 201.0\n",
      "Episode: 218, Reward: 46.0 Average Reward: 53.22 Best Reward: 201.0\n",
      "Episode: 219, Reward: 55.0 Average Reward: 53.61 Best Reward: 201.0\n",
      "Episode: 220, Reward: 55.0 Average Reward: 53.95 Best Reward: 201.0\n",
      "Episode: 221, Reward: 72.0 Average Reward: 54.52 Best Reward: 201.0\n",
      "Episode: 222, Reward: 62.0 Average Reward: 54.78 Best Reward: 201.0\n",
      "Episode: 223, Reward: 60.0 Average Reward: 54.92 Best Reward: 201.0\n",
      "Episode: 224, Reward: 41.0 Average Reward: 54.97 Best Reward: 201.0\n",
      "Episode: 225, Reward: 67.0 Average Reward: 55.41 Best Reward: 201.0\n",
      "Episode: 226, Reward: 51.0 Average Reward: 55.71 Best Reward: 201.0\n",
      "Episode: 227, Reward: 142.0 Average Reward: 56.91 Best Reward: 201.0\n",
      "Episode: 228, Reward: 43.0 Average Reward: 56.82 Best Reward: 201.0\n",
      "Episode: 229, Reward: 38.0 Average Reward: 56.32 Best Reward: 201.0\n",
      "Episode: 230, Reward: 116.0 Average Reward: 56.96 Best Reward: 201.0\n",
      "Model Saved Successfully.\n",
      "Episode: 231, Reward: 216.0 Average Reward: 58.81 Best Reward: 216.0\n",
      "Episode: 232, Reward: 43.0 Average Reward: 58.64 Best Reward: 216.0\n",
      "Episode: 233, Reward: 46.0 Average Reward: 58.26 Best Reward: 216.0\n",
      "Episode: 234, Reward: 66.0 Average Reward: 58.29 Best Reward: 216.0\n",
      "Episode: 235, Reward: 184.0 Average Reward: 59.85 Best Reward: 216.0\n",
      "Episode: 236, Reward: 46.0 Average Reward: 59.95 Best Reward: 216.0\n",
      "Episode: 237, Reward: 46.0 Average Reward: 60.04 Best Reward: 216.0\n",
      "Episode: 238, Reward: 58.0 Average Reward: 60.35 Best Reward: 216.0\n",
      "Episode: 239, Reward: 142.0 Average Reward: 61.55 Best Reward: 216.0\n",
      "Episode: 240, Reward: 64.0 Average Reward: 61.88 Best Reward: 216.0\n",
      "Episode: 241, Reward: 153.0 Average Reward: 62.78 Best Reward: 216.0\n",
      "Episode: 242, Reward: 13.0 Average Reward: 62.38 Best Reward: 216.0\n",
      "Episode: 243, Reward: 17.0 Average Reward: 61.99 Best Reward: 216.0\n",
      "Episode: 244, Reward: 44.0 Average Reward: 61.97 Best Reward: 216.0\n",
      "Episode: 245, Reward: 126.0 Average Reward: 62.93 Best Reward: 216.0\n",
      "Episode: 246, Reward: 53.0 Average Reward: 63.08 Best Reward: 216.0\n",
      "Episode: 247, Reward: 44.0 Average Reward: 63.27 Best Reward: 216.0\n",
      "Episode: 248, Reward: 70.0 Average Reward: 63.7 Best Reward: 216.0\n",
      "Episode: 249, Reward: 46.0 Average Reward: 63.94 Best Reward: 216.0\n",
      "Episode: 250, Reward: 57.0 Average Reward: 64.2 Best Reward: 216.0\n",
      "Episode: 251, Reward: 63.0 Average Reward: 64.5 Best Reward: 216.0\n",
      "Episode: 252, Reward: 91.0 Average Reward: 65.09 Best Reward: 216.0\n",
      "Episode: 253, Reward: 58.0 Average Reward: 64.91 Best Reward: 216.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "if __name__ == \"__main__\": \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = ActorCriticAgent(env, 0.003, 0.003, 0.99, 2, 1000, 10)\n",
    "    agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras \n",
    "import tensorflow as tf \n",
    "\n",
    "class ICMNetwork(tf.keras.Model): \n",
    "    def __init__(self, input_dims, out_dims):\n",
    "        super(ICMNetwork, self).__init__()\n",
    "        self.input_dims = input_dims \n",
    "        self.out_dims = out_dims\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding=\"same\", input_shape=input_dims, \n",
    "                                                                 activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.phi = keras.layers.Conv2D(3, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "     #   self.phi = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.inverse = keras.layers.Dense(256, activation='relu', kernel_initializer=\"he_uniform\")\n",
    "        self.pi_logits = keras.layers.Dense(out_dims, activation=\"softmax\")\n",
    "\n",
    "        self.dense1 = keras.layers.Dense(256)\n",
    "        self.phi_hat_next = keras.layers.Dense(7*7*3)\n",
    "        \n",
    "        self.flatten = keras.layers.Flatten()\n",
    "\n",
    "    def call(self, state, next_state, action):\n",
    "        #state = tf.reshape(state, (1, 7, 7, 3))\n",
    "        #next_state = tf.reshape(next_state, (1, 7, 7, 3))\n",
    "        phi = self.conv1(state)\n",
    "        phi = self.conv2(phi)\n",
    "        phi = self.phi(phi)\n",
    "        \n",
    "        phi_next = self.conv1(next_state)\n",
    "        phi_next = self.conv2(phi_next)\n",
    "        phi_next = self.phi(phi_next)\n",
    "        \n",
    "        phi = self.flatten(phi)\n",
    "        phi_next = self.flatten(phi_next)\n",
    "        inverse = self.inverse(tf.concat([phi, phi_next], axis=1))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "        \n",
    "        action = tf.cast(tf.reshape(action, (action.shape[0], 1)), tf.float32)\n",
    "        forward_input = tf.concat([phi, action], axis=1)\n",
    "        dense = self.dense1(forward_input)\n",
    "        phi_hat_next = self.phi_hat_next(dense)\n",
    "        \n",
    "        return phi_next, pi_logits, phi_hat_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "class ICMAgent: \n",
    "    def __init__(self, input_dims, out_dims, alpha, beta, lr):\n",
    "        self.input_dims = input_dims \n",
    "        self.out_dims = out_dims \n",
    "        self.alpha = alpha \n",
    "        self.beta = beta \n",
    "        \n",
    "        self.icm_model = ICMNetwork(self.input_dims, self.out_dims)\n",
    "        self.icm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    def update(self, states, next_states, actions, n_action): \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        actions_one_hot = tf.one_hot(actions, depth=n_action)\n",
    "        \n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "       # print(actions.shape, actions_one_hot, states.shape, next_states.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            phi_next, pi_logits, phi_hat_next = \\\n",
    "                            self.icm_model(states, next_states, actions)\n",
    "            \n",
    "           # pi_logits = tf.cast(pi_logits, dtype=tf.int32)\n",
    "            #print(pi_logits, actions_one_hot)\n",
    "            inverse_loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "            inverse_loss = (1 - self.beta) * inverse_loss_func(pi_logits, actions_one_hot)\n",
    "\n",
    "            forward_loss = tf.keras.losses.MSE(phi_hat_next, phi_next)\n",
    "            forward_loss = self.beta * forward_loss\n",
    "            forward_loss = tf.reduce_mean(forward_loss)\n",
    "           # print(\"forwaed\", forward_loss)\n",
    "\n",
    "            squared = tf.math.pow(phi_hat_next-phi_next, 2)\n",
    "           # print(squared, \"sqa\")\n",
    "            intrinsic_reward = self.alpha*0.5*squared\n",
    "            intrinsic_reward = tf.reduce_sum(intrinsic_reward, axis=1)\n",
    "\n",
    "            loss = inverse_loss + forward_loss\n",
    "            \n",
    "        params = self.icm_model.trainable_variables\n",
    "        grads = tape.gradient(loss, params)\n",
    "        self.icm_model.optimizer.apply_gradients(zip(grads, params))\n",
    "    \n",
    "        return intrinsic_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GcL3VRow8nS"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from gymnasium.wrappers import *\n",
    "\n",
    "\n",
    "def manage_memory():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def plot_learning_curve(scores, figure_file):\n",
    "\n",
    "    x = [_ for _ in range(len(scores))]\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg, label=\"Avg reward for agent\", color=\"black\")\n",
    "    plt.plot(scores, label=\"Reward for agent\", color=\"red\")\n",
    "    plt.xlabel(\"episodes\")\n",
    "    plt.ylabel(\"rewards\")\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.legend()\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "    \n",
    "class RepeatAction(gym.Wrapper):\n",
    "    def __init__(self, env=None, repeat=4, fire_first=False):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.get(\"image\").low[0][0][0]\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.repeat):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, t_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            obs, _, _, _, _ = self.env_step(1)\n",
    "        return obs\n",
    "\n",
    "    \n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.shape = (shape[2], shape[0], shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                                                shape=self.shape,\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
    "                                    interpolation=cv2.INTER_AREA)\n",
    "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0\n",
    "\n",
    "        return new_obs\n",
    "    \n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                env.observation_space.low.repeat(repeat, axis=0),\n",
    "                env.observation_space.high.repeat(repeat, axis=0),\n",
    "                dtype=np.float32)\n",
    "        self.stack = collections.deque(maxlen=repeat)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "    \n",
    "    \n",
    "def make_env(env_name): \n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = RepeatAction(env)\n",
    "#    env = PreprocessFrame(env.observation_space.get(\"image\").shape, env)\n",
    "  #  env = FrameStack(env, 4, lz4_compress=False)\n",
    "  #  env = NormalizeObservation(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5E1iSI07fi-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rq1BvlM7fl6"
   },
   "outputs": [],
   "source": [
    "class Writer:\n",
    "    def __init__(self, fname): \n",
    "        self.fname = fname \n",
    "\n",
    "    def write_to_file(self, content): \n",
    "        with open(self.fname, \"a\") as file: \n",
    "            file.write(content + \"\\n\")\n",
    "\n",
    "    def read_file(self, fname):\n",
    "        with open(fname, \"r\") as file: \n",
    "            return file.read()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNtP-BTO7fpi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAxjX1V87fr4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from telebot import TeleBot\n",
    "import datetime\n",
    "import telebot\n",
    "\n",
    "token = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\n",
    "chat_id = \"1055055706\"\n",
    "bot = TeleBot(token=token) \n",
    "\n",
    "def telegram_send(message, bot):\n",
    "    chat_id = \"1055055706\"\n",
    "    bot.send_message(chat_id=chat_id, text=message)\n",
    "\n",
    "def welcome_msg(multi_step, double_dqn, dueling):\n",
    "    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n",
    "    telegram_send(st, bot)\n",
    "    \n",
    "def info_msg(episode, max_episode, reward, best_score, loss): \n",
    "    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n",
    "    telegram_send(st, bot)\n",
    "\n",
    "def end_msg(learning_time):\n",
    "    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n",
    "    telegram_send(st, bot)\n",
    "    print(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_trkbFOo7fy8"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import imageio\n",
    "\n",
    "\n",
    "class RecordVideo: \n",
    "    \n",
    "    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n",
    "        self.prefix_fname = prefix_fname\n",
    "        self.out_directory = out_directory\n",
    "        self.fps = fps\n",
    "        self.images = []\n",
    "        \n",
    "    def add_image(self, image): \n",
    "        self.images.append(image)\n",
    "    \n",
    "    def save(self, episode_no): \n",
    "        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n",
    "        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n",
    "        self.images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSEo-Nz4w8ph"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, env, action_space, input_dims, out_dims, video_prefix, is_tg,\n",
    "                                     noe, max_steps, record, lr1, lr2, gamma, chkpt, algo_name, update_interval): \n",
    "        self.env = env\n",
    "        self.noe = noe \n",
    "        self.max_steps = max_steps \n",
    "        self.update_interval = update_interval\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.recorder = RecordVideo(video_prefix)\n",
    "        self.is_tg = is_tg \n",
    "        self.record = record\n",
    "        self.agent = ActorCriticAgent(input_dims, out_dims, gamma, lr1, lr2, action_space, 32, chkpt, algo_name)\n",
    "        self.memory = ExperienceReplayBuffer(update_interval, input_dims, update_interval, False)\n",
    "        self.icm_agent = ICMAgent(input_dims, out_dims, 0.1, 0.1, 0.001)\n",
    "\n",
    "    def train(self): \n",
    "\n",
    "        ep_rewards = []\n",
    "        avg_rewards = []\n",
    "        best_reward = float(\"-inf\")\n",
    "        replay_counter = 0\n",
    "\n",
    "        for episode in range(self.noe): \n",
    "           \n",
    "            state, _ = self.env.reset()   \n",
    "            state = state.get('image')\n",
    "            ep_reward = 0 \n",
    "            steps = 0\n",
    "\n",
    "            if self.record and episode % 50 == 0: \n",
    "                img = self.env.render()\n",
    "                self.recorder.add_image(img)\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                \n",
    "                action = self.agent.get_action(state)\n",
    " \n",
    "                next_info = self.env.step(action)\n",
    "\n",
    "                next_state, reward_prob, terminated, truncated, _ = next_info \n",
    "                next_state = next_state.get(\"image\")\n",
    "                done = terminated or truncated\n",
    "                ep_reward += reward_prob\n",
    "                            \n",
    "                if (step % update_interval == 0 and step!=0) or done:\n",
    "                    states, actions, rewards, next_states, dones = self.memory.sample_experience(update_interval)\n",
    "                    \n",
    "                    intrinsic_reward = self.icm_agent.update(states, next_states, actions, self.out_dims)\n",
    "\n",
    "                    self.agent.learn(states, actions, rewards+intrinsic_reward, next_states, dones)\n",
    "                   # print(actions)\n",
    "                    self.memory.make_empty()\n",
    "                self.memory.store_experience(state, action, reward_prob, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                if self.record and episode % 50 == 0:\n",
    "                    img = self.env.render()\n",
    "                    self.recorder.add_image(img)\n",
    "\n",
    "                if done: \n",
    "                    break \n",
    "                \n",
    "            if self.record and episode % 50 == 0:\n",
    "                self.recorder.save(episode)\n",
    "\n",
    "            ep_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(ep_rewards[-100:])\n",
    "            avg_rewards.append(avg_reward)\n",
    "            \n",
    "            print(f\"Episode: {episode} Steps: {steps} Reward: {ep_reward} Best Score: {best_reward}, Average Reward: {avg_reward}\")\n",
    "            \n",
    "            if ep_reward > best_reward: \n",
    "                self.agent.save_models()\n",
    "                best_reward = ep_reward\n",
    "                \n",
    "        return ep_rewards, avg_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "md8-b_oY7s20",
    "outputId": "d9fd7615-a91b-4595-8e72-db2de6ab8e18"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
    "import minigrid\n",
    "\n",
    "#env = gym.make(\"MiniGrid-FourRooms-v0\")\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "action_space = [_ for _ in range((env.action_space.n))]\n",
    "print(env.observation_space)\n",
    "n_actions = len(action_space)\n",
    "input_dims = (7, 7, 3)\n",
    "noe = 10000\n",
    "max_steps = 200\n",
    "video_prefix = \"actor_critic\"\n",
    "is_tg = True \n",
    "record = False\n",
    "lr1 = 0.003\n",
    "lr2 = 0.0005\n",
    "gamma = 0.99\n",
    "chpkt = 'models/'\n",
    "algo_name = \"actor_critic\"\n",
    "update_interval = 2\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trainer = Trainer(env, action_space, input_dims, n_actions, video_prefix, is_tg, \n",
    "                                          noe, max_steps, record, lr1, lr2, gamma, chpkt, algo_name, update_interval)\n",
    "    ep_rewards, _ = trainer.train()\n",
    "    plot_learning_curve(ep_rewards, \"actor_critic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwAbSdSq7s5t"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"actor_critic_eps_rewards.obj\", \"wb\") as f: \n",
    "    pickle.dump(ep_rewards[0], f)\n",
    "\n",
    "with open(\"actor_critic_avg_rewards.obj\", \"wb\") as f: \n",
    "    pickle.dump(ep_rewards[1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "qaS-tMXXV46C",
    "outputId": "9e862b2a-bcea-4d63-afd5-90ce5b9c0084"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(ep_rewards[0], \"actor_critic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOg3R5Lg7s9f"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(observation, q_val_network, action_space): \n",
    "    state = tf.convert_to_tensor([observation])\n",
    "    actions = q_val_network(state)\n",
    "    action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNGxYRRf7s_a"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class Eval: \n",
    "\n",
    "    def __init__(self, env, model_path, number_of_episode=50):\n",
    "        self.env = env \n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.recorder = RecordVideo('dqn_lunarlander', 'test_videos/', 15)\n",
    "        self.number_of_episode = number_of_episode\n",
    "        \n",
    "    def test(self): \n",
    "        rewards = []\n",
    "        steps = []\n",
    "        for episode in range(self.number_of_episode): \n",
    "            done = False\n",
    "            reward = 0\n",
    "            step = 0\n",
    "            state = env.reset(seed=random.randint(0,500))\n",
    "            if episode % 10 == 0: \n",
    "                img = env.render()\n",
    "                self.recorder.add_image(img) \n",
    "\n",
    "            while not done:\n",
    "\n",
    "                if type(state) == tuple: \n",
    "                    state = state[0]\n",
    "                action =  greedy_policy(state, self.model, action_space)\n",
    "                state, reward_prob, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated \n",
    "                reward += reward_prob\n",
    "                step += 1 \n",
    "                if episode % 10 == 0:\n",
    "                    img = env.render()\n",
    "                    self.recorder.add_image(img)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            steps.append(step)\n",
    "            self.recorder.save(1) if episode % 10 == 0 else None \n",
    "        \n",
    "        return rewards, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7k-z8PD2w8rZ",
    "outputId": "af222312-0f70-471d-9291-bfda79ee210b"
   },
   "outputs": [],
   "source": [
    "evaluator = Eval(env, \"/content/models/_actor_critic_actor_network\", 10)\n",
    "evaluator.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAf4X8UuX24T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.states = []\n",
    "        self.new_states = []\n",
    "        self.actions = []\n",
    "\n",
    "    def remember(self, state, action, new_state, reward, value, log_prob):\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.states.append(state)\n",
    "        self.new_states.append(new_state)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.actions = []\n",
    "        self.new_states = []\n",
    "        self.states = []\n",
    "\n",
    "    def sample_memory(self):\n",
    "        return self.states, self.actions, self.new_states, self.rewards,\\\n",
    "               self.values, self.log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "class ParallelEnv:\n",
    "    def __init__(self, env_id, global_idx,\n",
    "                 input_shape, n_actions, num_threads, icm=False):\n",
    "        names = [str(i) for i in range(num_threads)]\n",
    "        print(\"in\")\n",
    "        global_actor_critic = ActorCritic(input_shape, n_actions)\n",
    "        global_actor_critic.share_memory()\n",
    "        global_optim = SharedAdam(global_actor_critic.parameters(), lr=1e-4)\n",
    "\n",
    "        if icm:\n",
    "            global_icm = ICM(input_shape, n_actions)\n",
    "            global_icm.share_memory()\n",
    "            global_icm_optim = SharedAdam(global_icm.parameters(), lr=1e-4)\n",
    "        else:\n",
    "            global_icm = None\n",
    "            global_icm_optim = None\n",
    "\n",
    "        self.ps = [mp.Process(target=worker,\n",
    "                              args=(name, input_shape, n_actions,\n",
    "                                    global_actor_critic, global_optim, env_id,\n",
    "                                    num_threads, global_idx, global_icm,\n",
    "                                    global_icm_optim, icm))\n",
    "                   for name in names]\n",
    "\n",
    "        [p.start() for p in self.ps]\n",
    "        [p.join() for p in self.ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions=3, alpha=0.1, beta=0.2):\n",
    "        super(ICM, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.phi = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.inverse = nn.Linear(288*2, 256)\n",
    "        self.pi_logits = nn.Linear(256, n_actions)\n",
    "\n",
    "        self.dense1 = nn.Linear(288+1, 256)\n",
    "        self.phi_hat_new = nn.Linear(256, 288)\n",
    "\n",
    "        device = T.device('cpu')\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, new_state, action):\n",
    "        conv = F.elu(self.conv1(state))\n",
    "        conv = F.elu(self.conv2(conv))\n",
    "        conv = F.elu(self.conv3(conv))\n",
    "        phi = self.phi(conv)\n",
    "\n",
    "        conv_new = F.elu(self.conv1(new_state))\n",
    "        conv_new = F.elu(self.conv2(conv_new))\n",
    "        conv_new = F.elu(self.conv3(conv_new))\n",
    "        phi_new = self.phi(conv_new)\n",
    "\n",
    "        # [T, 32, 3, 3] to [T, 288]\n",
    "        phi = phi.view(phi.size()[0], -1).to(T.float)\n",
    "        phi_new = phi_new.view(phi_new.size()[0], -1).to(T.float)\n",
    "\n",
    "        inverse = self.inverse(T.cat([phi, phi_new], dim=1))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "\n",
    "        # from [T] to [T, 1]\n",
    "        action = action.reshape((action.size()[0], 1))\n",
    "        forward_input = T.cat([phi, action], dim=1)\n",
    "        dense = self.dense1(forward_input)\n",
    "        phi_hat_new = self.phi_hat_new(dense)\n",
    "\n",
    "        return phi_new, pi_logits, phi_hat_new\n",
    "\n",
    "    def calc_loss(self, states, new_states, actions):\n",
    "        # don't need [] b/c these are lists of states\n",
    "        states = T.tensor(states, dtype=T.float)\n",
    "        actions = T.tensor(actions, dtype=T.float)\n",
    "        new_states = T.tensor(new_states, dtype=T.float)\n",
    "\n",
    "        phi_new, pi_logits, phi_hat_new = \\\n",
    "            self.forward(states, new_states, actions)\n",
    "\n",
    "        inverse_loss = nn.CrossEntropyLoss()\n",
    "        L_I = (1 - self.beta) * inverse_loss(pi_logits, actions.to(T.long))\n",
    "\n",
    "        forward_loss = nn.MSELoss()\n",
    "        L_F = self.beta * forward_loss(phi_hat_new, phi_new)\n",
    "\n",
    "        intrinsic_reward = self.alpha*0.5*((phi_hat_new-phi_new).pow(2)).mean(dim=1)\n",
    "        return intrinsic_reward, L_I, L_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions, gamma=0.99, tau=1.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        conv_shape = self.calc_conv_output(input_dims)\n",
    "\n",
    "        self.gru = nn.GRUCell(conv_shape, 256)\n",
    "        self.pi = nn.Linear(256, n_actions)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "\n",
    "    def calc_conv_output(self, input_dims):\n",
    "        state = T.zeros(1, *input_dims)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        dims = self.conv4(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, state, hx):\n",
    "        conv = F.elu(self.conv1(state))\n",
    "        conv = F.elu(self.conv2(conv))\n",
    "        conv = F.elu(self.conv3(conv))\n",
    "        conv = F.elu(self.conv4(conv))\n",
    "\n",
    "        conv_state = conv.view((conv.size()[0], -1))\n",
    "\n",
    "        hx = self.gru(conv_state, (hx))\n",
    "\n",
    "        pi = self.pi(hx)\n",
    "        v = self.v(hx)\n",
    "\n",
    "        probs = T.softmax(pi, dim=1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.numpy()[0], v, log_prob, hx\n",
    "\n",
    "    def calc_R(self, done, rewards, values):\n",
    "        values = T.cat(values).squeeze()\n",
    "\n",
    "        if len(values.size()) == 1:  # batch of states\n",
    "            R = values[-1]*(1-int(done))\n",
    "        elif len(values.size()) == 0:  # single state\n",
    "            R = values*(1-int(done))\n",
    "\n",
    "        batch_return = []\n",
    "        for reward in rewards[::-1]:\n",
    "            R = reward + self.gamma * R\n",
    "            batch_return.append(R)\n",
    "        batch_return.reverse()\n",
    "        batch_return = T.tensor(batch_return,\n",
    "                                dtype=T.float).reshape(values.size())\n",
    "        return batch_return\n",
    "\n",
    "    def calc_cost(self, new_state, hx, done,\n",
    "                  rewards, values, log_probs, intrinsic_reward=None):\n",
    "\n",
    "        if intrinsic_reward is not None:\n",
    "            rewards += intrinsic_reward.detach().numpy()\n",
    "\n",
    "        returns = self.calc_R(done, rewards, values)\n",
    "\n",
    "        next_v = T.zeros(1, 1) if done else self.forward(T.tensor(\n",
    "                                        [new_state], dtype=T.float), hx)[1]\n",
    "        values.append(next_v.detach())\n",
    "        values = T.cat(values).squeeze()\n",
    "        log_probs = T.cat(log_probs)\n",
    "        rewards = T.tensor(rewards)\n",
    "\n",
    "        delta_t = rewards + self.gamma * values[1:] - values[:-1]\n",
    "        n_steps = len(delta_t)\n",
    "        gae = np.zeros(n_steps)\n",
    "        for t in range(n_steps):\n",
    "            for k in range(0, n_steps-t):\n",
    "                temp = (self.gamma*self.tau)**k * delta_t[t+k]\n",
    "                gae[t] += temp\n",
    "        gae = T.tensor(gae, dtype=T.float)\n",
    "\n",
    "        actor_loss = -(log_probs * gae).sum()\n",
    "        # if single then values is rank 1 and returns rank 0\n",
    "        # want to have same shape to avoid a warning\n",
    "        critic_loss = F.mse_loss(values[:-1].squeeze(), returns)\n",
    "\n",
    "        entropy_loss = (-log_probs * T.exp(log_probs)).sum()\n",
    "\n",
    "        total_loss = actor_loss + critic_loss - 0.01 * entropy_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "\n",
    "\n",
    "class SharedAdam(T.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,\n",
    "                                         weight_decay=weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = T.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = T.zeros_like(p.data)\n",
    "\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "def worker(name, input_shape, n_actions, global_agent,\n",
    "           optimizer, env_id, n_threads, global_idx, global_icm,\n",
    "           icm_optimizer, icm):\n",
    "    T_MAX = 20\n",
    "\n",
    "    local_agent = ActorCritic(input_shape, n_actions)\n",
    "\n",
    "    if icm:\n",
    "        local_icm = ICM(input_shape, n_actions)\n",
    "    else:\n",
    "        local_icm = None\n",
    "        intrinsic_reward = None\n",
    "\n",
    "    memory = Memory()\n",
    "\n",
    "    frame_buffer = [input_shape[1], input_shape[2], 1]\n",
    "    env = make_env(env_id, shape=frame_buffer)\n",
    "\n",
    "    episode, max_steps, t_steps, scores = 0, 5e5, 0, []\n",
    "\n",
    "    while episode < max_steps:\n",
    "        obs = env.reset()\n",
    "        score, done, ep_steps = 0, False, 0\n",
    "        hx = T.zeros(1, 256)\n",
    "        while not done:\n",
    "            state = T.tensor([obs], dtype=T.float)\n",
    "            action, value, log_prob, hx = local_agent(state, hx)\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            memory.remember(obs, action, obs_, reward, value, log_prob)\n",
    "            score += reward\n",
    "            obs = obs_\n",
    "            ep_steps += 1\n",
    "            t_steps += 1\n",
    "            if ep_steps % T_MAX == 0 or done:\n",
    "                states, actions, new_states, rewards, values, log_probs = \\\n",
    "                        memory.sample_memory()\n",
    "                if icm:\n",
    "                    intrinsic_reward, L_I, L_F = \\\n",
    "                            local_icm.calc_loss(states, new_states, actions)\n",
    "\n",
    "                loss = local_agent.calc_cost(obs, hx, done, rewards,\n",
    "                                             values, log_probs,\n",
    "                                             intrinsic_reward)\n",
    "                optimizer.zero_grad()\n",
    "                hx = hx.detach_()\n",
    "                if icm:\n",
    "                    icm_optimizer.zero_grad()\n",
    "                    (L_I + L_F).backward()\n",
    "                loss.backward()\n",
    "                T.nn.utils.clip_grad_norm_(local_agent.parameters(), 40)\n",
    "                for local_param, global_param in zip(\n",
    "                                        local_agent.parameters(),\n",
    "                                        global_agent.parameters()):\n",
    "                    global_param._grad = local_param.grad\n",
    "                optimizer.step()\n",
    "                local_agent.load_state_dict(global_agent.state_dict())\n",
    "\n",
    "                if icm:\n",
    "                    for local_param, global_param in zip(\n",
    "                                        local_icm.parameters(),\n",
    "                                        global_icm.parameters()):\n",
    "                        global_param._grad = local_param.grad\n",
    "                    icm_optimizer.step()\n",
    "                    local_icm.load_state_dict(global_icm.state_dict())\n",
    "\n",
    "                memory.clear_memory()\n",
    "        episode += 1\n",
    "        # with global_idx.get_lock():\n",
    "        #    global_idx.value += 1\n",
    "        if name == '1':\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_score_5000 = np.mean(scores[max(0, episode-5000): episode+1])\n",
    "            print('ICM episode {} thread {} of {} steps {:.2f}M score {:.2f} '\n",
    "                  'avg score (100) (5000) {:.2f} {:.2f}'.format(\n",
    "                                                episode, name, n_threads,\n",
    "                                                t_steps/1e6, score,\n",
    "                                                avg_score, avg_score_5000))\n",
    "    if name == '1':\n",
    "        x = [z for z in range(episode)]\n",
    "        plot_learning_curve(x, scores, 'ICM_hallway_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 games')\n",
    "    plt.savefig(figure_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "mp.set_start_method('spawn')\n",
    "global_ep = mp.Value('i', 0)\n",
    "    # env_id = 'PongNoFrameskip-v4'\n",
    "    # env_id = 'MiniWorld-Hallway-v0'\n",
    "env_id = 'MiniWorld-FourRooms-v0'\n",
    "n_threads = 12\n",
    "n_actions = 3\n",
    "input_shape = [4, 42, 42]\n",
    "env = ParallelEnv(env_id=env_id, num_threads=n_threads,\n",
    "                      n_actions=n_actions, global_idx=global_ep,\n",
    "                      input_shape=input_shape, icm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import cpu_count\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "CUR_EPISODE = 0\n",
    "\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(0.001)\n",
    "        self.entropy_beta = 0.01\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, actions, logits, advantages):\n",
    "        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        entropy_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = ce_loss(\n",
    "            actions, logits, sample_weight=tf.stop_gradient(advantages))\n",
    "        entropy = entropy_loss(logits, logits)\n",
    "        return policy_loss - self.entropy_beta * entropy\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(states, training=True)\n",
    "            loss = self.compute_loss(\n",
    "                actions, logits, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(0.0005)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.global_actor = Actor(self.state_dim, self.action_dim)\n",
    "        self.global_critic = Critic(self.state_dim)\n",
    "        self.num_workers = cpu_count()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            env = gym.make(self.env_name)\n",
    "            workers.append(WorkerAgent(\n",
    "                env, self.global_actor, self.global_critic, max_episodes))\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "\n",
    "class WorkerAgent(Thread):\n",
    "    def __init__(self, env, global_actor, global_critic, max_episodes):\n",
    "        Thread.__init__(self)\n",
    "        self.lock = Lock()\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "\n",
    "        self.max_episodes = max_episodes\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        self.actor = Actor(self.state_dim, self.action_dim)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "\n",
    "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = 0.99 * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "    def advatnage(self, td_targets, baselines):\n",
    "        return td_targets - baselines\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self):\n",
    "        global CUR_EPISODE\n",
    "\n",
    "        while self.max_episodes >= CUR_EPISODE:\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                probs = self.actor.model.predict(\n",
    "                    np.reshape(state, [1, self.state_dim]), verbose=False)\n",
    "                action = np.random.choice(self.action_dim, p=probs[0])\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append(reward)\n",
    "\n",
    "                if len(state_batch) >= 5 or done:\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "\n",
    "                    next_v_value = self.critic.model.predict(next_state, verbose=False)\n",
    "                    td_targets = self.n_step_td_target(\n",
    "                        rewards, next_v_value, done)\n",
    "                    advantages = td_targets - self.critic.model.predict(states, verbose=False)\n",
    "                    \n",
    "                    with self.lock:\n",
    "                        actor_loss = self.global_actor.train(\n",
    "                            states, actions, advantages)\n",
    "                        critic_loss = self.global_critic.train(\n",
    "                            states, td_targets)\n",
    "\n",
    "                        self.actor.model.set_weights(\n",
    "                            self.global_actor.model.get_weights())\n",
    "                        self.critic.model.set_weights(\n",
    "                            self.global_critic.model.get_weights())\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    td_target_batch = []\n",
    "                    advatnage_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(CUR_EPISODE, episode_reward))\n",
    "            CUR_EPISODE += 1\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_name = 'CartPole-v1'\n",
    "    agent = Agent(env_name)\n",
    "    agent.train()\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
