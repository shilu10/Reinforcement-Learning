{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "bETUFZq-wePe"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari] --quiet\n",
    "!pip install gymnasium --quiet\n",
    "!pip install -U gymnasium[atari] --quiet\n",
    "!pip install imageio_ffmpeg --quiet\n",
    "!pip install npy_append_array --quiet\n",
    "!pip install pyTelegramBotAPI --quiet\n",
    "!pip install gymnasium[accept-rom-license] --quiet\n",
    "!!pip install gymnasium[box2d] --quiet\n",
    "!pip install gym-super-mario-bros --quiet\n",
    "!pip install minigrid --quiet\n",
    "!pip install miniworld --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T06:17:09.351923Z",
     "iopub.status.busy": "2023-04-11T06:17:09.351306Z",
     "iopub.status.idle": "2023-04-11T06:17:11.989445Z",
     "shell.execute_reply": "2023-04-11T06:17:11.988298Z",
     "shell.execute_reply.started": "2023-04-11T06:17:09.351876Z"
    },
    "id": "SYJzThpJx_8e"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T06:17:13.220460Z",
     "iopub.status.busy": "2023-04-11T06:17:13.219743Z",
     "iopub.status.idle": "2023-04-11T06:17:13.235515Z",
     "shell.execute_reply": "2023-04-11T06:17:13.234169Z",
     "shell.execute_reply.started": "2023-04-11T06:17:13.220421Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class ExperienceReplayBuffer: \n",
    "    def __init__(self, max_memory, input_shape, batch_size, cer=False): \n",
    "        self.mem_size = max_memory\n",
    "        self.mem_counter = 0\n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.cer = cer\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done): \n",
    "        index = self.mem_counter % self.mem_size \n",
    "\n",
    "        self.state_memory.append(state)\n",
    "        self.next_state_memory.append(next_state)\n",
    "        self.reward_memory.append(reward)\n",
    "        self.action_memory.append(action)\n",
    "        self.terminal_memory.append(done)\n",
    "   #     self.action_probs_memory[index] = action_probs\n",
    "        self.mem_counter += 1\n",
    "\n",
    "    def sample_experience(self, batch_size):\n",
    "        # used to get the last transition\n",
    "        offset = 1 if self.cer else 0\n",
    "\n",
    "        max_mem = min(self.mem_counter, self.mem_size) - offset\n",
    "        batch_index = np.random.choice(max_mem, batch_size - offset, replace=False)\n",
    "\n",
    "        states = self.state_memory[: ]\n",
    "        next_states = self.next_state_memory[: ]\n",
    "        rewards = self.reward_memory[: ]\n",
    "        actions = self.action_memory[: ]\n",
    "        terminals = self.terminal_memory[: ]\n",
    "    #    action_probs = self.action_probs_memory[batch_index]\n",
    "\n",
    "        if self.cer: \n",
    "            last_index = self.mem_counter % self.mem_size - 1\n",
    "            last_state = self.state_memory[last_index]\n",
    "            last_action = self.action_memory[last_index]\n",
    "            last_terminal = self.terminal_memory[last_index]\n",
    "            last_next_state = self.next_state_memory[last_index]\n",
    "            last_reward = self.reward_memory[last_index]\n",
    "\n",
    "            # for 2d and 3d use vstack to append, for 1d array use append() to append the data\n",
    "            states = np.vstack((self.state_memory[batch_index], last_state))\n",
    "            next_states = np.vstack((self.next_state_memory[batch_index], last_next_state))\n",
    "\n",
    "            actions = np.append(actions, last_action)\n",
    "            terminals = np.append(terminals, last_terminal)\n",
    "            rewards = np.append(rewards, last_reward)\n",
    "    \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminals)\n",
    "    \n",
    "    \n",
    "    def is_sufficient(self): \n",
    "        return self.mem_counter > self.batch_size\n",
    "    \n",
    "    def make_empty(self): \n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T06:17:13.468688Z",
     "iopub.status.busy": "2023-04-11T06:17:13.467717Z",
     "iopub.status.idle": "2023-04-11T06:17:13.480095Z",
     "shell.execute_reply": "2023-04-11T06:17:13.479050Z",
     "shell.execute_reply.started": "2023-04-11T06:17:13.468649Z"
    },
    "id": "kPM5Ptfjw799"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Flatten\n",
    " \n",
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dims, action_dim=1):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.conv1 = Conv2D(64, 3, activation=\"relu\", input_shape=input_dims)\n",
    "        self.conv2 = Conv2D(64, 3, activation=\"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(64, activation=\"relu\")\n",
    "        self.fc2 = Dense(32, activation=\"relu\")\n",
    "        self.fc3 = Dense(action_dim, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dims, action_dim=1):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.conv1 = Conv2D(64, 3, activation=\"relu\", kernel_initializer=\"he_uniform\", input_shape=input_dims)\n",
    "        self.conv2 = Conv2D(64, 3, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(64, activation=\"relu\",  kernel_initializer=\"he_uniform\")\n",
    "        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.fc3 = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goC4wfIYw8eI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:52.071554Z",
     "iopub.status.busy": "2023-04-11T07:42:52.071182Z",
     "iopub.status.idle": "2023-04-11T07:42:52.091185Z",
     "shell.execute_reply": "2023-04-11T07:42:52.090219Z",
     "shell.execute_reply.started": "2023-04-11T07:42:52.071521Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Lambda\n",
    "import numpy as np \n",
    "from tensorflow.keras.optimizers import Adam\n",
    " \n",
    "class Actor:\n",
    "    def __init__(self, input_dims, action_dim, actor_lr): \n",
    "        self.input_dims = input_dims \n",
    "        self.action_dim = action_dim\n",
    "        self.actor_lr = actor_lr \n",
    "        self.model = None\n",
    "        self.entropy_beta = 0.001\n",
    "        \n",
    "    def build_network(self): \n",
    "        input_ = tf.keras.layers.Input(self.input_dims)\n",
    "        x = Conv2D(64, 3, activation=\"relu\")(input_)\n",
    "        x = Conv2D(64, 3, activation=\"relu\")(x)\n",
    "        x = Conv2D(32, 3, activation=\"relu\")(x)\n",
    "        \n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        output_ = Dense(self.action_dim, activation=None)(x)\n",
    "        model = keras.models.Model(input_, output_)\n",
    "        model.compile(Adam(learning_rate=self.actor_lr))\n",
    "        \n",
    "        self.model = model \n",
    "        \n",
    "    def get_action(self, state): \n",
    "        state = tf.convert_to_tensor(np.reshape(state, (1, 7, 7, 3)), dtype=tf.float32)\n",
    "        action_probabilities = self.model(state)\n",
    "        action_probabilities = tf.nn.softmax(action_probabilities)\n",
    "        action_probabilities = action_probabilities.numpy()\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs=action_probabilities, dtype=tf.float32\n",
    "        )\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "\n",
    "    def learn(self, states, action, td_target, baselines):   \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape: \n",
    "            \n",
    "            logits = self.model(states)\n",
    "           # action_probs = tf.nn.softmax(logits)\n",
    "            actor_loss = self.actor_loss(logits, action, td_target, baselines)\n",
    "\n",
    "        actor_params = self.model.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "\n",
    "        self.model.optimizer.apply_gradients(zip(actor_grads, actor_params))  \n",
    "        return actor_loss\n",
    "\n",
    "    def actor_loss(self, logits, action, td, baselines): \n",
    "        baselines = tf.cast(baselines, dtype=tf.float32)\n",
    "        baselines = tf.squeeze(baselines)\n",
    "        advantages = tf.convert_to_tensor(td - baselines, dtype=tf.float32)\n",
    "        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True)\n",
    "        entropy_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "                from_logits=True)\n",
    "        actions = tf.cast(action, tf.int32)\n",
    "        policy_loss = ce_loss(\n",
    "                action, logits, sample_weight=tf.stop_gradient(advantages))\n",
    "        entropy = entropy_loss(logits, logits)\n",
    "        return policy_loss - self.entropy_beta * entropy\n",
    "       \n",
    "        #baselines = tf.cast(baselines, dtype=tf.float32)\n",
    "        #baselines = tf.squeeze(baselines)\n",
    "        #action_probs = tfp.distributions.Categorical(probs=action_probs)\n",
    "        #log_prob = action_probs.log_prob(action)\n",
    "        #loss = -log_prob * tf.convert_to_tensor(td - baselines, dtype=tf.float32)\n",
    "        #loss = tf.reduce_mean(loss)\n",
    "        #return loss\n",
    "    \n",
    "       \n",
    "        \n",
    "    def save_model(self): \n",
    "        self.model.save(\"models/a3c/\" + \"actor_network\")  \n",
    "        print(\"Model Saved Successfully.\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:04.449873Z",
     "iopub.status.busy": "2023-04-11T07:42:04.449070Z",
     "iopub.status.idle": "2023-04-11T07:42:04.465120Z",
     "shell.execute_reply": "2023-04-11T07:42:04.463741Z",
     "shell.execute_reply.started": "2023-04-11T07:42:04.449816Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam \n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras \n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, input_dims, action_dim, critic_lr): \n",
    "        self.input_dims = input_dims \n",
    "        self.action_dim = action_dim\n",
    "        self.critic_lr = critic_lr\n",
    "        self.model = None\n",
    "        \n",
    "    def build_network(self): \n",
    "        input_ = tf.keras.layers.Input(self.input_dims)\n",
    "        x = Conv2D(64, 3, activation=\"relu\")(input_)\n",
    "        x = Conv2D(64, 3, activation=\"relu\")(x)\n",
    "        x = Conv2D(32, 3, activation=\"relu\")(x)\n",
    "        \n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        output_ = Dense(1, activation=None)(x)\n",
    "        model = keras.models.Model(input_, output_)\n",
    "        model.compile(Adam(learning_rate=self.critic_lr))\n",
    "        self.model = model\n",
    "    \n",
    "    def load_model(self): \n",
    "        self.model = tf.keras.models.load_model(self.fname + \"_actor_network\")\n",
    "        print(\"loaded the model\")\n",
    "        \n",
    "    def critic_loss(self, value, reward, next_state_value, gamma, done):\n",
    "        value = tf.cast(value, dtype=tf.float32)\n",
    "        next_state_value = tf.cast(next_state_value, dtype=tf.float32)\n",
    "        next_state_value = tf.squeeze(next_state_value)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        value = tf.squeeze(value)\n",
    "        td_target = reward + gamma * next_state_value * tf.convert_to_tensor([1-int(d) for d in done], dtype=tf.float32)\n",
    "        delta = tf.keras.losses.MSE(td_target, value)\n",
    "        return tf.convert_to_tensor(delta, dtype=tf.float32), tf.convert_to_tensor(td_target, dtype=tf.float32)\n",
    "    \n",
    "    def save_model(self): \n",
    "        self.model.save(\"models/a3c/\" + \"critic_network\") \n",
    "    \n",
    "    def learn(self, state, next_state_v_value, reward, done, gamma): \n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        #next_state = tf.convert_to_tensor(next_state, dtype=tf.float32)  \n",
    "        td_target = None\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            v_value = self.model(state)\n",
    "            v_target = next_state_v_value\n",
    "\n",
    "            critic_loss, td = self.critic_loss(v_value, reward, v_target, gamma, done)\n",
    "            \n",
    "        critic_params = self.model.trainable_variables \n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.model.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "        \n",
    "        td_target = td\n",
    "        return  critic_loss, td_target, v_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:04.798921Z",
     "iopub.status.busy": "2023-04-11T07:42:04.797899Z",
     "iopub.status.idle": "2023-04-11T07:42:04.807888Z",
     "shell.execute_reply": "2023-04-11T07:42:04.806674Z",
     "shell.execute_reply.started": "2023-04-11T07:42:04.798872Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalNetwork: \n",
    "    def __init__(self, input_dims, action_dims, actor_lr, critic_lr): \n",
    "        self.actor = Actor(input_dims, action_dims, actor_lr)        \n",
    "        self.critic= Critic(input_dims, action_dims, critic_lr)\n",
    "        \n",
    "    def update_global_params(self, state, next_state_v_value, actions, reward, done, gamma):\n",
    "        critic_l, td_target, v_value = self.critic.learn(state, next_state_v_value, reward, done, gamma)\n",
    "        actor_l = self.actor.learn(state, actions, td_target, baselines=v_value)\n",
    "        \n",
    "        return actor_l , critic_l\n",
    "        \n",
    "    def pull_global_params(self): \n",
    "        actor_params = self.actor.model.get_weights()\n",
    "        critic_params = self.critic.model.get_weights()\n",
    "        \n",
    "        return actor_params, critic_params        \n",
    "    \n",
    "    def save_models(self): \n",
    "        pass \n",
    "    \n",
    "    def load_models(self):\n",
    "        pass \n",
    "\n",
    "class WorkerNetwork: \n",
    "    def __init__(self , input_dims, action_dims, actor_lr, critic_lr): \n",
    "        self.actor = Actor(input_dims, action_dims, actor_lr)     \n",
    "        self.critic = Critic(input_dims, action_dims, critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:04.924247Z",
     "iopub.status.busy": "2023-04-11T07:42:04.923971Z",
     "iopub.status.idle": "2023-04-11T07:42:04.939093Z",
     "shell.execute_reply": "2023-04-11T07:42:04.937854Z",
     "shell.execute_reply.started": "2023-04-11T07:42:04.924222Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "# global_networks will be created in the A3C Agent\n",
    "# global_network = GlobalNetwork()\n",
    "\n",
    "CURR_EPISODE = 0\n",
    "BEST_REWARD = float(\"-inf\")\n",
    "EPISODE_REWARDS = []\n",
    "\n",
    "class A3CWorker: \n",
    "    def __init__(self, env, noe, gamma, update_interval, global_network, worker_network, input_dims, out_dims): \n",
    "        self.env = env \n",
    "        self.noe = noe\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.input_dims = input_dims \n",
    "        self.out_dims = out_dims\n",
    "        \n",
    "        self.icm_agent = ICMAgent(input_dims, out_dims, 0.1, 0.1, 0.002)\n",
    "        self.global_network = global_network\n",
    "        self.worker_network = worker_network\n",
    "        \n",
    "        actor_weights, critic_weights = self.global_network.pull_global_params()\n",
    "        self.worker_network.actor.model.set_weights(actor_weights)\n",
    "        self.worker_network.critic.model.set_weights(critic_weights)\n",
    "        \n",
    "        self.memory = ExperienceReplayBuffer(self.update_interval, input_dims, self.update_interval, False)\n",
    "        \n",
    "    \n",
    "    def learn(self): \n",
    "        \n",
    "        global CURR_EPISODE\n",
    "        global BEST_REWARD\n",
    "        global EPISODE_REWARDS\n",
    "        \n",
    "        while CURR_EPISODE <= self.noe: \n",
    "            state, _ = self.env.reset()\n",
    "            state = state.get(\"image\")\n",
    "            \n",
    "            done = False \n",
    "            episodic_reward = 0\n",
    "            step = 0\n",
    "            while not done:  \n",
    "                                    \n",
    "                action = self.worker_network.actor.get_action(state)\n",
    "                next_state_info = self.env.step(action)\n",
    "                    \n",
    "                next_state, reward_prob, terminated, truncated, _ = next_state_info \n",
    "                done = terminated or truncated \n",
    "                \n",
    "                next_state = next_state.get(\"image\")\n",
    "                \n",
    "                if (step % self.update_interval == 0 and step!=0) or done:\n",
    "                    states, actions, rewards, next_states, dones = self.memory.sample_experience(self.update_interval)\n",
    "                    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "\n",
    "                    next_state_v_value = self.worker_network.critic.model(next_states)\n",
    "                    action_probs = self.worker_network.actor.model(states)\n",
    "                    \n",
    "                    intrinsic_rewards = self.icm_agent.update(states, next_states, actions, self.out_dims)\n",
    "                    \n",
    "                    rewards += intrinsic_rewards\n",
    "\n",
    "                    actor_l, critic_l = self.global_network.update_global_params(states, next_state_v_value, \n",
    "                                                                                 actions, rewards, dones, self.gamma)\n",
    "                                 \n",
    "                    actor_weights, critic_weights = self.global_network.pull_global_params()\n",
    "                    self.worker_network.actor.model.set_weights(actor_weights)\n",
    "                    self.worker_network.critic.model.set_weights(critic_weights)\n",
    "                        \n",
    "                    self.memory.make_empty()\n",
    "                \n",
    "                self.memory.store_experience(state, action, reward_prob, next_state, done)\n",
    "                step += 1\n",
    "                episodic_reward += reward_prob\n",
    "                state  = next_state\n",
    "            \n",
    "            EPISODE_REWARDS.append(episodic_reward)\n",
    "            avg_reward = np.mean(EPISODE_REWARDS[-100: ])\n",
    "            \n",
    "            if episodic_reward > BEST_REWARD: \n",
    "                self.global_network.actor.save_model()\n",
    "                self.global_network.critic.save_model()\n",
    "                BEST_REWARD = episodic_reward\n",
    "                \n",
    "            print(f\"Episode: {CURR_EPISODE}, Reward: {episodic_reward} Average Reward: {avg_reward} Best Reward: {BEST_REWARD}\")\n",
    "            \n",
    "            CURR_EPISODE += 1 \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:05.168523Z",
     "iopub.status.busy": "2023-04-11T07:42:05.167781Z",
     "iopub.status.idle": "2023-04-11T07:42:05.178529Z",
     "shell.execute_reply": "2023-04-11T07:42:05.177508Z",
     "shell.execute_reply.started": "2023-04-11T07:42:05.168482Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread \n",
    "from multiprocessing import cpu_count \n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, actor_lr, critic_lr, gamma, update_interval, noe, worker_count=cpu_count()):\n",
    "        self.input_dims  = (7, 7, 3)\n",
    "        action_space = [_ for _ in range((env.action_space.n))]\n",
    "        self.out_dims = len(action_space)\n",
    "        self.worker_count = worker_count\n",
    "        self.noe = noe\n",
    "        self.update_interval = update_interval\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_lr \n",
    "        self.critic_lr = critic_lr\n",
    "        self.global_network = GlobalNetwork(self.input_dims, self.out_dims, actor_lr, critic_lr)\n",
    "        self.worker_network = WorkerNetwork(self.input_dims, self.out_dims, actor_lr, critic_lr)\n",
    "        self.global_network.actor.build_network()\n",
    "        self.global_network.critic.build_network()\n",
    "        self.worker_network.actor.build_network()\n",
    "        self.worker_network.critic.build_network()\n",
    "        \n",
    "    def learn(self): \n",
    "        workers = []\n",
    "        for _ in range(self.worker_count): \n",
    "            a3c_worker = A3CWorker(self.env, self.noe, self.gamma, \n",
    "                                        self.update_interval, self.global_network, self.worker_network, self.input_dims, self.out_dims)\n",
    "            workers.append(Thread(target=a3c_worker.learn()))\n",
    "        \n",
    "        for worker in workers: \n",
    "            worker.start()\n",
    "        \n",
    "        for worker in workers: \n",
    "            worker.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:05.569147Z",
     "iopub.status.busy": "2023-04-11T07:42:05.568872Z",
     "iopub.status.idle": "2023-04-11T07:42:05.580991Z",
     "shell.execute_reply": "2023-04-11T07:42:05.579847Z",
     "shell.execute_reply.started": "2023-04-11T07:42:05.569121Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras \n",
    "import tensorflow as tf \n",
    "\n",
    "class ICMNetwork(tf.keras.Model): \n",
    "    def __init__(self, input_dims, out_dims):\n",
    "        super(ICMNetwork, self).__init__()\n",
    "        self.input_dims = input_dims \n",
    "        self.out_dims = out_dims\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding=\"same\", input_shape=input_dims, \n",
    "                                                                 activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "        self.phi = keras.layers.Conv2D(3, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "     #   self.phi = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.inverse = keras.layers.Dense(256, activation='relu', kernel_initializer=\"he_uniform\")\n",
    "        self.pi_logits = keras.layers.Dense(out_dims, activation=\"softmax\")\n",
    "\n",
    "        self.dense1 = keras.layers.Dense(256)\n",
    "        self.phi_hat_next = keras.layers.Dense(7*7*3)\n",
    "        \n",
    "        self.flatten = keras.layers.Flatten()\n",
    "\n",
    "    def call(self, state, next_state, action):\n",
    "        #state = tf.reshape(state, (1, 7, 7, 3))\n",
    "        #next_state = tf.reshape(next_state, (1, 7, 7, 3))\n",
    "        phi = self.conv1(state)\n",
    "        phi = self.conv2(phi)\n",
    "        phi = self.phi(phi)\n",
    "        \n",
    "        phi_next = self.conv1(next_state)\n",
    "        phi_next = self.conv2(phi_next)\n",
    "        phi_next = self.phi(phi_next)\n",
    "        \n",
    "        phi = self.flatten(phi)\n",
    "        phi_next = self.flatten(phi_next)\n",
    "        inverse = self.inverse(tf.concat([phi, phi_next], axis=1))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "        \n",
    "        action = tf.cast(tf.reshape(action, (action.shape[0], 1)), tf.float32)\n",
    "        forward_input = tf.concat([phi, action], axis=1)\n",
    "        dense = self.dense1(forward_input)\n",
    "        phi_hat_next = self.phi_hat_next(dense)\n",
    "        \n",
    "        return phi_next, pi_logits, phi_hat_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:06.220736Z",
     "iopub.status.busy": "2023-04-11T07:42:06.220233Z",
     "iopub.status.idle": "2023-04-11T07:42:06.238624Z",
     "shell.execute_reply": "2023-04-11T07:42:06.237282Z",
     "shell.execute_reply.started": "2023-04-11T07:42:06.220693Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "class ICMAgent: \n",
    "    def __init__(self, input_dims, out_dims, alpha, beta, lr):\n",
    "        self.input_dims = input_dims \n",
    "        self.out_dims = out_dims \n",
    "        self.alpha = alpha \n",
    "        self.beta = beta \n",
    "        \n",
    "        self.icm_model = ICMNetwork(self.input_dims, self.out_dims)\n",
    "        self.icm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    def update(self, states, next_states, actions, n_action): \n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        actions_one_hot = tf.one_hot(actions, depth=n_action)\n",
    "        \n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "       # print(actions.shape, actions_one_hot, states.shape, next_states.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            phi_next, pi_logits, phi_hat_next = \\\n",
    "                            self.icm_model(states, next_states, actions)\n",
    "            \n",
    "           # pi_logits = tf.cast(pi_logits, dtype=tf.int32)\n",
    "            #print(pi_logits, actions_one_hot)\n",
    "            inverse_loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "            inverse_loss = (1 - self.beta) * inverse_loss_func(pi_logits, actions_one_hot)\n",
    "\n",
    "            forward_loss = tf.keras.losses.MSE(phi_hat_next, phi_next)\n",
    "            forward_loss = self.beta * forward_loss\n",
    "            forward_loss = tf.reduce_mean(forward_loss)\n",
    "           # print(\"forwaed\", forward_loss)\n",
    "\n",
    "            squared = tf.math.pow(phi_hat_next-phi_next, 2)\n",
    "           # print(squared, \"sqa\")\n",
    "            intrinsic_reward = self.alpha*0.5*squared\n",
    "            intrinsic_reward = tf.reduce_sum(intrinsic_reward, axis=1)\n",
    "\n",
    "            loss = inverse_loss + forward_loss\n",
    "            \n",
    "        params = self.icm_model.trainable_variables\n",
    "        grads = tape.gradient(loss, params)\n",
    "        self.icm_model.optimizer.apply_gradients(zip(grads, params))\n",
    "    \n",
    "        return intrinsic_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T07:42:56.509086Z",
     "iopub.status.busy": "2023-04-11T07:42:56.508452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully.\n",
      "Episode: 0, Reward: 0 Average Reward: 0.0 Best Reward: 0\n",
      "Episode: 1, Reward: 0 Average Reward: 0.0 Best Reward: 0\n",
      "Episode: 2, Reward: 0 Average Reward: 0.0 Best Reward: 0\n",
      "Episode: 3, Reward: 0 Average Reward: 0.0 Best Reward: 0\n",
      "Episode: 4, Reward: 0 Average Reward: 0.0 Best Reward: 0\n",
      "Model Saved Successfully.\n",
      "Episode: 5, Reward: 0.613 Average Reward: 0.10216666666666667 Best Reward: 0.613\n",
      "Episode: 6, Reward: 0 Average Reward: 0.08757142857142856 Best Reward: 0.613\n",
      "Episode: 7, Reward: 0 Average Reward: 0.076625 Best Reward: 0.613\n",
      "Episode: 8, Reward: 0 Average Reward: 0.06811111111111111 Best Reward: 0.613\n",
      "Episode: 9, Reward: 0.487 Average Reward: 0.11000000000000001 Best Reward: 0.613\n",
      "Episode: 10, Reward: 0.478 Average Reward: 0.14345454545454547 Best Reward: 0.613\n",
      "Episode: 11, Reward: 0.44199999999999995 Average Reward: 0.16833333333333333 Best Reward: 0.613\n",
      "Episode: 12, Reward: 0.42399999999999993 Average Reward: 0.188 Best Reward: 0.613\n",
      "Episode: 13, Reward: 0 Average Reward: 0.17457142857142857 Best Reward: 0.613\n",
      "Episode: 14, Reward: 0.5680000000000001 Average Reward: 0.2008 Best Reward: 0.613\n",
      "Episode: 15, Reward: 0 Average Reward: 0.18825 Best Reward: 0.613\n",
      "Episode: 16, Reward: 0 Average Reward: 0.1771764705882353 Best Reward: 0.613\n",
      "Episode: 17, Reward: 0 Average Reward: 0.16733333333333333 Best Reward: 0.613\n",
      "Episode: 18, Reward: 0 Average Reward: 0.15852631578947368 Best Reward: 0.613\n",
      "Episode: 19, Reward: 0.262 Average Reward: 0.1637 Best Reward: 0.613\n",
      "Episode: 20, Reward: 0 Average Reward: 0.1559047619047619 Best Reward: 0.613\n",
      "Episode: 21, Reward: 0.2889999999999999 Average Reward: 0.16195454545454543 Best Reward: 0.613\n",
      "Episode: 22, Reward: 0 Average Reward: 0.15491304347826085 Best Reward: 0.613\n",
      "Episode: 23, Reward: 0 Average Reward: 0.14845833333333333 Best Reward: 0.613\n",
      "Model Saved Successfully.\n",
      "Episode: 24, Reward: 0.775 Average Reward: 0.17352 Best Reward: 0.775\n",
      "Episode: 25, Reward: 0 Average Reward: 0.16684615384615384 Best Reward: 0.775\n",
      "Episode: 26, Reward: 0.18999999999999995 Average Reward: 0.16770370370370372 Best Reward: 0.775\n",
      "Episode: 27, Reward: 0 Average Reward: 0.16171428571428573 Best Reward: 0.775\n",
      "Episode: 28, Reward: 0.118 Average Reward: 0.16020689655172415 Best Reward: 0.775\n",
      "Episode: 29, Reward: 0 Average Reward: 0.15486666666666668 Best Reward: 0.775\n",
      "Episode: 30, Reward: 0.18999999999999995 Average Reward: 0.156 Best Reward: 0.775\n",
      "Episode: 31, Reward: 0.487 Average Reward: 0.16634375 Best Reward: 0.775\n",
      "Episode: 32, Reward: 0 Average Reward: 0.1613030303030303 Best Reward: 0.775\n",
      "Episode: 33, Reward: 0 Average Reward: 0.15655882352941178 Best Reward: 0.775\n",
      "Episode: 34, Reward: 0 Average Reward: 0.1520857142857143 Best Reward: 0.775\n",
      "Episode: 35, Reward: 0.15400000000000003 Average Reward: 0.1521388888888889 Best Reward: 0.775\n",
      "Episode: 36, Reward: 0 Average Reward: 0.14802702702702702 Best Reward: 0.775\n",
      "Episode: 37, Reward: 0 Average Reward: 0.14413157894736842 Best Reward: 0.775\n",
      "Episode: 38, Reward: 0 Average Reward: 0.14043589743589743 Best Reward: 0.775\n",
      "Episode: 39, Reward: 0.73 Average Reward: 0.155175 Best Reward: 0.775\n",
      "Episode: 40, Reward: 0.29799999999999993 Average Reward: 0.15865853658536586 Best Reward: 0.775\n",
      "Episode: 41, Reward: 0.6579999999999999 Average Reward: 0.17054761904761906 Best Reward: 0.775\n",
      "Episode: 42, Reward: 0 Average Reward: 0.1665813953488372 Best Reward: 0.775\n",
      "Episode: 43, Reward: 0 Average Reward: 0.16279545454545455 Best Reward: 0.775\n",
      "Episode: 44, Reward: 0.757 Average Reward: 0.176 Best Reward: 0.775\n",
      "Episode: 45, Reward: 0 Average Reward: 0.17217391304347826 Best Reward: 0.775\n",
      "Episode: 46, Reward: 0.55 Average Reward: 0.1802127659574468 Best Reward: 0.775\n",
      "Episode: 47, Reward: 0 Average Reward: 0.1764583333333333 Best Reward: 0.775\n",
      "Episode: 48, Reward: 0.33399999999999996 Average Reward: 0.17967346938775508 Best Reward: 0.775\n",
      "Model Saved Successfully.\n",
      "Episode: 49, Reward: 0.847 Average Reward: 0.19301999999999997 Best Reward: 0.847\n",
      "Episode: 50, Reward: 0.5680000000000001 Average Reward: 0.2003725490196078 Best Reward: 0.847\n",
      "Model Saved Successfully.\n",
      "Episode: 51, Reward: 0.892 Average Reward: 0.21367307692307685 Best Reward: 0.892\n",
      "Episode: 52, Reward: 0.2889999999999999 Average Reward: 0.21509433962264146 Best Reward: 0.892\n",
      "Episode: 53, Reward: 0.757 Average Reward: 0.22512962962962957 Best Reward: 0.892\n",
      "Episode: 54, Reward: 0.127 Average Reward: 0.22334545454545449 Best Reward: 0.892\n",
      "Episode: 55, Reward: 0.487 Average Reward: 0.22805357142857144 Best Reward: 0.892\n",
      "Episode: 56, Reward: 0 Average Reward: 0.2240526315789474 Best Reward: 0.892\n",
      "Episode: 57, Reward: 0 Average Reward: 0.22018965517241382 Best Reward: 0.892\n",
      "Episode: 58, Reward: 0.514 Average Reward: 0.22516949152542373 Best Reward: 0.892\n",
      "Episode: 59, Reward: 0 Average Reward: 0.22141666666666668 Best Reward: 0.892\n",
      "Episode: 60, Reward: 0 Average Reward: 0.21778688524590165 Best Reward: 0.892\n",
      "Episode: 61, Reward: 0.42399999999999993 Average Reward: 0.22111290322580646 Best Reward: 0.892\n",
      "Episode: 62, Reward: 0.21699999999999997 Average Reward: 0.22104761904761905 Best Reward: 0.892\n",
      "Episode: 63, Reward: 0 Average Reward: 0.21759374999999997 Best Reward: 0.892\n",
      "Episode: 64, Reward: 0 Average Reward: 0.2142461538461538 Best Reward: 0.892\n",
      "Episode: 65, Reward: 0 Average Reward: 0.21099999999999997 Best Reward: 0.892\n",
      "Episode: 66, Reward: 0.856 Average Reward: 0.22062686567164178 Best Reward: 0.892\n",
      "Episode: 67, Reward: 0.55 Average Reward: 0.2254705882352941 Best Reward: 0.892\n",
      "Episode: 68, Reward: 0.2799999999999999 Average Reward: 0.22626086956521738 Best Reward: 0.892\n",
      "Episode: 69, Reward: 0.532 Average Reward: 0.2306285714285714 Best Reward: 0.892\n",
      "Episode: 70, Reward: 0 Average Reward: 0.22738028169014082 Best Reward: 0.892\n",
      "Episode: 71, Reward: 0.685 Average Reward: 0.23373611111111112 Best Reward: 0.892\n",
      "Episode: 72, Reward: 0.5409999999999999 Average Reward: 0.23794520547945208 Best Reward: 0.892\n",
      "Model Saved Successfully.\n",
      "Episode: 73, Reward: 0.91 Average Reward: 0.24702702702702703 Best Reward: 0.91\n",
      "Episode: 74, Reward: 0 Average Reward: 0.24373333333333336 Best Reward: 0.91\n",
      "Episode: 75, Reward: 0.721 Average Reward: 0.2500131578947369 Best Reward: 0.91\n",
      "Episode: 76, Reward: 0.4959999999999999 Average Reward: 0.2532077922077922 Best Reward: 0.91\n",
      "Episode: 77, Reward: 0.73 Average Reward: 0.2593205128205128 Best Reward: 0.91\n",
      "Episode: 78, Reward: 0.613 Average Reward: 0.26379746835443035 Best Reward: 0.91\n",
      "Episode: 79, Reward: 0.379 Average Reward: 0.2652375 Best Reward: 0.91\n",
      "Episode: 80, Reward: 0 Average Reward: 0.26196296296296295 Best Reward: 0.91\n",
      "Episode: 81, Reward: 0.21699999999999997 Average Reward: 0.2614146341463415 Best Reward: 0.91\n",
      "Episode: 82, Reward: 0 Average Reward: 0.25826506024096385 Best Reward: 0.91\n",
      "Episode: 83, Reward: 0.3879999999999999 Average Reward: 0.25980952380952377 Best Reward: 0.91\n",
      "Model Saved Successfully.\n",
      "Episode: 84, Reward: 0.9279999999999999 Average Reward: 0.2676705882352941 Best Reward: 0.9279999999999999\n",
      "Episode: 85, Reward: 0 Average Reward: 0.2645581395348837 Best Reward: 0.9279999999999999\n",
      "Episode: 86, Reward: 0.649 Average Reward: 0.26897701149425285 Best Reward: 0.9279999999999999\n",
      "Episode: 87, Reward: 0.14500000000000002 Average Reward: 0.2675681818181818 Best Reward: 0.9279999999999999\n",
      "Episode: 88, Reward: 0.829 Average Reward: 0.273876404494382 Best Reward: 0.9279999999999999\n",
      "Episode: 89, Reward: 0 Average Reward: 0.2708333333333333 Best Reward: 0.9279999999999999\n",
      "Episode: 90, Reward: 0.577 Average Reward: 0.27419780219780215 Best Reward: 0.9279999999999999\n",
      "Episode: 91, Reward: 0 Average Reward: 0.2712173913043478 Best Reward: 0.9279999999999999\n",
      "Episode: 92, Reward: 0.43299999999999994 Average Reward: 0.2729569892473118 Best Reward: 0.9279999999999999\n",
      "Episode: 93, Reward: 0.7929999999999999 Average Reward: 0.2784893617021276 Best Reward: 0.9279999999999999\n",
      "Episode: 94, Reward: 0.685 Average Reward: 0.28276842105263156 Best Reward: 0.9279999999999999\n",
      "Episode: 95, Reward: 0.82 Average Reward: 0.2883645833333333 Best Reward: 0.9279999999999999\n",
      "Episode: 96, Reward: 0 Average Reward: 0.2853917525773196 Best Reward: 0.9279999999999999\n",
      "Episode: 97, Reward: 0.694 Average Reward: 0.28956122448979593 Best Reward: 0.9279999999999999\n",
      "Episode: 98, Reward: 0.865 Average Reward: 0.29537373737373734 Best Reward: 0.9279999999999999\n",
      "Episode: 99, Reward: 0 Average Reward: 0.29241999999999996 Best Reward: 0.9279999999999999\n",
      "Episode: 100, Reward: 0.721 Average Reward: 0.29962999999999995 Best Reward: 0.9279999999999999\n",
      "Episode: 101, Reward: 0 Average Reward: 0.29962999999999995 Best Reward: 0.9279999999999999\n",
      "Episode: 102, Reward: 0.478 Average Reward: 0.30441 Best Reward: 0.9279999999999999\n",
      "Episode: 103, Reward: 0.586 Average Reward: 0.31027 Best Reward: 0.9279999999999999\n",
      "Episode: 104, Reward: 0 Average Reward: 0.31027 Best Reward: 0.9279999999999999\n",
      "Episode: 105, Reward: 0 Average Reward: 0.30414 Best Reward: 0.9279999999999999\n",
      "Episode: 106, Reward: 0.2889999999999999 Average Reward: 0.30702999999999997 Best Reward: 0.9279999999999999\n",
      "Episode: 107, Reward: 0.685 Average Reward: 0.31388 Best Reward: 0.9279999999999999\n",
      "Episode: 108, Reward: 0.14500000000000002 Average Reward: 0.31533 Best Reward: 0.9279999999999999\n",
      "Episode: 109, Reward: 0 Average Reward: 0.31046 Best Reward: 0.9279999999999999\n",
      "Episode: 110, Reward: 0.43299999999999994 Average Reward: 0.31000999999999995 Best Reward: 0.9279999999999999\n",
      "Episode: 111, Reward: 0.685 Average Reward: 0.31244 Best Reward: 0.9279999999999999\n",
      "Episode: 112, Reward: 0.9279999999999999 Average Reward: 0.31748 Best Reward: 0.9279999999999999\n",
      "Episode: 113, Reward: 0.766 Average Reward: 0.32514 Best Reward: 0.9279999999999999\n",
      "Episode: 114, Reward: 0.649 Average Reward: 0.32594999999999996 Best Reward: 0.9279999999999999\n",
      "Episode: 115, Reward: 0 Average Reward: 0.32594999999999996 Best Reward: 0.9279999999999999\n",
      "Episode: 116, Reward: 0.33399999999999996 Average Reward: 0.32929 Best Reward: 0.9279999999999999\n",
      "Episode: 117, Reward: 0.667 Average Reward: 0.33596000000000004 Best Reward: 0.9279999999999999\n",
      "Episode: 118, Reward: 0.45999999999999996 Average Reward: 0.34056000000000003 Best Reward: 0.9279999999999999\n",
      "Episode: 119, Reward: 0.34299999999999997 Average Reward: 0.34137 Best Reward: 0.9279999999999999\n",
      "Episode: 120, Reward: 0 Average Reward: 0.34137 Best Reward: 0.9279999999999999\n",
      "Episode: 121, Reward: 0.901 Average Reward: 0.34749 Best Reward: 0.9279999999999999\n",
      "Episode: 122, Reward: 0.4059999999999999 Average Reward: 0.35155000000000003 Best Reward: 0.9279999999999999\n",
      "Episode: 123, Reward: 0.33399999999999996 Average Reward: 0.35489000000000004 Best Reward: 0.9279999999999999\n",
      "Episode: 124, Reward: 0 Average Reward: 0.34714000000000006 Best Reward: 0.9279999999999999\n",
      "Episode: 125, Reward: 0.703 Average Reward: 0.35417000000000004 Best Reward: 0.9279999999999999\n",
      "Episode: 126, Reward: 0 Average Reward: 0.35227 Best Reward: 0.9279999999999999\n",
      "Episode: 127, Reward: 0 Average Reward: 0.35227 Best Reward: 0.9279999999999999\n",
      "Episode: 128, Reward: 0.874 Average Reward: 0.35983000000000004 Best Reward: 0.9279999999999999\n",
      "Episode: 129, Reward: 0.757 Average Reward: 0.36739999999999995 Best Reward: 0.9279999999999999\n",
      "Episode: 130, Reward: 0 Average Reward: 0.3655 Best Reward: 0.9279999999999999\n",
      "Episode: 131, Reward: 0.91 Average Reward: 0.36973 Best Reward: 0.9279999999999999\n",
      "Episode: 132, Reward: 0.892 Average Reward: 0.37864999999999993 Best Reward: 0.9279999999999999\n",
      "Episode: 133, Reward: 0.17199999999999993 Average Reward: 0.38036999999999993 Best Reward: 0.9279999999999999\n",
      "Episode: 134, Reward: 0.3969999999999999 Average Reward: 0.3843399999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 135, Reward: 0 Average Reward: 0.38280000000000003 Best Reward: 0.9279999999999999\n",
      "Episode: 136, Reward: 0 Average Reward: 0.3827999999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 137, Reward: 0 Average Reward: 0.3827999999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 138, Reward: 0 Average Reward: 0.38280000000000003 Best Reward: 0.9279999999999999\n",
      "Episode: 139, Reward: 0 Average Reward: 0.37550000000000006 Best Reward: 0.9279999999999999\n",
      "Episode: 140, Reward: 0 Average Reward: 0.37251999999999996 Best Reward: 0.9279999999999999\n",
      "Episode: 141, Reward: 0.19899999999999995 Average Reward: 0.3679299999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 142, Reward: 0 Average Reward: 0.3679299999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 143, Reward: 0.20799999999999996 Average Reward: 0.37000999999999995 Best Reward: 0.9279999999999999\n",
      "Episode: 144, Reward: 0.802 Average Reward: 0.37046 Best Reward: 0.9279999999999999\n",
      "Episode: 145, Reward: 0 Average Reward: 0.37046 Best Reward: 0.9279999999999999\n",
      "Episode: 146, Reward: 0.17199999999999993 Average Reward: 0.3666799999999999 Best Reward: 0.9279999999999999\n",
      "Episode: 147, Reward: 0.649 Average Reward: 0.37317 Best Reward: 0.9279999999999999\n",
      "Episode: 148, Reward: 0.802 Average Reward: 0.37784999999999996 Best Reward: 0.9279999999999999\n",
      "Episode: 149, Reward: 0.712 Average Reward: 0.3765 Best Reward: 0.9279999999999999\n",
      "Episode: 150, Reward: 0.32499999999999996 Average Reward: 0.37407 Best Reward: 0.9279999999999999\n",
      "Model Saved Successfully.\n",
      "Episode: 151, Reward: 0.9369999999999999 Average Reward: 0.3745200000000001 Best Reward: 0.9369999999999999\n",
      "Episode: 152, Reward: 0.865 Average Reward: 0.38028000000000006 Best Reward: 0.9369999999999999\n",
      "Episode: 153, Reward: 0 Average Reward: 0.37271 Best Reward: 0.9369999999999999\n",
      "Episode: 154, Reward: 0 Average Reward: 0.37144 Best Reward: 0.9369999999999999\n",
      "Episode: 155, Reward: 0 Average Reward: 0.36657000000000006 Best Reward: 0.9369999999999999\n",
      "Episode: 156, Reward: 0.757 Average Reward: 0.37414000000000003 Best Reward: 0.9369999999999999\n",
      "Episode: 157, Reward: 0.766 Average Reward: 0.3817999999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 158, Reward: 0.5680000000000001 Average Reward: 0.38233999999999996 Best Reward: 0.9369999999999999\n",
      "Episode: 159, Reward: 0.4149999999999999 Average Reward: 0.38648999999999994 Best Reward: 0.9369999999999999\n",
      "Episode: 160, Reward: 0.127 Average Reward: 0.38775999999999994 Best Reward: 0.9369999999999999\n",
      "Episode: 161, Reward: 0.784 Average Reward: 0.39135999999999993 Best Reward: 0.9369999999999999\n",
      "Episode: 162, Reward: 0.865 Average Reward: 0.39783999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 163, Reward: 0.721 Average Reward: 0.40504999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 164, Reward: 0.14500000000000002 Average Reward: 0.4064999999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 165, Reward: 0.721 Average Reward: 0.41370999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 166, Reward: 0.379 Average Reward: 0.4089399999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 167, Reward: 0 Average Reward: 0.40343999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 168, Reward: 0.10899999999999999 Average Reward: 0.4017299999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 169, Reward: 0.21699999999999997 Average Reward: 0.3985799999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 170, Reward: 0.118 Average Reward: 0.39976 Best Reward: 0.9369999999999999\n",
      "Episode: 171, Reward: 0.532 Average Reward: 0.3982299999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 172, Reward: 0 Average Reward: 0.39281999999999995 Best Reward: 0.9369999999999999\n",
      "Episode: 173, Reward: 0.901 Average Reward: 0.39273 Best Reward: 0.9369999999999999\n",
      "Episode: 174, Reward: 0 Average Reward: 0.39273 Best Reward: 0.9369999999999999\n",
      "Episode: 175, Reward: 0 Average Reward: 0.38552 Best Reward: 0.9369999999999999\n",
      "Episode: 176, Reward: 0.262 Average Reward: 0.38317999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 177, Reward: 0.262 Average Reward: 0.3785 Best Reward: 0.9369999999999999\n",
      "Episode: 178, Reward: 0.847 Average Reward: 0.38084 Best Reward: 0.9369999999999999\n",
      "Episode: 179, Reward: 0.757 Average Reward: 0.38462 Best Reward: 0.9369999999999999\n",
      "Episode: 180, Reward: 0 Average Reward: 0.38461999999999996 Best Reward: 0.9369999999999999\n",
      "Episode: 181, Reward: 0 Average Reward: 0.38244999999999996 Best Reward: 0.9369999999999999\n",
      "Episode: 182, Reward: 0 Average Reward: 0.38244999999999996 Best Reward: 0.9369999999999999\n",
      "Episode: 183, Reward: 0.469 Average Reward: 0.38326 Best Reward: 0.9369999999999999\n",
      "Episode: 184, Reward: 0.5409999999999999 Average Reward: 0.37939 Best Reward: 0.9369999999999999\n",
      "Episode: 185, Reward: 0.685 Average Reward: 0.38623999999999997 Best Reward: 0.9369999999999999\n",
      "Episode: 186, Reward: 0 Average Reward: 0.37975000000000003 Best Reward: 0.9369999999999999\n",
      "Episode: 187, Reward: 0.44199999999999995 Average Reward: 0.38272 Best Reward: 0.9369999999999999\n",
      "Episode: 188, Reward: 0 Average Reward: 0.37443000000000004 Best Reward: 0.9369999999999999\n",
      "Episode: 189, Reward: 0 Average Reward: 0.37443 Best Reward: 0.9369999999999999\n",
      "Episode: 190, Reward: 0 Average Reward: 0.36866 Best Reward: 0.9369999999999999\n",
      "Episode: 191, Reward: 0.6399999999999999 Average Reward: 0.37506 Best Reward: 0.9369999999999999\n",
      "Episode: 192, Reward: 0.532 Average Reward: 0.37605000000000005 Best Reward: 0.9369999999999999\n",
      "Episode: 193, Reward: 0.9279999999999999 Average Reward: 0.37739999999999996 Best Reward: 0.9369999999999999\n",
      "Episode: 194, Reward: 0.721 Average Reward: 0.3777599999999999 Best Reward: 0.9369999999999999\n",
      "Episode: 195, Reward: 0 Average Reward: 0.36955999999999994 Best Reward: 0.9369999999999999\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
    "import minigrid\n",
    "\n",
    "#env = gym.make(\"MiniGrid-FourRooms-v0\")\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "noe = 10000\n",
    "actor_lr = 0.003\n",
    "critic_lr = 0.003\n",
    "gamma = 0.99\n",
    "update_interval = 2\n",
    "num_workers = 15\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    agent = ActorCriticAgent(env, actor_lr, critic_lr, gamma, update_interval, noe, num_workers)\n",
    "    agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T07:39:11.370035Z",
     "iopub.status.idle": "2023-04-11T07:39:11.370802Z",
     "shell.execute_reply": "2023-04-11T07:39:11.370551Z",
     "shell.execute_reply.started": "2023-04-11T07:39:11.370523Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "if __name__ == \"__main__\": \n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    agent = ActorCriticAgent(env, 0.0005, 0.005, 0.99, 4, 1000, 10)\n",
    "    agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "88854253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GcL3VRow8nS"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from gymnasium.wrappers import *\n",
    "\n",
    "\n",
    "def manage_memory():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def plot_learning_curve(scores, figure_file):\n",
    "\n",
    "    x = [_ for _ in range(len(scores))]\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg, label=\"Avg reward for agent\", color=\"black\")\n",
    "    plt.plot(scores, label=\"Reward for agent\", color=\"red\")\n",
    "    plt.xlabel(\"episodes\")\n",
    "    plt.ylabel(\"rewards\")\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.legend()\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "    \n",
    "class RepeatAction(gym.Wrapper):\n",
    "    def __init__(self, env=None, repeat=4, fire_first=False):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.get(\"image\").low[0][0][0]\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.repeat):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, t_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            obs, _, _, _, _ = self.env_step(1)\n",
    "        return obs\n",
    "\n",
    "    \n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.shape = (shape[2], shape[0], shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                                                shape=self.shape,\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
    "                                    interpolation=cv2.INTER_AREA)\n",
    "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0\n",
    "\n",
    "        return new_obs\n",
    "    \n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                env.observation_space.low.repeat(repeat, axis=0),\n",
    "                env.observation_space.high.repeat(repeat, axis=0),\n",
    "                dtype=np.float32)\n",
    "        self.stack = collections.deque(maxlen=repeat)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "    \n",
    "    \n",
    "def make_env(env_name): \n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = RepeatAction(env)\n",
    "#    env = PreprocessFrame(env.observation_space.get(\"image\").shape, env)\n",
    "  #  env = FrameStack(env, 4, lz4_compress=False)\n",
    "  #  env = NormalizeObservation(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5E1iSI07fi-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rq1BvlM7fl6"
   },
   "outputs": [],
   "source": [
    "class Writer:\n",
    "    def __init__(self, fname): \n",
    "        self.fname = fname \n",
    "\n",
    "    def write_to_file(self, content): \n",
    "        with open(self.fname, \"a\") as file: \n",
    "            file.write(content + \"\\n\")\n",
    "\n",
    "    def read_file(self, fname):\n",
    "        with open(fname, \"r\") as file: \n",
    "            return file.read()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNtP-BTO7fpi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAxjX1V87fr4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from telebot import TeleBot\n",
    "import datetime\n",
    "import telebot\n",
    "\n",
    "token = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\n",
    "chat_id = \"1055055706\"\n",
    "bot = TeleBot(token=token) \n",
    "\n",
    "def telegram_send(message, bot):\n",
    "    chat_id = \"1055055706\"\n",
    "    bot.send_message(chat_id=chat_id, text=message)\n",
    "\n",
    "def welcome_msg(multi_step, double_dqn, dueling):\n",
    "    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n",
    "    telegram_send(st, bot)\n",
    "    \n",
    "def info_msg(episode, max_episode, reward, best_score, loss): \n",
    "    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n",
    "    telegram_send(st, bot)\n",
    "\n",
    "def end_msg(learning_time):\n",
    "    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n",
    "    telegram_send(st, bot)\n",
    "    print(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_trkbFOo7fy8"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import imageio\n",
    "\n",
    "\n",
    "class RecordVideo: \n",
    "    \n",
    "    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n",
    "        self.prefix_fname = prefix_fname\n",
    "        self.out_directory = out_directory\n",
    "        self.fps = fps\n",
    "        self.images = []\n",
    "        \n",
    "    def add_image(self, image): \n",
    "        self.images.append(image)\n",
    "    \n",
    "    def save(self, episode_no): \n",
    "        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n",
    "        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n",
    "        self.images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSEo-Nz4w8ph"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, env, action_space, input_dims, out_dims, video_prefix, is_tg,\n",
    "                                     noe, max_steps, record, lr1, lr2, gamma, chkpt, algo_name, update_interval): \n",
    "        self.env = env\n",
    "        self.noe = noe \n",
    "        self.max_steps = max_steps \n",
    "        self.update_interval = update_interval\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.recorder = RecordVideo(video_prefix)\n",
    "        self.is_tg = is_tg \n",
    "        self.record = record\n",
    "        self.agent = ActorCriticAgent(input_dims, out_dims, gamma, lr1, lr2, action_space, 32, chkpt, algo_name)\n",
    "        self.memory = ExperienceReplayBuffer(update_interval, input_dims, update_interval, False)\n",
    "        self.icm_agent = ICMAgent(input_dims, out_dims, 0.1, 0.1, 0.001)\n",
    "\n",
    "    def train(self): \n",
    "\n",
    "        ep_rewards = []\n",
    "        avg_rewards = []\n",
    "        best_reward = float(\"-inf\")\n",
    "        replay_counter = 0\n",
    "\n",
    "        for episode in range(self.noe): \n",
    "           \n",
    "            state, _ = self.env.reset()   \n",
    "            state = state.get('image')\n",
    "            ep_reward = 0 \n",
    "            steps = 0\n",
    "\n",
    "            if self.record and episode % 50 == 0: \n",
    "                img = self.env.render()\n",
    "                self.recorder.add_image(img)\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                \n",
    "                action = self.agent.get_action(state)\n",
    " \n",
    "                next_info = self.env.step(action)\n",
    "\n",
    "                next_state, reward_prob, terminated, truncated, _ = next_info \n",
    "                next_state = next_state.get(\"image\")\n",
    "                done = terminated or truncated\n",
    "                ep_reward += reward_prob\n",
    "                            \n",
    "                if (step % update_interval == 0 and step!=0) or done:\n",
    "                    states, actions, rewards, next_states, dones = self.memory.sample_experience(update_interval)\n",
    "                    \n",
    "                    intrinsic_reward = self.icm_agent.update(states, next_states, actions, self.out_dims)\n",
    "\n",
    "                    self.agent.learn(states, actions, rewards+intrinsic_reward, next_states, dones)\n",
    "                   # print(actions)\n",
    "                    self.memory.make_empty()\n",
    "                self.memory.store_experience(state, action, reward_prob, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                if self.record and episode % 50 == 0:\n",
    "                    img = self.env.render()\n",
    "                    self.recorder.add_image(img)\n",
    "\n",
    "                if done: \n",
    "                    break \n",
    "                \n",
    "            if self.record and episode % 50 == 0:\n",
    "                self.recorder.save(episode)\n",
    "\n",
    "            ep_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(ep_rewards[-100:])\n",
    "            avg_rewards.append(avg_reward)\n",
    "            \n",
    "            print(f\"Episode: {episode} Steps: {steps} Reward: {ep_reward} Best Score: {best_reward}, Average Reward: {avg_reward}\")\n",
    "            \n",
    "            if ep_reward > best_reward: \n",
    "                self.agent.save_models()\n",
    "                best_reward = ep_reward\n",
    "                \n",
    "        return ep_rewards, avg_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "md8-b_oY7s20",
    "outputId": "d9fd7615-a91b-4595-8e72-db2de6ab8e18"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
    "import minigrid\n",
    "\n",
    "#env = gym.make(\"MiniGrid-FourRooms-v0\")\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "action_space = [_ for _ in range((env.action_space.n))]\n",
    "print(env.observation_space)\n",
    "n_actions = len(action_space)\n",
    "input_dims = (7, 7, 3)\n",
    "noe = 10000\n",
    "max_steps = 200\n",
    "video_prefix = \"actor_critic\"\n",
    "is_tg = True \n",
    "record = False\n",
    "lr1 = 0.003\n",
    "lr2 = 0.0005\n",
    "gamma = 0.99\n",
    "chpkt = 'models/'\n",
    "algo_name = \"actor_critic\"\n",
    "update_interval = 2\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trainer = Trainer(env, action_space, input_dims, n_actions, video_prefix, is_tg, \n",
    "                                          noe, max_steps, record, lr1, lr2, gamma, chpkt, algo_name, update_interval)\n",
    "    ep_rewards, _ = trainer.train()\n",
    "    plot_learning_curve(ep_rewards, \"actor_critic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwAbSdSq7s5t"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"actor_critic_eps_rewards.obj\", \"wb\") as f: \n",
    "    pickle.dump(ep_rewards[0], f)\n",
    "\n",
    "with open(\"actor_critic_avg_rewards.obj\", \"wb\") as f: \n",
    "    pickle.dump(ep_rewards[1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "qaS-tMXXV46C",
    "outputId": "9e862b2a-bcea-4d63-afd5-90ce5b9c0084"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(ep_rewards[0], \"actor_critic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOg3R5Lg7s9f"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(observation, q_val_network, action_space): \n",
    "    state = tf.convert_to_tensor([observation])\n",
    "    actions = q_val_network(state)\n",
    "    action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNGxYRRf7s_a"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class Eval: \n",
    "\n",
    "    def __init__(self, env, model_path, number_of_episode=50):\n",
    "        self.env = env \n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.recorder = RecordVideo('dqn_lunarlander', 'test_videos/', 15)\n",
    "        self.number_of_episode = number_of_episode\n",
    "        \n",
    "    def test(self): \n",
    "        rewards = []\n",
    "        steps = []\n",
    "        for episode in range(self.number_of_episode): \n",
    "            done = False\n",
    "            reward = 0\n",
    "            step = 0\n",
    "            state = env.reset(seed=random.randint(0,500))\n",
    "            if episode % 10 == 0: \n",
    "                img = env.render()\n",
    "                self.recorder.add_image(img) \n",
    "\n",
    "            while not done:\n",
    "\n",
    "                if type(state) == tuple: \n",
    "                    state = state[0]\n",
    "                action =  greedy_policy(state, self.model, action_space)\n",
    "                state, reward_prob, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated \n",
    "                reward += reward_prob\n",
    "                step += 1 \n",
    "                if episode % 10 == 0:\n",
    "                    img = env.render()\n",
    "                    self.recorder.add_image(img)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            steps.append(step)\n",
    "            self.recorder.save(1) if episode % 10 == 0 else None \n",
    "        \n",
    "        return rewards, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7k-z8PD2w8rZ",
    "outputId": "af222312-0f70-471d-9291-bfda79ee210b"
   },
   "outputs": [],
   "source": [
    "evaluator = Eval(env, \"/content/models/_actor_critic_actor_network\", 10)\n",
    "evaluator.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAf4X8UuX24T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.states = []\n",
    "        self.new_states = []\n",
    "        self.actions = []\n",
    "\n",
    "    def remember(self, state, action, new_state, reward, value, log_prob):\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.states.append(state)\n",
    "        self.new_states.append(new_state)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.actions = []\n",
    "        self.new_states = []\n",
    "        self.states = []\n",
    "\n",
    "    def sample_memory(self):\n",
    "        return self.states, self.actions, self.new_states, self.rewards,\\\n",
    "               self.values, self.log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "class ParallelEnv:\n",
    "    def __init__(self, env_id, global_idx,\n",
    "                 input_shape, n_actions, num_threads, icm=False):\n",
    "        names = [str(i) for i in range(num_threads)]\n",
    "        print(\"in\")\n",
    "        global_actor_critic = ActorCritic(input_shape, n_actions)\n",
    "        global_actor_critic.share_memory()\n",
    "        global_optim = SharedAdam(global_actor_critic.parameters(), lr=1e-4)\n",
    "\n",
    "        if icm:\n",
    "            global_icm = ICM(input_shape, n_actions)\n",
    "            global_icm.share_memory()\n",
    "            global_icm_optim = SharedAdam(global_icm.parameters(), lr=1e-4)\n",
    "        else:\n",
    "            global_icm = None\n",
    "            global_icm_optim = None\n",
    "\n",
    "        self.ps = [mp.Process(target=worker,\n",
    "                              args=(name, input_shape, n_actions,\n",
    "                                    global_actor_critic, global_optim, env_id,\n",
    "                                    num_threads, global_idx, global_icm,\n",
    "                                    global_icm_optim, icm))\n",
    "                   for name in names]\n",
    "\n",
    "        [p.start() for p in self.ps]\n",
    "        [p.join() for p in self.ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions=3, alpha=0.1, beta=0.2):\n",
    "        super(ICM, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.phi = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.inverse = nn.Linear(288*2, 256)\n",
    "        self.pi_logits = nn.Linear(256, n_actions)\n",
    "\n",
    "        self.dense1 = nn.Linear(288+1, 256)\n",
    "        self.phi_hat_new = nn.Linear(256, 288)\n",
    "\n",
    "        device = T.device('cpu')\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, new_state, action):\n",
    "        conv = F.elu(self.conv1(state))\n",
    "        conv = F.elu(self.conv2(conv))\n",
    "        conv = F.elu(self.conv3(conv))\n",
    "        phi = self.phi(conv)\n",
    "\n",
    "        conv_new = F.elu(self.conv1(new_state))\n",
    "        conv_new = F.elu(self.conv2(conv_new))\n",
    "        conv_new = F.elu(self.conv3(conv_new))\n",
    "        phi_new = self.phi(conv_new)\n",
    "\n",
    "        # [T, 32, 3, 3] to [T, 288]\n",
    "        phi = phi.view(phi.size()[0], -1).to(T.float)\n",
    "        phi_new = phi_new.view(phi_new.size()[0], -1).to(T.float)\n",
    "\n",
    "        inverse = self.inverse(T.cat([phi, phi_new], dim=1))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "\n",
    "        # from [T] to [T, 1]\n",
    "        action = action.reshape((action.size()[0], 1))\n",
    "        forward_input = T.cat([phi, action], dim=1)\n",
    "        dense = self.dense1(forward_input)\n",
    "        phi_hat_new = self.phi_hat_new(dense)\n",
    "\n",
    "        return phi_new, pi_logits, phi_hat_new\n",
    "\n",
    "    def calc_loss(self, states, new_states, actions):\n",
    "        # don't need [] b/c these are lists of states\n",
    "        states = T.tensor(states, dtype=T.float)\n",
    "        actions = T.tensor(actions, dtype=T.float)\n",
    "        new_states = T.tensor(new_states, dtype=T.float)\n",
    "\n",
    "        phi_new, pi_logits, phi_hat_new = \\\n",
    "            self.forward(states, new_states, actions)\n",
    "\n",
    "        inverse_loss = nn.CrossEntropyLoss()\n",
    "        L_I = (1 - self.beta) * inverse_loss(pi_logits, actions.to(T.long))\n",
    "\n",
    "        forward_loss = nn.MSELoss()\n",
    "        L_F = self.beta * forward_loss(phi_hat_new, phi_new)\n",
    "\n",
    "        intrinsic_reward = self.alpha*0.5*((phi_hat_new-phi_new).pow(2)).mean(dim=1)\n",
    "        return intrinsic_reward, L_I, L_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions, gamma=0.99, tau=1.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        conv_shape = self.calc_conv_output(input_dims)\n",
    "\n",
    "        self.gru = nn.GRUCell(conv_shape, 256)\n",
    "        self.pi = nn.Linear(256, n_actions)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "\n",
    "    def calc_conv_output(self, input_dims):\n",
    "        state = T.zeros(1, *input_dims)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        dims = self.conv4(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, state, hx):\n",
    "        conv = F.elu(self.conv1(state))\n",
    "        conv = F.elu(self.conv2(conv))\n",
    "        conv = F.elu(self.conv3(conv))\n",
    "        conv = F.elu(self.conv4(conv))\n",
    "\n",
    "        conv_state = conv.view((conv.size()[0], -1))\n",
    "\n",
    "        hx = self.gru(conv_state, (hx))\n",
    "\n",
    "        pi = self.pi(hx)\n",
    "        v = self.v(hx)\n",
    "\n",
    "        probs = T.softmax(pi, dim=1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.numpy()[0], v, log_prob, hx\n",
    "\n",
    "    def calc_R(self, done, rewards, values):\n",
    "        values = T.cat(values).squeeze()\n",
    "\n",
    "        if len(values.size()) == 1:  # batch of states\n",
    "            R = values[-1]*(1-int(done))\n",
    "        elif len(values.size()) == 0:  # single state\n",
    "            R = values*(1-int(done))\n",
    "\n",
    "        batch_return = []\n",
    "        for reward in rewards[::-1]:\n",
    "            R = reward + self.gamma * R\n",
    "            batch_return.append(R)\n",
    "        batch_return.reverse()\n",
    "        batch_return = T.tensor(batch_return,\n",
    "                                dtype=T.float).reshape(values.size())\n",
    "        return batch_return\n",
    "\n",
    "    def calc_cost(self, new_state, hx, done,\n",
    "                  rewards, values, log_probs, intrinsic_reward=None):\n",
    "\n",
    "        if intrinsic_reward is not None:\n",
    "            rewards += intrinsic_reward.detach().numpy()\n",
    "\n",
    "        returns = self.calc_R(done, rewards, values)\n",
    "\n",
    "        next_v = T.zeros(1, 1) if done else self.forward(T.tensor(\n",
    "                                        [new_state], dtype=T.float), hx)[1]\n",
    "        values.append(next_v.detach())\n",
    "        values = T.cat(values).squeeze()\n",
    "        log_probs = T.cat(log_probs)\n",
    "        rewards = T.tensor(rewards)\n",
    "\n",
    "        delta_t = rewards + self.gamma * values[1:] - values[:-1]\n",
    "        n_steps = len(delta_t)\n",
    "        gae = np.zeros(n_steps)\n",
    "        for t in range(n_steps):\n",
    "            for k in range(0, n_steps-t):\n",
    "                temp = (self.gamma*self.tau)**k * delta_t[t+k]\n",
    "                gae[t] += temp\n",
    "        gae = T.tensor(gae, dtype=T.float)\n",
    "\n",
    "        actor_loss = -(log_probs * gae).sum()\n",
    "        # if single then values is rank 1 and returns rank 0\n",
    "        # want to have same shape to avoid a warning\n",
    "        critic_loss = F.mse_loss(values[:-1].squeeze(), returns)\n",
    "\n",
    "        entropy_loss = (-log_probs * T.exp(log_probs)).sum()\n",
    "\n",
    "        total_loss = actor_loss + critic_loss - 0.01 * entropy_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "\n",
    "\n",
    "class SharedAdam(T.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,\n",
    "                                         weight_decay=weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = T.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = T.zeros_like(p.data)\n",
    "\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "def worker(name, input_shape, n_actions, global_agent,\n",
    "           optimizer, env_id, n_threads, global_idx, global_icm,\n",
    "           icm_optimizer, icm):\n",
    "    T_MAX = 20\n",
    "\n",
    "    local_agent = ActorCritic(input_shape, n_actions)\n",
    "\n",
    "    if icm:\n",
    "        local_icm = ICM(input_shape, n_actions)\n",
    "    else:\n",
    "        local_icm = None\n",
    "        intrinsic_reward = None\n",
    "\n",
    "    memory = Memory()\n",
    "\n",
    "    frame_buffer = [input_shape[1], input_shape[2], 1]\n",
    "    env = make_env(env_id, shape=frame_buffer)\n",
    "\n",
    "    episode, max_steps, t_steps, scores = 0, 5e5, 0, []\n",
    "\n",
    "    while episode < max_steps:\n",
    "        obs = env.reset()\n",
    "        score, done, ep_steps = 0, False, 0\n",
    "        hx = T.zeros(1, 256)\n",
    "        while not done:\n",
    "            state = T.tensor([obs], dtype=T.float)\n",
    "            action, value, log_prob, hx = local_agent(state, hx)\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            memory.remember(obs, action, obs_, reward, value, log_prob)\n",
    "            score += reward\n",
    "            obs = obs_\n",
    "            ep_steps += 1\n",
    "            t_steps += 1\n",
    "            if ep_steps % T_MAX == 0 or done:\n",
    "                states, actions, new_states, rewards, values, log_probs = \\\n",
    "                        memory.sample_memory()\n",
    "                if icm:\n",
    "                    intrinsic_reward, L_I, L_F = \\\n",
    "                            local_icm.calc_loss(states, new_states, actions)\n",
    "\n",
    "                loss = local_agent.calc_cost(obs, hx, done, rewards,\n",
    "                                             values, log_probs,\n",
    "                                             intrinsic_reward)\n",
    "                optimizer.zero_grad()\n",
    "                hx = hx.detach_()\n",
    "                if icm:\n",
    "                    icm_optimizer.zero_grad()\n",
    "                    (L_I + L_F).backward()\n",
    "                loss.backward()\n",
    "                T.nn.utils.clip_grad_norm_(local_agent.parameters(), 40)\n",
    "                for local_param, global_param in zip(\n",
    "                                        local_agent.parameters(),\n",
    "                                        global_agent.parameters()):\n",
    "                    global_param._grad = local_param.grad\n",
    "                optimizer.step()\n",
    "                local_agent.load_state_dict(global_agent.state_dict())\n",
    "\n",
    "                if icm:\n",
    "                    for local_param, global_param in zip(\n",
    "                                        local_icm.parameters(),\n",
    "                                        global_icm.parameters()):\n",
    "                        global_param._grad = local_param.grad\n",
    "                    icm_optimizer.step()\n",
    "                    local_icm.load_state_dict(global_icm.state_dict())\n",
    "\n",
    "                memory.clear_memory()\n",
    "        episode += 1\n",
    "        # with global_idx.get_lock():\n",
    "        #    global_idx.value += 1\n",
    "        if name == '1':\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_score_5000 = np.mean(scores[max(0, episode-5000): episode+1])\n",
    "            print('ICM episode {} thread {} of {} steps {:.2f}M score {:.2f} '\n",
    "                  'avg score (100) (5000) {:.2f} {:.2f}'.format(\n",
    "                                                episode, name, n_threads,\n",
    "                                                t_steps/1e6, score,\n",
    "                                                avg_score, avg_score_5000))\n",
    "    if name == '1':\n",
    "        x = [z for z in range(episode)]\n",
    "        plot_learning_curve(x, scores, 'ICM_hallway_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 games')\n",
    "    plt.savefig(figure_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "mp.set_start_method('spawn')\n",
    "global_ep = mp.Value('i', 0)\n",
    "    # env_id = 'PongNoFrameskip-v4'\n",
    "    # env_id = 'MiniWorld-Hallway-v0'\n",
    "env_id = 'MiniWorld-FourRooms-v0'\n",
    "n_threads = 12\n",
    "n_actions = 3\n",
    "input_shape = [4, 42, 42]\n",
    "env = ParallelEnv(env_id=env_id, num_threads=n_threads,\n",
    "                      n_actions=n_actions, global_idx=global_ep,\n",
    "                      input_shape=input_shape, icm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import cpu_count\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "CUR_EPISODE = 0\n",
    "\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(0.001)\n",
    "        self.entropy_beta = 0.01\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, actions, logits, advantages):\n",
    "        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        entropy_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = ce_loss(\n",
    "            actions, logits, sample_weight=tf.stop_gradient(advantages))\n",
    "        entropy = entropy_loss(logits, logits)\n",
    "        return policy_loss - self.entropy_beta * entropy\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(states, training=True)\n",
    "            loss = self.compute_loss(\n",
    "                actions, logits, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(0.0005)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.global_actor = Actor(self.state_dim, self.action_dim)\n",
    "        self.global_critic = Critic(self.state_dim)\n",
    "        self.num_workers = cpu_count()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            env = gym.make(self.env_name)\n",
    "            workers.append(WorkerAgent(\n",
    "                env, self.global_actor, self.global_critic, max_episodes))\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "\n",
    "class WorkerAgent(Thread):\n",
    "    def __init__(self, env, global_actor, global_critic, max_episodes):\n",
    "        Thread.__init__(self)\n",
    "        self.lock = Lock()\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "\n",
    "        self.max_episodes = max_episodes\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        self.actor = Actor(self.state_dim, self.action_dim)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "\n",
    "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = 0.99 * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "    def advatnage(self, td_targets, baselines):\n",
    "        return td_targets - baselines\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self):\n",
    "        global CUR_EPISODE\n",
    "\n",
    "        while self.max_episodes >= CUR_EPISODE:\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                probs = self.actor.model.predict(\n",
    "                    np.reshape(state, [1, self.state_dim]), verbose=False)\n",
    "                action = np.random.choice(self.action_dim, p=probs[0])\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append(reward)\n",
    "\n",
    "                if len(state_batch) >= 5 or done:\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "\n",
    "                    next_v_value = self.critic.model.predict(next_state, verbose=False)\n",
    "                    td_targets = self.n_step_td_target(\n",
    "                        rewards, next_v_value, done)\n",
    "                    advantages = td_targets - self.critic.model.predict(states, verbose=False)\n",
    "                    \n",
    "                    with self.lock:\n",
    "                        actor_loss = self.global_actor.train(\n",
    "                            states, actions, advantages)\n",
    "                        critic_loss = self.global_critic.train(\n",
    "                            states, td_targets)\n",
    "\n",
    "                        self.actor.model.set_weights(\n",
    "                            self.global_actor.model.get_weights())\n",
    "                        self.critic.model.set_weights(\n",
    "                            self.global_critic.model.get_weights())\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    td_target_batch = []\n",
    "                    advatnage_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(CUR_EPISODE, episode_reward))\n",
    "            CUR_EPISODE += 1\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_name = 'CartPole-v1'\n",
    "    agent = Agent(env_name)\n",
    "    agent.train()\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
