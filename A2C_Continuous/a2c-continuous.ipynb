{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium[atari] --quiet\n!pip install gymnasium --quiet\n!pip install -U gymnasium[atari] --quiet\n!pip install imageio_ffmpeg --quiet\n!pip install npy_append_array --quiet\n!pip install pyTelegramBotAPI --quiet\n!pip install gymnasium[accept-rom-license] --quiet\n!pip install gymnasium[box2d] --quiet","metadata":{"id":"bETUFZq-wePe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nimport itertools\nimport scipy.misc\nimport matplotlib.pyplot as plt\n\n\nclass gameOb():\n    def __init__(self,coordinates,size,intensity,channel,reward,name):\n        self.x = coordinates[0]\n        self.y = coordinates[1]\n        self.size = size\n        self.intensity = intensity\n        self.channel = channel\n        self.reward = reward\n        self.name = name\n        \nclass gameEnv():\n    def __init__(self,partial,size):\n        self.sizeX = size\n        self.sizeY = size\n        self.actions = 4\n        self.objects = []\n        self.partial = partial\n        a = self.reset()\n        plt.imshow(a,interpolation=\"nearest\")\n        \n        \n    def reset(self):\n        self.objects = []\n        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n        self.objects.append(hero)\n        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug)\n        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole)\n        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug2)\n        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole2)\n        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug3)\n        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug4)\n        state = self.renderEnv()\n        self.state = state\n        return state\n\n    def moveChar(self,direction):\n        # 0 - up, 1 - down, 2 - left, 3 - right\n        hero = self.objects[0]\n        heroX = hero.x\n        heroY = hero.y\n        penalize = 0.\n        if direction == 0 and hero.y >= 1:\n            hero.y -= 1\n        if direction == 1 and hero.y <= self.sizeY-2:\n            hero.y += 1\n        if direction == 2 and hero.x >= 1:\n            hero.x -= 1\n        if direction == 3 and hero.x <= self.sizeX-2:\n            hero.x += 1     \n        if hero.x == heroX and hero.y == heroY:\n            penalize = 0.0\n        self.objects[0] = hero\n        return penalize\n    \n    def newPosition(self):\n        iterables = [ range(self.sizeX), range(self.sizeY)]\n        points = []\n        for t in itertools.product(*iterables):\n            points.append(t)\n        currentPositions = []\n        for objectA in self.objects:\n            if (objectA.x,objectA.y) not in currentPositions:\n                currentPositions.append((objectA.x,objectA.y))\n        for pos in currentPositions:\n            points.remove(pos)\n        location = np.random.choice(range(len(points)),replace=False)\n        return points[location]\n\n    def checkGoal(self):\n        others = []\n        for obj in self.objects:\n            if obj.name == 'hero':\n                hero = obj\n            else:\n                others.append(obj)\n        ended = False\n        for other in others:\n            if hero.x == other.x and hero.y == other.y:\n                self.objects.remove(other)\n                if other.reward == 1:\n                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n                else: \n                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n                return other.reward,False\n        if ended == False:\n            return 0.0,False\n\n    def renderEnv(self):\n        #a = np.zeros([self.sizeY,self.sizeX,3])\n        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n        a[1:-1,1:-1,:] = 0\n        hero = None\n        for item in self.objects:\n            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n            if item.name == 'hero':\n                hero = item\n        if self.partial == True:\n            a = a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n        a = np.stack([b,c,d],axis=2)\n        return a\n\n    def step(self,action):\n        penalty = self.moveChar(action)\n        reward,done = self.checkGoal()\n        state = self.renderEnv()\n        if reward == None:\n            print(done)\n            print(reward)\n            print(penalty)\n            return state,(reward+penalty),done\n        else:\n            return state,(reward+penalty),","metadata":{"id":"SYJzThpJx_8e","execution":{"iopub.status.busy":"2023-03-18T03:52:38.347583Z","iopub.execute_input":"2023-03-18T03:52:38.348063Z","iopub.status.idle":"2023-03-18T03:52:38.376821Z","shell.execute_reply.started":"2023-03-18T03:52:38.348026Z","shell.execute_reply":"2023-03-18T03:52:38.375724Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras.layers import Dense, Conv2D, Input, Lambda\n \nclass ActorNetwork(tf.keras.Model):\n    def __init__(self, input_dims, action_bound, std_bound, action_dim=1):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = Dense(256, activation=\"relu\", input_shape=input_dims, kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.out_mu = Dense(action_dim, activation='tanh')\n        self.mu_output = Lambda(lambda x: x * action_bound)\n        self.std_output = Dense(action_dim, activation='softplus')\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        out_mu = self.out_mu(x)\n        mu_output = self.mu_output(out_mu)\n        std_output = self.std_output(x)\n        return mu_output, std_output\n\n\nclass CriticNetwork(tf.keras.Model):\n    def __init__(self, input_dims, action_dim=1):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = Dense(256, activation=\"relu\", input_shape=input_dims, kernel_initializer=\"he_uniform\")\n        self.fc2 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")\n        self.fc3 = Dense(1)\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","metadata":{"id":"kPM5Ptfjw799","execution":{"iopub.status.busy":"2023-03-18T04:09:33.219749Z","iopub.execute_input":"2023-03-18T04:09:33.220904Z","iopub.status.idle":"2023-03-18T04:09:33.231982Z","shell.execute_reply.started":"2023-03-18T04:09:33.220862Z","shell.execute_reply":"2023-03-18T04:09:33.230862Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"goC4wfIYw8eI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nimport tensorflow as tf \nimport tensorflow_probability as tfp\nimport numpy as np\n\nclass ActorCriticAgent:\n  \n    def __init__(self, input_dims, out_dims, action_bound, std_bound, gamma, lr1, lr2, action_space, batch_size, chkpt, algo_name): \n        self.gamma = gamma\n        self.input_dims = input_dims\n        self.out_dims = out_dims\n        self.batch_size = batch_size \n        self.action_space = action_space \n        self.action_bound = action_bound\n        self.std_bound = std_bound\n        self.action = None  \n        self.fname = chkpt + '_' + algo_name \n        self.actor_network = ActorNetwork(self.input_dims, self.action_bound, self.std_bound, self.out_dims)\n        self.actor_network.compile(optimizer=Adam(learning_rate=lr1))\n        self.critic_network = CriticNetwork(input_dims, out_dims)\n        self.critic_network.compile(optimizer=Adam(learning_rate=lr2))\n\n\n    def get_action(self, state): \n        state = np.reshape(state, [1, self.input_dims[0]])\n        mu, std = self.actor_network(state)\n        mu, std = mu[0], std[0]\n        return np.random.normal(mu, std, size=self.out_dims)\n \n    def save_models(self):\n        self.actor_network.save(self.fname + \"_\" + \"actor_network\")\n        self.critic_network.save(self.fname + \"_\" + \"critic_network\")\n        print(\"[+] Saving the model\")\n\n\n    def load_models(self):\n        self.actor_network = tf.keras.models.load_model(self.fname + \"_\" + \"actor_network\") \n        self.critic_network = tf.keras.models.load_model(self.fname + \"_\" + \"critic_network\") \n        print(\"[+] Loading the model\")\n\n  \n    def log_pdf(self, mu, std, action):\n        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n        var = std ** 2\n        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n            var * 2 * np.pi\n        )\n        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n    \n    def get_critic_prediction(self, state): \n        return self.critic_network(state)\n        \n    def compute_actor_loss(self, mu, std, actions, advantages):\n        log_policy_pdf = self.log_pdf(mu, std, actions)\n        loss_policy = log_policy_pdf * advantages\n        return tf.reduce_sum(-loss_policy)\n    \n    def learn(self, states, actions, advantages, td_targets): \n    \n        state = tf.convert_to_tensor(np.array(state).reshape(1, -1))\n        next_state = tf.convert_to_tensor(np.array(next_state).reshape(1, -1))    \n\n        with tf.GradientTape() as tape:\n            values = self.critic_network(states, training=True)\n            loss = self.compute_loss(values, td_targets)\n        \n        params = self.critic_network.trainable_variables\n        grads = tape.gradient(loss, params)\n        \n        self.critic_network.optimizer.apply_gradients(zip(grads, params))\n\n        with tf.GradientTape() as tape: \n            mu, std = self.actor_network(states, training=True)\n            mu, std = mu[0], std[0]\n            loss = self.compute_actor_loss(mu, std, actions, advantages)\n        \n        params = self.actor_network.trainable_variables\n        grads = tape.gradient(loss, params)\n        \n        self.actor_network.optimizer.apply_gradients(zip(grads, params))\n\n\n    def compute_loss(self, v_pred, td_targets):\n        mse = tf.keras.losses.MeanSquaredError()\n        return mse(td_targets, v_pred)","metadata":{"id":"DW0X3mELw8gJ","execution":{"iopub.status.busy":"2023-03-18T04:10:10.242495Z","iopub.execute_input":"2023-03-18T04:10:10.243198Z","iopub.status.idle":"2023-03-18T04:10:10.262965Z","shell.execute_reply.started":"2023-03-18T04:10:10.243151Z","shell.execute_reply":"2023-03-18T04:10:10.261755Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hWNg8-8uw8kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gymnasium as gym\nimport tensorflow as tf\nfrom gymnasium.wrappers import *\n\n\ndef manage_memory():\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\n\ndef plot_learning_curve(scores, figure_file):\n\n    x = [_ for _ in range(len(scores))]\n    running_avg = np.zeros(len(scores))\n    for i in range(len(running_avg)):\n        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n    plt.plot(x, running_avg, label=\"Avg reward for agent\", color=\"black\")\n    plt.plot(scores, label=\"Reward for agent\", color=\"red\")\n    plt.xlabel(\"episodes\")\n    plt.ylabel(\"rewards\")\n    plt.title('Running average of previous 100 scores')\n    plt.legend()\n    plt.savefig(figure_file)\n\n\ndef make_env(env_name): \n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    \n    if len(env.observation_space.shape) >= 3: \n        #env = AtariPreprocessing(env, 10, 4, 84, False, True)\n        env = ResizeObservation(env, 84)\n        env = GrayScaleObservation(env, keep_dim=False)\n        env = FrameStack(env, 4, lz4_compress=False)\n        env = NormalizeObservation(env)\n\n    return env","metadata":{"id":"-GcL3VRow8nS","execution":{"iopub.status.busy":"2023-03-18T04:02:18.179835Z","iopub.execute_input":"2023-03-18T04:02:18.180210Z","iopub.status.idle":"2023-03-18T04:02:18.191801Z","shell.execute_reply.started":"2023-03-18T04:02:18.180174Z","shell.execute_reply":"2023-03-18T04:02:18.190622Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"V5E1iSI07fi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Writer:\n    def __init__(self, fname): \n        self.fname = fname \n\n    def write_to_file(self, content): \n        with open(self.fname, \"a\") as file: \n            file.write(content + \"\\n\")\n\n    def read_file(self, fname):\n        with open(fname, \"r\") as file: \n            return file.read()\n            ","metadata":{"id":"3rq1BvlM7fl6","execution":{"iopub.status.busy":"2023-03-18T04:02:18.720073Z","iopub.execute_input":"2023-03-18T04:02:18.721323Z","iopub.status.idle":"2023-03-18T04:02:18.728087Z","shell.execute_reply.started":"2023-03-18T04:02:18.721269Z","shell.execute_reply":"2023-03-18T04:02:18.727003Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"FNtP-BTO7fpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom telebot import TeleBot\nimport datetime\nimport telebot\n\ntoken = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\nchat_id = \"1055055706\"\nbot = TeleBot(token=token) \n\ndef telegram_send(message, bot):\n    chat_id = \"1055055706\"\n    bot.send_message(chat_id=chat_id, text=message)\n\ndef welcome_msg(multi_step, double_dqn, dueling):\n    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n    telegram_send(st, bot)\n    \ndef info_msg(episode, max_episode, reward, best_score, loss): \n    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n    telegram_send(st, bot)\n\ndef end_msg(learning_time):\n    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n    telegram_send(st, bot)\n    print(st)\n","metadata":{"id":"WAxjX1V87fr4","execution":{"iopub.status.busy":"2023-03-18T03:52:40.259262Z","iopub.execute_input":"2023-03-18T03:52:40.259591Z","iopub.status.idle":"2023-03-18T03:52:40.268104Z","shell.execute_reply.started":"2023-03-18T03:52:40.259561Z","shell.execute_reply":"2023-03-18T03:52:40.267154Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"6Osd7ITs7fvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport imageio\n\n\nclass RecordVideo: \n    \n    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n        self.prefix_fname = prefix_fname\n        self.out_directory = out_directory\n        self.fps = fps\n        self.images = []\n        \n    def add_image(self, image): \n        self.images.append(image)\n    \n    def save(self, episode_no): \n        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n        self.images = []","metadata":{"id":"_trkbFOo7fy8","execution":{"iopub.status.busy":"2023-03-18T04:03:35.015338Z","iopub.execute_input":"2023-03-18T04:03:35.016125Z","iopub.status.idle":"2023-03-18T04:03:35.024401Z","shell.execute_reply.started":"2023-03-18T04:03:35.016083Z","shell.execute_reply":"2023-03-18T04:03:35.023040Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Trainer: \n    \n    def __init__(self, env, action_space, input_dims, out_dims, video_prefix, is_tg,\n                                 noe, max_steps, record, lr1, lr2, gamma, chkpt,\n                                 algo_name, update_interval, action_bound, std_bound): \n        self.env = env\n        self.noe = noe \n        self.max_steps = max_steps \n        self.input_dims = input_dims\n        self.out_dims = out_dims\n        self.update_interval = update_interval\n        self.action_bound = action_bound\n        self.std_bound = std_bound\n\n        self.recorder = RecordVideo(video_prefix)\n        self.is_tg = is_tg \n        self.record = record\n        self.agent = ActorCriticAgent(input_dims, out_dims, action_bound, std_bound,\n                                          gamma, lr1, lr2, action_space, 32, chkpt, algo_name)\n        \n    def td_target(self, reward, next_state, done):\n        if done:\n            return reward\n        v_value = self.agent.get_critic_prediction(\n            np.reshape(next_state, [1, self.input_dims]))\n        return np.reshape(reward + args.gamma * v_value[0], [1, 1])\n    \n    def list_to_batch(self, list):\n        batch = list[0]\n        for elem in list[1:]:\n            batch = np.append(batch, elem, axis=0)\n        return batch\n    \n    def advatnage(self, td_targets, baselines):\n        return td_targets - baselines\n    \n    def list_to_batch(self, list):\n        batch = list[0]\n        for elem in list[1:]:\n            batch = np.append(batch, elem, axis=0)\n        return batch\n    \n    def train(self): \n\n        ep_rewards = []\n        avg_rewards = []\n        best_reward = float(\"-inf\")\n        done = False\n\n        for episode in range(self.noe): \n            state_batch = []\n            action_batch = []\n            td_target_batch = []\n            advatnage_batch = []\n            \n            state = self.env.reset()\n            rewards = 0 \n\n            if self.record and episode % 50 == 0: \n                img = self.env.render()\n                self.recorder.add_image(img)\n            \n            step = 0\n            while not done or step <= self.max_steps:\n               \n                if type(state) == tuple: \n                    state = state[0]\n                \n             \n                action = self.agent.get_action(state)\n                action = np.clip(action, -self.action_bound, self.action_bound)\n                next_info = self.env.step(action)\n                next_state, reward, terminated, truncated, _ = next_info \n                done = terminated or truncated \n                rewards += reward\n                \n                state = np.reshape(state, [1, self.input_dims[0]])\n                action = np.reshape(action, [1, self.out_dims])\n                next_state = np.reshape(next_state, [1, self.input_dims[0]])\n                reward = np.reshape(reward, [1, 1])\n\n                td_target = self.td_target((reward), next_state, done)\n                advantage = self.advatnage(\n                                    td_target, self.agent.get_critic_prediction(state)\n                            )\n\n                state_batch.append(state)\n                action_batch.append(action)\n                td_target_batch.append(td_target)\n                advatnage_batch.append(advantage) \n                \n                state = next_state\n                step += 1 \n                \n                if self.update_interval % step == 0 or done:\n                    states = self.list_to_batch(state_batch)\n                    actions = self.list_to_batch(action_batch)\n                    td_targets = self.list_to_batch(td_target_batch)\n                    advantages = self.list_to_batch(advatnage_batch)\n                    \n                    self.agent.learn(states, actions, advantages, td_targets)\n                    \n                    state_batch = []\n                    action_batch = []\n                    td_target_batch = []\n                    advatnage_batch = []\n                \n\n                if self.record and episode % 50 == 0:\n                    img = self.env.render()\n                    self.recorder.add_image(img)\n                    \n\n            if self.record and episode % 50 == 0:\n                self.recorder.save(episode)\n\n            if rewards > best_reward: \n                self.agent.save_models()\n                best_reward = rewards\n\n            ep_rewards.append(rewards)\n            avg_reward = np.mean(ep_rewards[-100:])\n            avg_rewards.append(avg_reward)\n            print(f\"Episode: {episode} Reward: {rewards} Best Score: {best_reward}, Average Reward: {avg_reward}\")\n\n        return ep_rewards, avg_rewards\n\n","metadata":{"id":"aSEo-Nz4w8ph","execution":{"iopub.status.busy":"2023-03-18T04:15:40.372132Z","iopub.execute_input":"2023-03-18T04:15:40.372584Z","iopub.status.idle":"2023-03-18T04:15:40.394489Z","shell.execute_reply.started":"2023-03-18T04:15:40.372547Z","shell.execute_reply":"2023-03-18T04:15:40.393165Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Q63NvNRI7s0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make_env(\"MountainCarContinuous-v0\")\nn_actions = env.action_space.shape[0]\ninput_dims = env.observation_space.shape\nnoe = 1000 \nprint(input_dims, n_actions)\nmax_steps = 1000000\nvideo_prefix = \"actor_critic\"\nis_tg = True \nrecord = True\nlr1 = 1e-4\nlr2 = 1e-4\ngamma = 0.95\nchpkt = 'models/'\nalgo_name = \"actor_critic\"\n\naction_bound = env.action_space.high[0]\nstd_bound = [1e-2, 1.0]\nupdate_interval = 20\n\nif __name__ == \"__main__\": \n  \n    trainer = Trainer(env, action_space, input_dims, n_actions, video_prefix,\n                              is_tg, noe, max_steps, record, lr1, lr2, gamma,\n                              chpkt, algo_name, update_interval, action_bound, std_bound)\n    ep_rewards = trainer.train()\n    plot_learning_curve(ep_rewards, \"actor_critic.png\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"md8-b_oY7s20","outputId":"d9fd7615-a91b-4595-8e72-db2de6ab8e18","execution":{"iopub.status.busy":"2023-03-18T04:15:40.779801Z","iopub.execute_input":"2023-03-18T04:15:40.780379Z","iopub.status.idle":"2023-03-18T04:15:40.902105Z","shell.execute_reply.started":"2023-03-18T04:15:40.780336Z","shell.execute_reply":"2023-03-18T04:15:40.900648Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"(2,) 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_658744/2441348944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0mis_tg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                               chpkt, algo_name, update_interval, action_bound, std_bound)\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mep_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"actor_critic.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_658744/473923539.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtd_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 advantage = self.advatnage(\n\u001b[1;32m     84\u001b[0m                                     \u001b[0mtd_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_critic_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_658744/473923539.py\u001b[0m in \u001b[0;36mtd_target\u001b[0;34m(self, reward, next_state, done)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         v_value = self.agent.get_critic_prediction(\n\u001b[0;32m---> 25\u001b[0;31m             np.reshape(next_state, [1, self.input_dims]))\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Call _wrapit from within the except clause to ensure a potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# exception has a traceback chain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"],"ename":"TypeError","evalue":"'tuple' object cannot be interpreted as an integer","output_type":"error"}]},{"cell_type":"code","source":"import pickle \n\nwith open(\"actor_critic_eps_rewards.obj\", \"wb\") as f: \n  pickle.dump(ep_rewards[0], f)\n\nwith open(\"actor_critic_avg_rewards.obj\", \"wb\") as f: \n  pickle.dump(ep_rewards[1], f)","metadata":{"id":"DwAbSdSq7s5t","execution":{"iopub.status.busy":"2023-03-18T03:52:42.839511Z","iopub.execute_input":"2023-03-18T03:52:42.840140Z","iopub.status.idle":"2023-03-18T03:52:42.867245Z","shell.execute_reply.started":"2023-03-18T03:52:42.840103Z","shell.execute_reply":"2023-03-18T03:52:42.865729Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_658744/1545450532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actor_critic_eps_rewards.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actor_critic_avg_rewards.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ep_rewards' is not defined"],"ename":"NameError","evalue":"name 'ep_rewards' is not defined","output_type":"error"}]},{"cell_type":"code","source":"plot_learning_curve(ep_rewards[0], \"actor_critic.png\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"qaS-tMXXV46C","outputId":"9e862b2a-bcea-4d63-afd5-90ce5b9c0084"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_policy(observation, q_val_network, action_space): \n    state = tf.convert_to_tensor([observation])\n    actions = q_val_network(state)\n    action = tf.math.argmax(actions, axis=1).numpy()[0]\n    return action","metadata":{"id":"SOg3R5Lg7s9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \n\nclass Eval: \n\n    def __init__(self, env, model_path, number_of_episode=50):\n        self.env = env \n        self.model = tf.keras.models.load_model(model_path)\n        self.recorder = RecordVideo('dqn_lunarlander', 'test_videos/', 15)\n        self.number_of_episode = number_of_episode\n        \n    def test(self): \n        rewards = []\n        steps = []\n        for episode in range(self.number_of_episode): \n            done = False\n            reward = 0\n            step = 0\n            state = env.reset(seed=random.randint(0,500))\n            if episode % 10 == 0: \n                img = env.render()\n                self.recorder.add_image(img) \n\n            while not done:\n\n                if type(state) == tuple: \n                  state = state[0]\n                action =  greedy_policy(state, self.model, action_space)\n                state, reward_prob, terminated, truncated, _ = env.step(action)\n                done = terminated or truncated \n                reward += reward_prob\n                step += 1 \n                if episode % 10 == 0:\n                    img = env.render()\n                    self.recorder.add_image(img)\n            \n            rewards.append(reward)\n            steps.append(step)\n            self.recorder.save(1) if episode % 10 == 0 else None \n        \n        return rewards, steps\n","metadata":{"id":"vNGxYRRf7s_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator = Eval(env, \"/content/models/_actor_critic_actor_network\", 10)\nevaluator.test()","metadata":{"id":"7k-z8PD2w8rZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af222312-0f70-471d-9291-bfda79ee210b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UAf4X8UuX24T"},"execution_count":null,"outputs":[]}]}