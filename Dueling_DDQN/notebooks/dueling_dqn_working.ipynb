{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXAptd1q1Ma2"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[atari] --quiet\n",
        "!pip install gymnasium --quiet\n",
        "!pip install -U gymnasium[atari] --quiet\n",
        "!pip install imageio_ffmpeg --quiet\n",
        "!pip install npy_append_array --quiet\n",
        "!pip install pyTelegramBotAPI --quiet\n",
        "!pip install gymnasium[accept-rom-license] --quiet\n",
        "!pip install gymnasium[box2d] --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mRGrVhqy1MbB"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "class ExperienceReplayBuffer: \n",
        "    def __init__(self, max_memory, input_shape, n_actions): \n",
        "        self.mem_size = max_memory\n",
        "        self.mem_counter = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                     dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                         dtype=np.float32)\n",
        "\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done): \n",
        "        index = self.mem_counter % self.mem_size \n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = next_state\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_counter += 1\n",
        "        \n",
        "\n",
        "    def sample_experience(self, batch_size):\n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        batch_index = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch_index]\n",
        "        next_states = self.new_state_memory[batch_index]\n",
        "        rewards = self.reward_memory[batch_index]\n",
        "        actions = self.action_memory[batch_index]\n",
        "        terminal = self.terminal_memory[batch_index]\n",
        "\n",
        "        return states, actions, rewards, next_states, terminal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-T7q-rzI9Jej"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPool2D, Input, Add, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class DeepQNetwork2D(keras.Model):\n",
        "    def __init__(self, input_dims, n_actions):\n",
        "        super(DeepQNetwork2D, self).__init__()\n",
        "     #   self.fc1 = Dense(64, activation='relu')\n",
        "        self.fc1 = Dense(64, activation='relu')\n",
        "        self.fc2 = Dense(64, activation='relu')\n",
        "        \n",
        "        self.value_output = Dense(1)\n",
        "        self.advantage_output = Dense(n_actions)\n",
        "        self.add = Add()\n",
        "\n",
        "    def call(self, state):\n",
        "\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        value_output = self.value_output(x)\n",
        "        advantage_output = self.advantage_output(x)\n",
        "        output = self.add([value_output, advantage_output])\n",
        "        return output\n",
        "\n",
        "\n",
        "class DeepQNetwork3D(keras.Model): \n",
        "    def __init__(self, input_dims, n_actions):\n",
        "        super(DeepQNetwork3D, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2D(32, 8, strides=(4, 4), activation='relu', data_format=\"channels_first\", input_shape=input_dims)\n",
        "        self.conv2 = Conv2D(32, 4, strides=(2, 2), activation='relu', data_format=\"channels_first\")\n",
        "        self.conv3 = Conv2D(64, 3, strides=(1, 1), activation='relu', data_format=\"channels_first\")\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "        self.fc2 = Dense(128, activation='relu')\n",
        "        value_output = Dense(1)(backbone_2)\n",
        "        advantage_output = Dense(self.action_dim)(backbone_2)\n",
        "        output = Add()([value_output, advantage_output])\n",
        "\n",
        "\n",
        "    def call(self, state):\n",
        "\n",
        "        x = self.conv1(state)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        A = self.A(x)\n",
        "        V = self.V(x)\n",
        "        return V, A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lNM-BVCr1MbE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy_policy(q_val_network, observation, action_space, epsilon):\n",
        "        if np.random.random() > epsilon:\n",
        "            state = tf.convert_to_tensor([observation])\n",
        "            actions = q_val_network.predict(state, verbose=0)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "        else:\n",
        "            action = np.random.choice(action_space)\n",
        "        return action\n",
        "\n",
        "\n",
        "def greedy_policy(observation, q_val_network, action_space): \n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    actions = q_val_network(state)\n",
        "    action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1VaoOXKO1MbG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/blob/master/DQN/tf2/agent.py\n",
        "class Agent: \n",
        "    def __init__(self, agent_params):\n",
        "        # Parameters\n",
        "        self.gamma = agent_params.get(\"gamma\")\n",
        "        self.lr = agent_params.get(\"lr\")\n",
        "        self.input_dims = agent_params.get(\"input_dims\")\n",
        "        self.batch_size = agent_params.get(\"batch_size\")\n",
        "        self.replace_target_weight_counter = agent_params.get(\"replace\")\n",
        "        self.algo = agent_params.get(\"algo\")\n",
        "        self.env_name = agent_params.get(\"env_name\")\n",
        "        self.chkpt_dir = agent_params.get(\"chkpt_dir\")\n",
        "        self.n_actions = agent_params.get(\"n_actions\")\n",
        "        self.action_space = agent_params.get('actions')\n",
        "        \n",
        "        self.eps = agent_params.get(\"eps\")\n",
        "        self.min_eps = agent_params.get(\"min_eps\")\n",
        "        self.eps_decay_rate = agent_params.get(\"eps_decay_rate\")\n",
        "        \n",
        "        self.learn_step_counter = 0\n",
        "        self.fname = self.chkpt_dir + self.env_name + '_' + self.algo + '_'\n",
        "        self.mem_size = agent_params.get(\"mem_size\")\n",
        "\n",
        "        # networks and replaybuffer\n",
        "        self.memory = ExperienceReplayBuffer(self.mem_size, self.input_dims, self.n_actions)\n",
        "        self.q_value_network = DeepQNetwork2D(self.input_dims, self.n_actions) if len(self.input_dims) < 3 else \\\n",
        "                                                        DeepQNetwork3D(self.input_dims, self.n_actions)\n",
        "        self.q_value_network.compile(optimizer=Adam(learning_rate=self.lr))\n",
        "        self.target_q_network = DeepQNetwork2D(self.input_dims, self.n_actions) if len(self.input_dims) < 3 else \\\n",
        "                                                        DeepQNetwork3D(self.input_dims, self.n_actions)\n",
        "        self.target_q_network.compile(optimizer=Adam(learning_rate=self.lr))\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_value_network.save(self.fname+'q_value')\n",
        "        self.target_q_network.save(self.fname+'target_q')\n",
        "        print('... models saved successfully ...')\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_value_network = keras.models.load_model(self.fname+'q_value')\n",
        "        self.target_q_network = keras.models.load_model(self.fname+'target_q')\n",
        "        print('... models loaded successfully ...')\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_, done):\n",
        "        self.memory.store_experience(state, action, reward, state_, done)\n",
        "\n",
        "    def sample_experience(self):\n",
        "        state, action, reward, new_state, done = \\\n",
        "                                  self.memory.sample_experience(self.batch_size)\n",
        "        states = tf.convert_to_tensor(state)\n",
        "        rewards = tf.convert_to_tensor(reward)\n",
        "        dones = tf.convert_to_tensor(done)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.int32)\n",
        "        states_ = tf.convert_to_tensor(new_state)\n",
        "        return states, actions, rewards, states_, dones\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.eps:\n",
        "            state = tf.convert_to_tensor([observation])\n",
        "            actions = self.q_value_network(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        return action\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.learn_step_counter % self.replace_target_weight_counter == 0:\n",
        "            self.target_q_network.set_weights(self.q_value_network.get_weights())\n",
        "    \n",
        "    def decrement_epsilon(self): \n",
        "        self.eps -= self.eps_decay_rate\n",
        "        self.eps = max(self.eps, self.min_eps)\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.replace_target_network()\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.sample_experience()\n",
        "        rewards, actions, dones = rewards.numpy(), actions.numpy(), dones.numpy()\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_pred = self.q_value_network(states)\n",
        "            q_eval = self.target_q_network(states_)\n",
        "\n",
        "            q_target = tf.identity(q_pred)\n",
        "            q_target = q_target.numpy()\n",
        "            q_eval = q_eval.numpy()\n",
        "\n",
        "\n",
        "            batch_rewards = rewards\n",
        "            batch_max_vals = np.array([self.gamma * max(val) for val in q_eval])\n",
        "            batch_dones = [1 - int(done) for done in dones]\n",
        "\n",
        "            q_target[[np.arange(self.batch_size)], actions] =  batch_rewards + batch_max_vals * batch_dones \n",
        "\n",
        "            loss = keras.losses.MSE(q_pred, q_target)\n",
        "\n",
        "        params = self.q_value_network.trainable_variables\n",
        "        grads = tape.gradient(loss, params)\n",
        "\n",
        "        self.q_value_network.optimizer.apply_gradients(zip(grads, params))\n",
        "\n",
        "        self.learn_step_counter += 1\n",
        "        \n",
        "        self.decrement_epsilon()\n",
        "\n",
        "        \n",
        "        return self.eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BNvLxTwp1MbH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Tiz4C1Qz1MbI"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "from gymnasium.wrappers import *\n",
        "\n",
        "\n",
        "def manage_memory():\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "\n",
        "\n",
        "def plot_learning_curve(scores, epsilons, filename, lines=None):\n",
        "    x = [_ for _ in range(len(scores))]\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)\n",
        "\n",
        "\n",
        "def make_env(env_name, video_file_name, episode_freq_fo_video): \n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    \n",
        "    if len(env.observation_space.shape) >= 3: \n",
        "        #env = AtariPreprocessing(env, 10, 4, 84, False, True)\n",
        "        env = ResizeObservation(env, 84)\n",
        "        env = GrayScaleObservation(env, keep_dim=False)\n",
        "        env = FrameStack(env, 4, lz4_compress=False)\n",
        "        env = NormalizeObservation(env)\n",
        "\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jZ4fYqeS1MbJ"
      },
      "outputs": [],
      "source": [
        "class Writer:\n",
        "    def __init__(self, fname): \n",
        "        self.fname = fname \n",
        "\n",
        "    def write_to_file(self, content): \n",
        "        with open(self.fname, \"a\") as file: \n",
        "            file.write(content + \"\\n\")\n",
        "\n",
        "    def read_file(self, fname):\n",
        "        with open(fname, \"r\") as file: \n",
        "            return file.read()\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CjiJPh3h1MbK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from telebot import TeleBot\n",
        "import datetime\n",
        "import telebot\n",
        "\n",
        "token = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\n",
        "chat_id = \"1055055706\"\n",
        "bot = TeleBot(token=token) \n",
        "\n",
        "def telegram_send(message, bot):\n",
        "    chat_id = \"1055055706\"\n",
        "    bot.send_message(chat_id=chat_id, text=message)\n",
        "\n",
        "def welcome_msg(multi_step, double_dqn, dueling):\n",
        "    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n",
        "    telegram_send(st, bot)\n",
        "    \n",
        "def info_msg(episode, max_episode, reward, best_score, loss): \n",
        "    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n",
        "    telegram_send(st, bot)\n",
        "\n",
        "def end_msg(learning_time):\n",
        "    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n",
        "    telegram_send(st, bot)\n",
        "    print(st)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uMSnlRbo1MbM"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import imageio\n",
        "\n",
        "\n",
        "class RecordVideo: \n",
        "    \n",
        "    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n",
        "        self.prefix_fname = prefix_fname\n",
        "        self.out_directory = out_directory\n",
        "        self.fps = fps\n",
        "        self.images = []\n",
        "        \n",
        "    def add_image(self, image): \n",
        "        self.images.append(image)\n",
        "    \n",
        "    def save(self, episode_no): \n",
        "        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n",
        "        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n",
        "        self.images = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ofHJ4SXr1MbN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l308D8Hf1MbN"
      },
      "outputs": [],
      "source": [
        "from npy_append_array import NpyAppendArray\n",
        "import numpy as np\n",
        "\n",
        "class Trainer:   \n",
        "    def __init__(self, env, trainer_params): \n",
        "       \n",
        "        self.env = env \n",
        "        self.noe = trainer_params.get(\"noe\")\n",
        "        self.max_steps = trainer_params.get(\"max_steps\")\n",
        "       \n",
        "        self.eps_decay_rate = trainer_params.get(\"eps_decay_rate\")\n",
        "        self.action_space = trainer_params.get(\"action_space\")\n",
        "        self.is_tg = trainer_params.get(\"is_tg\")\n",
        "        self.tg_bot_freq_epi = trainer_params.get(\"tg_bot_freq_epi\")\n",
        "        self.record = trainer_params.get(\"record\")\n",
        "        self.agent_params = {\n",
        "                        \"gamma\":  trainer_params.get(\"gamma\"), \n",
        "                        \"lr\":  trainer_params.get(\"lr\"), \n",
        "                        \"input_dims\":  trainer_params.get(\"input_dims\"),\n",
        "                        \"mem_size\" :  trainer_params.get(\"mem_size\"),\n",
        "                        \"batch_size\" :  trainer_params.get(\"batch_size\"),\n",
        "                        \"replace\" :  trainer_params.get(\"replace\"),\n",
        "                        \"algo\" :  trainer_params.get(\"algo\"),\n",
        "                        \"env_name\" :  trainer_params.get(\"env_name\"),\n",
        "                        \"n_actions\" :  trainer_params.get(\"n_actions\"),\n",
        "                        \"chkpt_dir\":  trainer_params.get(\"chkpt_dir\"),\n",
        "                        \"actions\":  trainer_params.get(\"actions\"),\n",
        "                        \"eps\": trainer_params.get(\"eps\"),\n",
        "                        \"min_eps\": trainer_params.get(\"min_eps\"),\n",
        "                        \"eps_decay_rate\": trainer_params.get(\"eps_decay_rate\"),\n",
        "                        \n",
        "                    }\n",
        "        \n",
        "        self.agent = Agent(self.agent_params)\n",
        "        self.checkpoint = trainer_params.get(\"checkpoint\")\n",
        "        \n",
        "        self.writer = Writer(\"model_training_results.txt\")\n",
        "        self.recorder = RecordVideo(trainer_params.get(\"video_prefix\"), \"videos/\", 20)\n",
        "        \n",
        "        self.target_score = trainer_params.get(\"target_score\")\n",
        "\n",
        "    def train_rl_model(self): \n",
        "        episode_rewards = []\n",
        "        epsilon_history = []\n",
        "        avg_rewards = []\n",
        "        best_reward = float(\"-inf\")\n",
        "\n",
        "        if self.checkpoint:\n",
        "          self.agent.load_models()\n",
        "\n",
        "        for episode in range(self.noe): \n",
        "            n_steps = 0 \n",
        "            episodic_loss = 0\n",
        "            state = self.env.reset()\n",
        "            reward = 0 \n",
        "            \n",
        "            if self.record and episode%100==0: \n",
        "                img = self.env.render()\n",
        "                self.recorder.add_image(img)\n",
        "\n",
        "            for step in range(self.max_steps): \n",
        "                \n",
        "                if self.record and episode%100==0: \n",
        "                    img = self.env.render()\n",
        "                    self.recorder.add_image(img)\n",
        "\n",
        "                if type(state) == tuple: \n",
        "                    state = state[0]\n",
        "                state = state\n",
        "\n",
        "                action = self.agent.choose_action(state)\n",
        "\n",
        "                next_info = self.env.step(action)\n",
        "                next_state, reward_prob, terminated, truncated, _ = next_info\n",
        "                done = truncated or terminated\n",
        "                reward += reward_prob\n",
        "\n",
        "                self.agent.store_experience(state, action, reward_prob, next_state, done)\n",
        "                eps = self.agent.learn()\n",
        "\n",
        "                state = next_state\n",
        "                n_steps += 1 \n",
        "               \n",
        "                \n",
        "                if done: \n",
        "                    break\n",
        "\n",
        "            epsilon_history.append(eps)\n",
        "            episode_rewards.append(reward)\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            avg_rewards.append(avg_reward)\n",
        "\n",
        "            result = f\"Episode: {episode}, Epsilon: {eps}, Steps: {n_steps}, Reward: {reward}, Best reward: {best_reward}, Avg reward: {avg_reward}\"\n",
        "            self.writer.write_to_file(result)\n",
        "            print(result)\n",
        "            \n",
        "            # Saving Best Model\n",
        "            if reward > best_reward: \n",
        "                best_reward = reward\n",
        "                self.agent.save_models()\n",
        "            \n",
        "            # video Recorder\n",
        "            if episode % 100 ==0:\n",
        "                self.recorder.save(episode)\n",
        "                \n",
        "          # Telegram bot\n",
        "            if self.is_tg and episode % self.tg_bot_freq_epi == 0: \n",
        "                info_msg(episode+1, self.noe, reward, best_reward, \"d\")\n",
        "                \n",
        "         # Eatly Stopping\n",
        "            if episode > 100 and np.mean(episode_rewards[-100:]) >= self.target_score: \n",
        "                break\n",
        "                \n",
        "                \n",
        "        return episode_rewards, epsilon_history, avg_rewards, best_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP3r0OBS1MbO"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "\n",
        "env = make_env(\"ALE/Pong-v5\", \"videos/\", 50)\n",
        "action_space = [_ for _ in range(env.action_space.n)]\n",
        "\n",
        "episodic_rewards_filename = 'array_files/episodic_reward.npy'\n",
        "epsilon_history_filename = 'array_files/epsilon_history.npy'\n",
        "cum_avg_reward_filename = 'array_files/cum_avg_rewards.npy'\n",
        "losses_filename = 'array_files/losses.npy'\n",
        "\n",
        "agent_params = {\n",
        "    \"gamma\": 0.99, \n",
        "    \"lr\": 0.001, \n",
        "    \"input_dims\": env.observation_space.shape,\n",
        "    \"mem_size\" : 20000,\n",
        "    \"batch_size\" : 32,\n",
        "    \"replace\" : 1000,\n",
        "    \"algo\" : \"DQN\",\n",
        "    \"env_name\" : \"pong-v5\",\n",
        "    \"n_actions\" : len(action_space),\n",
        "    \"chkpt_dir\": \"tmp/dqn/\",\n",
        "    \"actions\": action_space\n",
        "}\n",
        "\n",
        "trainer_params = {\n",
        "    \"noe\": 50, \n",
        "    \"max_steps\": 10000,\n",
        "    \"max_eps\": 1,\n",
        "    \"min_eps\": 0.02,\n",
        "    \"eps_decay_rate\": 1e-4,\n",
        "    \"eps\": 1,\n",
        "    \"action_space\": action_space,\n",
        "    \"is_tg\": True,\n",
        "    \"tg_bot_freq_epi\": 10,\n",
        "    \n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "    try: \n",
        "        manage_memory()\n",
        "        \n",
        "        trainer = Trainer(agent, env, trainer_params)\n",
        "\n",
        "        episode_rewards, avg_rewards, best_reward = trainer.train_rl_model()\n",
        "    #    plot_learning_curve(episode_rewards, \"plot_file\")\n",
        "\n",
        "       # eval_model(env, \"keras model\", \"videos/\", fps=10)\n",
        "    \n",
        "    except Exception as error:\n",
        "        raise error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BdK3I3qq1MbP",
        "outputId": "76292112-3530-497d-f70e-b03488c24574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Physical devices cannot be modified after being initialized\n",
            "Episode: 0, Epsilon: 0.9996500000000016, Steps: 66, Reward: -93.09148340846082, Best reward: -inf, Avg reward: -93.09148340846082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 1, Epsilon: 0.9986400000000062, Steps: 101, Reward: -105.2511857933674, Best reward: -93.09148340846082, Avg reward: -99.17133460091411\n",
            "Episode: 2, Epsilon: 0.9972900000000123, Steps: 135, Reward: -600.6154898378174, Best reward: -93.09148340846082, Avg reward: -266.3193863465486\n",
            "Episode: 3, Epsilon: 0.9961300000000176, Steps: 116, Reward: -106.71273673242585, Best reward: -93.09148340846082, Avg reward: -226.41772394301788\n",
            "Episode: 4, Epsilon: 0.9955400000000203, Steps: 59, Reward: -112.0739966879449, Best reward: -93.09148340846082, Avg reward: -203.5489784920033\n",
            "Episode: 5, Epsilon: 0.9946200000000245, Steps: 92, Reward: -319.6103366406335, Best reward: -93.09148340846082, Avg reward: -222.89253818344164\n",
            "Episode: 6, Epsilon: 0.9939400000000276, Steps: 68, Reward: -35.268680704297054, Best reward: -93.09148340846082, Avg reward: -196.08912997213528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 7, Epsilon: 0.992970000000032, Steps: 97, Reward: -190.8908382829972, Best reward: -35.268680704297054, Avg reward: -195.439343510993\n",
            "Episode: 8, Epsilon: 0.9922100000000355, Steps: 76, Reward: -61.401516439883274, Best reward: -35.268680704297054, Avg reward: -180.54625161420304\n",
            "Episode: 9, Epsilon: 0.9916100000000382, Steps: 60, Reward: -107.64795544761856, Best reward: -35.268680704297054, Avg reward: -173.2564219975446\n",
            "Episode: 10, Epsilon: 0.9909200000000413, Steps: 69, Reward: -115.12260657869412, Best reward: -35.268680704297054, Avg reward: -167.97152968674\n",
            "Episode: 11, Epsilon: 0.9899500000000457, Steps: 97, Reward: -168.19482827393074, Best reward: -35.268680704297054, Avg reward: -167.99013790233926\n",
            "Episode: 12, Epsilon: 0.9892900000000487, Steps: 66, Reward: -97.25020634889454, Best reward: -35.268680704297054, Avg reward: -162.54860470592044\n",
            "Episode: 13, Epsilon: 0.9884300000000527, Steps: 86, Reward: -342.42333500505777, Best reward: -35.268680704297054, Avg reward: -175.39679972728737\n",
            "Episode: 14, Epsilon: 0.987700000000056, Steps: 73, Reward: -242.21852506368757, Best reward: -35.268680704297054, Avg reward: -179.85158141638072\n",
            "Episode: 15, Epsilon: 0.9867600000000603, Steps: 94, Reward: -164.7928029034424, Best reward: -35.268680704297054, Avg reward: -178.91040775932206\n",
            "Episode: 16, Epsilon: 0.9858900000000642, Steps: 87, Reward: -246.25510749531358, Best reward: -35.268680704297054, Avg reward: -182.87186068496862\n",
            "Episode: 17, Epsilon: 0.985060000000068, Steps: 83, Reward: -400.9926323256757, Best reward: -35.268680704297054, Avg reward: -194.98968133167455\n",
            "Episode: 18, Epsilon: 0.9843500000000712, Steps: 71, Reward: -76.56118187989438, Best reward: -35.268680704297054, Avg reward: -188.7566024131598\n",
            "Episode: 19, Epsilon: 0.9831200000000768, Steps: 123, Reward: -370.97824573486133, Best reward: -35.268680704297054, Avg reward: -197.8676845792449\n",
            "Episode: 20, Epsilon: 0.9821600000000812, Steps: 96, Reward: -108.90072784288628, Best reward: -35.268680704297054, Avg reward: -193.63116282989446\n",
            "Episode: 21, Epsilon: 0.9811900000000856, Steps: 97, Reward: -328.50555721261117, Best reward: -35.268680704297054, Avg reward: -199.76181712001792\n",
            "Episode: 22, Epsilon: 0.9801100000000905, Steps: 108, Reward: -216.15911288417374, Best reward: -35.268680704297054, Avg reward: -200.47474302280733\n",
            "Episode: 23, Epsilon: 0.9793300000000941, Steps: 78, Reward: -100.11071834453502, Best reward: -35.268680704297054, Avg reward: -196.29290866121266\n",
            "Episode: 24, Epsilon: 0.9784500000000981, Steps: 88, Reward: -96.47800385692878, Best reward: -35.268680704297054, Avg reward: -192.3003124690413\n",
            "Episode: 25, Epsilon: 0.9775600000001021, Steps: 89, Reward: -261.7815372235717, Best reward: -35.268680704297054, Avg reward: -194.97266726729248\n",
            "Episode: 26, Epsilon: 0.9768100000001055, Steps: 75, Reward: -158.343280121107, Best reward: -35.268680704297054, Avg reward: -193.61602329891522\n",
            "Episode: 27, Epsilon: 0.9759200000001096, Steps: 89, Reward: -80.67122759525972, Best reward: -35.268680704297054, Avg reward: -189.58228059521323\n",
            "Episode: 28, Epsilon: 0.9752600000001126, Steps: 66, Reward: 14.706329786765068, Best reward: -35.268680704297054, Avg reward: -182.53784575445536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 29, Epsilon: 0.974290000000117, Steps: 97, Reward: -169.29899876394936, Best reward: 14.706329786765068, Avg reward: -182.09655085477183\n",
            "Episode: 30, Epsilon: 0.9735800000001202, Steps: 71, Reward: -46.7828817055544, Best reward: 14.706329786765068, Avg reward: -177.73159378544221\n",
            "Episode: 31, Epsilon: 0.9726200000001246, Steps: 96, Reward: -340.0337113229991, Best reward: 14.706329786765068, Avg reward: -182.8035349584909\n",
            "Episode: 32, Epsilon: 0.971650000000129, Steps: 97, Reward: -90.06444893052725, Best reward: 14.706329786765068, Avg reward: -179.99325962431018\n",
            "Episode: 33, Epsilon: 0.9704800000001343, Steps: 117, Reward: -103.64656352649041, Best reward: 14.706329786765068, Avg reward: -177.7477685626096\n",
            "Episode: 34, Epsilon: 0.9697300000001378, Steps: 75, Reward: -202.30555082093144, Best reward: 14.706329786765068, Avg reward: -178.44941948427595\n",
            "Episode: 35, Epsilon: 0.968570000000143, Steps: 116, Reward: -156.9234258924821, Best reward: 14.706329786765068, Avg reward: -177.85147521783722\n",
            "Episode: 36, Epsilon: 0.9676700000001471, Steps: 90, Reward: -80.08675400815794, Best reward: 14.706329786765068, Avg reward: -175.20918545541343\n",
            "Episode: 37, Epsilon: 0.9662500000001536, Steps: 142, Reward: -339.31764668049505, Best reward: 14.706329786765068, Avg reward: -179.52782917186298\n",
            "Episode: 38, Epsilon: 0.9648700000001599, Steps: 138, Reward: -291.25080620910165, Best reward: 14.706329786765068, Avg reward: -182.39252089076652\n",
            "Episode: 39, Epsilon: 0.9640500000001636, Steps: 82, Reward: -57.969176474736514, Best reward: 14.706329786765068, Avg reward: -179.2819372803658\n",
            "Episode: 40, Epsilon: 0.963090000000168, Steps: 96, Reward: -135.26148347773466, Best reward: 14.706329786765068, Avg reward: -178.2082676754236\n",
            "Episode: 41, Epsilon: 0.9619200000001733, Steps: 117, Reward: -224.85680065981154, Best reward: 14.706329786765068, Avg reward: -179.31894703219473\n",
            "Episode: 42, Epsilon: 0.9607800000001785, Steps: 114, Reward: -395.53960495263425, Best reward: 14.706329786765068, Avg reward: -184.34733442569333\n",
            "Episode: 43, Epsilon: 0.9599200000001824, Steps: 86, Reward: -74.7600961552599, Best reward: 14.706329786765068, Avg reward: -181.85671537409257\n",
            "Episode: 44, Epsilon: 0.9587100000001879, Steps: 121, Reward: -325.7128731973165, Best reward: 14.706329786765068, Avg reward: -185.05351888127532\n",
            "Episode: 45, Epsilon: 0.957580000000193, Steps: 113, Reward: -126.67026714308858, Best reward: 14.706329786765068, Avg reward: -183.78431775653212\n",
            "Episode: 46, Epsilon: 0.9563000000001989, Steps: 128, Reward: -108.58217251291647, Best reward: 14.706329786765068, Avg reward: -182.18427211305095\n",
            "Episode: 47, Epsilon: 0.9555900000002021, Steps: 71, Reward: -83.00712572633078, Best reward: 14.706329786765068, Avg reward: -180.1180815633276\n",
            "Episode: 48, Epsilon: 0.9546900000002062, Steps: 90, Reward: -120.18541682987149, Best reward: 14.706329786765068, Avg reward: -178.89496595652238\n",
            "Episode: 49, Epsilon: 0.9538700000002099, Steps: 82, Reward: -7.996524006940092, Best reward: 14.706329786765068, Avg reward: -175.47699711753074\n",
            "Episode: 50, Epsilon: 0.952750000000215, Steps: 112, Reward: -122.17088938756832, Best reward: 14.706329786765068, Avg reward: -174.431779318904\n",
            "Episode: 51, Epsilon: 0.9520500000002182, Steps: 70, Reward: -111.56756222866014, Best reward: 14.706329786765068, Avg reward: -173.22285206716853\n",
            "Episode: 52, Epsilon: 0.9510900000002226, Steps: 96, Reward: -146.85890968919784, Best reward: 14.706329786765068, Avg reward: -172.7254191921125\n",
            "Episode: 53, Epsilon: 0.9501700000002268, Steps: 92, Reward: -85.58819324449671, Best reward: 14.706329786765068, Avg reward: -171.11176685974922\n",
            "Episode: 54, Epsilon: 0.9495100000002298, Steps: 66, Reward: -75.01835168225864, Best reward: 14.706329786765068, Avg reward: -169.3646138565221\n",
            "Episode: 55, Epsilon: 0.9483200000002352, Steps: 119, Reward: -175.2490535096462, Best reward: 14.706329786765068, Avg reward: -169.46969313604222\n",
            "Episode: 56, Epsilon: 0.9473000000002398, Steps: 102, Reward: -256.53811042115944, Best reward: 14.706329786765068, Avg reward: -170.99720922876358\n",
            "Episode: 57, Epsilon: 0.9464300000002438, Steps: 87, Reward: -144.28609086851932, Best reward: 14.706329786765068, Avg reward: -170.5366727053111\n",
            "Episode: 58, Epsilon: 0.9457900000002467, Steps: 64, Reward: -100.86131252520045, Best reward: 14.706329786765068, Avg reward: -169.35573439717362\n",
            "Episode: 59, Epsilon: 0.9448800000002509, Steps: 91, Reward: -134.01873825746762, Best reward: 14.706329786765068, Avg reward: -168.76678446151183\n",
            "Episode: 60, Epsilon: 0.9438900000002554, Steps: 99, Reward: -70.99856245886511, Best reward: 14.706329786765068, Avg reward: -167.16402672376353\n",
            "Episode: 61, Epsilon: 0.9427300000002606, Steps: 116, Reward: -397.9691607542848, Best reward: 14.706329786765068, Avg reward: -170.8866901758687\n",
            "Episode: 62, Epsilon: 0.9419100000002644, Steps: 82, Reward: -111.80152895114134, Best reward: 14.706329786765068, Avg reward: -169.94883047388893\n",
            "Episode: 63, Epsilon: 0.9410200000002684, Steps: 89, Reward: -101.6645817469658, Best reward: 14.706329786765068, Avg reward: -168.88188908753077\n",
            "Episode: 64, Epsilon: 0.9402100000002721, Steps: 81, Reward: -122.65406945727347, Best reward: 14.706329786765068, Avg reward: -168.1706918624499\n",
            "Episode: 65, Epsilon: 0.9396200000002748, Steps: 59, Reward: -82.62317621526988, Best reward: 14.706329786765068, Avg reward: -166.87451738294715\n",
            "Episode: 66, Epsilon: 0.9387800000002786, Steps: 84, Reward: -77.77176153363644, Best reward: 14.706329786765068, Avg reward: -165.54462550459925\n",
            "Episode: 67, Epsilon: 0.9376200000002839, Steps: 116, Reward: -198.7278077767711, Best reward: 14.706329786765068, Avg reward: -166.03261347919002\n",
            "Episode: 68, Epsilon: 0.9366600000002883, Steps: 96, Reward: -115.69299106856525, Best reward: 14.706329786765068, Avg reward: -165.3030537341085\n",
            "Episode: 69, Epsilon: 0.935840000000292, Steps: 82, Reward: -121.14716372389725, Best reward: 14.706329786765068, Avg reward: -164.6722553053912\n",
            "Episode: 70, Epsilon: 0.9352100000002949, Steps: 63, Reward: -118.49705326788973, Best reward: 14.706329786765068, Avg reward: -164.0219003471165\n",
            "Episode: 71, Epsilon: 0.9339300000003007, Steps: 128, Reward: -187.56529449490188, Best reward: 14.706329786765068, Avg reward: -164.3488919325024\n",
            "Episode: 72, Epsilon: 0.933210000000304, Steps: 72, Reward: -68.95638241842646, Best reward: 14.706329786765068, Avg reward: -163.04214522683014\n",
            "Episode: 73, Epsilon: 0.9321800000003087, Steps: 103, Reward: -238.4860219732062, Best reward: 14.706329786765068, Avg reward: -164.06165707475412\n",
            "Episode: 74, Epsilon: 0.9313900000003122, Steps: 79, Reward: -147.4944749530274, Best reward: 14.706329786765068, Avg reward: -163.8407613131311\n",
            "Episode: 75, Epsilon: 0.9299600000003188, Steps: 143, Reward: -145.35860443701515, Best reward: 14.706329786765068, Avg reward: -163.59757503844537\n",
            "Episode: 76, Epsilon: 0.9291500000003224, Steps: 81, Reward: -22.742593932350587, Best reward: 14.706329786765068, Avg reward: -161.76828956953506\n",
            "Episode: 77, Epsilon: 0.9282900000003264, Steps: 86, Reward: -133.3298409248421, Best reward: 14.706329786765068, Avg reward: -161.40369407409025\n",
            "Episode: 78, Epsilon: 0.9273900000003305, Steps: 90, Reward: -196.92979170823477, Best reward: 14.706329786765068, Avg reward: -161.85339151249715\n",
            "Episode: 79, Epsilon: 0.9267700000003333, Steps: 62, Reward: -87.80565342484965, Best reward: 14.706329786765068, Avg reward: -160.92779478640156\n",
            "Episode: 80, Epsilon: 0.92529000000034, Steps: 148, Reward: -394.8167099484293, Best reward: 14.706329786765068, Avg reward: -163.8153122575377\n",
            "Episode: 81, Epsilon: 0.9241600000003451, Steps: 113, Reward: -153.44050515613793, Best reward: 14.706329786765068, Avg reward: -163.68879021971574\n",
            "Episode: 82, Epsilon: 0.9231400000003498, Steps: 102, Reward: -185.17011745297026, Best reward: 14.706329786765068, Avg reward: -163.94760139120075\n",
            "Episode: 83, Epsilon: 0.9221400000003543, Steps: 100, Reward: -136.6648468302561, Best reward: 14.706329786765068, Avg reward: -163.62280669404663\n",
            "Episode: 84, Epsilon: 0.9210900000003591, Steps: 105, Reward: -156.60173396570167, Best reward: 14.706329786765068, Avg reward: -163.54020583841904\n",
            "Episode: 85, Epsilon: 0.9198800000003646, Steps: 121, Reward: -172.49471787234086, Best reward: 14.706329786765068, Avg reward: -163.64432807137163\n",
            "Episode: 86, Epsilon: 0.9189000000003691, Steps: 98, Reward: -113.66074911460333, Best reward: 14.706329786765068, Avg reward: -163.06980417531682\n",
            "Episode: 87, Epsilon: 0.9183200000003717, Steps: 58, Reward: -115.8548679547564, Best reward: 14.706329786765068, Avg reward: -162.53327080917413\n",
            "Episode: 88, Epsilon: 0.9173500000003761, Steps: 97, Reward: -104.93782666845624, Best reward: 14.706329786765068, Avg reward: -161.8861309873683\n",
            "Episode: 89, Epsilon: 0.9166600000003793, Steps: 69, Reward: -102.49795499647308, Best reward: 14.706329786765068, Avg reward: -161.22626236524724\n",
            "Episode: 90, Epsilon: 0.915630000000384, Steps: 103, Reward: -120.68127759370915, Best reward: 14.706329786765068, Avg reward: -160.78071308204352\n",
            "Episode: 91, Epsilon: 0.9149100000003872, Steps: 72, Reward: -111.4354119913417, Best reward: 14.706329786765068, Avg reward: -160.24435111366634\n",
            "Episode: 92, Epsilon: 0.9140000000003914, Steps: 91, Reward: -125.60861154668933, Best reward: 14.706329786765068, Avg reward: -159.87192380649455\n",
            "Episode: 93, Epsilon: 0.913200000000395, Steps: 80, Reward: -90.62648953324978, Best reward: 14.706329786765068, Avg reward: -159.1352702503962\n",
            "Episode: 94, Epsilon: 0.9122000000003996, Steps: 100, Reward: -118.04016320368981, Best reward: 14.706329786765068, Avg reward: -158.70269017622033\n",
            "Episode: 95, Epsilon: 0.9111300000004044, Steps: 107, Reward: -142.10053064511538, Best reward: 14.706329786765068, Avg reward: -158.52975101443798\n",
            "Episode: 96, Epsilon: 0.9102400000004085, Steps: 89, Reward: -159.40611268857208, Best reward: 14.706329786765068, Avg reward: -158.53878567087236\n",
            "Episode: 97, Epsilon: 0.9095400000004117, Steps: 70, Reward: -63.00627307131706, Best reward: 14.706329786765068, Avg reward: -157.56396411373402\n",
            "Episode: 98, Epsilon: 0.9081800000004179, Steps: 136, Reward: -87.240144898857, Best reward: 14.706329786765068, Avg reward: -156.85362250550293\n",
            "Episode: 99, Epsilon: 0.9072000000004223, Steps: 98, Reward: -93.72334962044442, Best reward: 14.706329786765068, Avg reward: -156.22231977665234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100, Epsilon: 0.9063500000004262, Steps: 85, Reward: -68.56636730349933, Best reward: 14.706329786765068, Avg reward: -155.97706861560275\n",
            "Episode: 101, Epsilon: 0.9057500000004289, Steps: 60, Reward: -155.70263236234769, Best reward: 14.706329786765068, Avg reward: -156.48158308129257\n",
            "Episode: 102, Epsilon: 0.9051500000004317, Steps: 60, Reward: -79.46306616091087, Best reward: 14.706329786765068, Avg reward: -151.27005884452348\n",
            "Episode: 103, Epsilon: 0.9045000000004346, Steps: 65, Reward: -94.89857544950803, Best reward: 14.706329786765068, Avg reward: -151.1519172316943\n",
            "Episode: 104, Epsilon: 0.903550000000439, Steps: 95, Reward: -134.62456614108424, Best reward: 14.706329786765068, Avg reward: -151.3774229262257\n",
            "Episode: 105, Epsilon: 0.9027800000004425, Steps: 77, Reward: -52.363646154471766, Best reward: 14.706329786765068, Avg reward: -148.7049560213641\n",
            "Episode: 106, Epsilon: 0.9018100000004469, Steps: 97, Reward: -101.00433201748959, Best reward: 14.706329786765068, Avg reward: -149.36231253449603\n",
            "Episode: 107, Epsilon: 0.9010800000004502, Steps: 73, Reward: -66.81819992217302, Best reward: 14.706329786765068, Avg reward: -148.12158615088777\n",
            "Episode: 108, Epsilon: 0.9004000000004533, Steps: 68, Reward: -95.02616543523392, Best reward: 14.706329786765068, Avg reward: -148.45783264084127\n",
            "Episode: 109, Epsilon: 0.8992100000004587, Steps: 119, Reward: -65.29362015296702, Best reward: 14.706329786765068, Avg reward: -148.03428928789475\n",
            "Episode: 110, Epsilon: 0.8984700000004621, Steps: 74, Reward: -299.38533563921897, Best reward: 14.706329786765068, Avg reward: -149.87691657850002\n",
            "Episode: 111, Epsilon: 0.8973700000004671, Steps: 110, Reward: -137.26644256167174, Best reward: 14.706329786765068, Avg reward: -149.5676327213774\n",
            "Episode: 112, Epsilon: 0.8964800000004711, Steps: 89, Reward: -100.53112374525895, Best reward: 14.706329786765068, Avg reward: -149.60044189534105\n",
            "Episode: 113, Epsilon: 0.8957500000004744, Steps: 73, Reward: -47.28137018956589, Best reward: 14.706329786765068, Avg reward: -146.64902224718614\n",
            "Episode: 114, Epsilon: 0.8947300000004791, Steps: 102, Reward: -141.33300937314473, Best reward: 14.706329786765068, Avg reward: -145.64016709028073\n",
            "Episode: 115, Epsilon: 0.8935800000004843, Steps: 115, Reward: -169.4277657804977, Best reward: 14.706329786765068, Avg reward: -145.6865167190513\n",
            "Episode: 116, Epsilon: 0.8928800000004875, Steps: 70, Reward: -79.40327333756737, Best reward: 14.706329786765068, Avg reward: -144.01799837747382\n",
            "Episode: 117, Epsilon: 0.8921900000004906, Steps: 69, Reward: -212.696596926059, Best reward: 14.706329786765068, Avg reward: -142.13503802347765\n",
            "Episode: 118, Epsilon: 0.8910500000004958, Steps: 114, Reward: -91.11706406892256, Best reward: 14.706329786765068, Avg reward: -142.2805968453679\n",
            "Episode: 119, Epsilon: 0.8900900000005002, Steps: 96, Reward: -80.05473360925377, Best reward: 14.706329786765068, Avg reward: -139.37136172411184\n",
            "Episode: 120, Epsilon: 0.8890900000005048, Steps: 100, Reward: -99.69886314423594, Best reward: 14.706329786765068, Avg reward: -139.27934307712533\n",
            "Episode: 121, Epsilon: 0.8882900000005084, Steps: 80, Reward: -102.18478819573738, Best reward: 14.706329786765068, Avg reward: -137.0161353869566\n",
            "Episode: 122, Epsilon: 0.8872000000005134, Steps: 109, Reward: -104.3292767851506, Best reward: 14.706329786765068, Avg reward: -135.89783702596634\n",
            "Episode: 123, Epsilon: 0.8859800000005189, Steps: 122, Reward: -116.06736262793866, Best reward: 14.706329786765068, Avg reward: -136.05740346880043\n",
            "Episode: 124, Epsilon: 0.8849400000005236, Steps: 104, Reward: -86.77313701978817, Best reward: 14.706329786765068, Avg reward: -135.960354800429\n",
            "Episode: 125, Epsilon: 0.8838000000005288, Steps: 114, Reward: -182.46782169113607, Best reward: 14.706329786765068, Avg reward: -135.16721764510464\n",
            "Episode: 126, Epsilon: 0.8830300000005323, Steps: 77, Reward: -103.95496173959019, Best reward: 14.706329786765068, Avg reward: -134.62333446128946\n",
            "Episode: 127, Epsilon: 0.8818300000005378, Steps: 120, Reward: -269.39795140576564, Best reward: 14.706329786765068, Avg reward: -136.51060169939453\n",
            "Episode: 128, Epsilon: 0.8811100000005411, Steps: 72, Reward: -46.51321846685309, Best reward: 14.706329786765068, Avg reward: -137.1227971819307\n",
            "Episode: 129, Epsilon: 0.8803500000005445, Steps: 76, Reward: -117.74095649255703, Best reward: 14.706329786765068, Avg reward: -136.60721675921678\n",
            "Episode: 130, Epsilon: 0.8797000000005475, Steps: 65, Reward: -71.99244286973449, Best reward: 14.706329786765068, Avg reward: -136.8593123708586\n",
            "Episode: 131, Epsilon: 0.8788300000005514, Steps: 87, Reward: -104.77384077888138, Best reward: 14.706329786765068, Avg reward: -134.5067136654174\n",
            "Episode: 132, Epsilon: 0.8779700000005554, Steps: 86, Reward: -82.84288672359673, Best reward: 14.706329786765068, Avg reward: -134.4344980433481\n",
            "Episode: 133, Epsilon: 0.877180000000559, Steps: 79, Reward: -55.58893471571776, Best reward: 14.706329786765068, Avg reward: -133.95392175524037\n",
            "Episode: 134, Epsilon: 0.8757600000005654, Steps: 142, Reward: -66.60976558730466, Best reward: 14.706329786765068, Avg reward: -132.59696390290412\n",
            "Episode: 135, Epsilon: 0.8749300000005692, Steps: 83, Reward: -94.92811306805348, Best reward: 14.706329786765068, Avg reward: -131.97701077465982\n",
            "Episode: 136, Epsilon: 0.8734500000005759, Steps: 148, Reward: -139.82975984454166, Best reward: 14.706329786765068, Avg reward: -132.5744408330237\n",
            "Episode: 137, Epsilon: 0.8727600000005791, Steps: 69, Reward: -95.90546991085726, Best reward: 14.706329786765068, Avg reward: -130.1403190653273\n",
            "Episode: 138, Epsilon: 0.8715600000005845, Steps: 120, Reward: -326.7379172549978, Best reward: 14.706329786765068, Avg reward: -130.49519017578623\n",
            "Episode: 139, Epsilon: 0.8705000000005894, Steps: 106, Reward: -139.34517012712948, Best reward: 14.706329786765068, Avg reward: -131.30895011231019\n",
            "Episode: 140, Epsilon: 0.8692200000005952, Steps: 128, Reward: -44.85852194969378, Best reward: 14.706329786765068, Avg reward: -130.40492049702976\n",
            "Episode: 141, Epsilon: 0.8683100000005993, Steps: 91, Reward: -138.01622855403195, Best reward: 14.706329786765068, Avg reward: -129.53651477597197\n",
            "Episode: 142, Epsilon: 0.8674100000006034, Steps: 90, Reward: -122.0205322944815, Best reward: 14.706329786765068, Avg reward: -126.80132404939046\n",
            "Episode: 143, Epsilon: 0.8665200000006075, Steps: 89, Reward: -83.43744485455515, Best reward: 14.706329786765068, Avg reward: -126.8880975363834\n",
            "Episode: 144, Epsilon: 0.8658800000006104, Steps: 64, Reward: -127.92754015828362, Best reward: 14.706329786765068, Avg reward: -124.91024420599308\n",
            "Episode: 145, Epsilon: 0.8646000000006162, Steps: 128, Reward: -377.8545559211042, Best reward: 14.706329786765068, Avg reward: -127.42208709377323\n",
            "Episode: 146, Epsilon: 0.8635700000006209, Steps: 103, Reward: -61.59092336746845, Best reward: 14.706329786765068, Avg reward: -126.95217460231873\n",
            "Episode: 147, Epsilon: 0.8617300000006293, Steps: 184, Reward: -127.73730580581015, Best reward: 14.706329786765068, Avg reward: -127.39947640311355\n",
            "Episode: 148, Epsilon: 0.860690000000634, Steps: 104, Reward: -93.37000822089082, Best reward: 14.706329786765068, Avg reward: -127.13132231702373\n",
            "Episode: 149, Epsilon: 0.859810000000638, Steps: 88, Reward: -94.83621025687056, Best reward: 14.706329786765068, Avg reward: -127.99971917952305\n",
            "Episode: 150, Epsilon: 0.8589800000006418, Steps: 83, Reward: -138.78208681893292, Best reward: 14.706329786765068, Avg reward: -128.1658311538367\n",
            "Episode: 151, Epsilon: 0.8579300000006466, Steps: 105, Reward: -98.16782778523678, Best reward: 14.706329786765068, Avg reward: -128.03183380940246\n",
            "Episode: 152, Epsilon: 0.8569100000006512, Steps: 102, Reward: -121.90786476648898, Best reward: 14.706329786765068, Avg reward: -127.78232336017535\n",
            "Episode: 153, Epsilon: 0.8557700000006564, Steps: 114, Reward: -91.76816742663294, Best reward: 14.706329786765068, Avg reward: -127.84412310199674\n",
            "Episode: 154, Epsilon: 0.8550100000006599, Steps: 76, Reward: -96.22357068831683, Best reward: 14.706329786765068, Avg reward: -128.0561752920573\n",
            "Episode: 155, Epsilon: 0.854090000000664, Steps: 92, Reward: -112.57552943118768, Best reward: 14.706329786765068, Avg reward: -127.42944005127272\n",
            "Episode: 156, Epsilon: 0.8532500000006679, Steps: 84, Reward: -137.43404384862302, Best reward: 14.706329786765068, Avg reward: -126.23839938554737\n",
            "Episode: 157, Epsilon: 0.8521000000006731, Steps: 115, Reward: -63.92271987921282, Best reward: 14.706329786765068, Avg reward: -125.43476567565429\n",
            "Episode: 158, Epsilon: 0.8514200000006762, Steps: 68, Reward: -111.40077340957284, Best reward: 14.706329786765068, Avg reward: -125.54016028449801\n",
            "Episode: 159, Epsilon: 0.850370000000681, Steps: 105, Reward: -90.7254813368171, Best reward: 14.706329786765068, Avg reward: -125.10722771529149\n",
            "Episode: 160, Epsilon: 0.8494100000006853, Steps: 96, Reward: -100.01612165830917, Best reward: 14.706329786765068, Avg reward: -125.39740330728593\n",
            "Episode: 161, Epsilon: 0.8483000000006904, Steps: 111, Reward: -42.51222982478019, Best reward: 14.706329786765068, Avg reward: -121.8428339979909\n",
            "Episode: 162, Epsilon: 0.8476900000006932, Steps: 61, Reward: -77.07693968381764, Best reward: 14.706329786765068, Avg reward: -121.49558810531765\n",
            "Episode: 163, Epsilon: 0.8467900000006973, Steps: 90, Reward: -132.98552027106658, Best reward: 14.706329786765068, Avg reward: -121.80879749055866\n",
            "Episode: 164, Epsilon: 0.8460500000007006, Steps: 74, Reward: -115.60323868487748, Best reward: 14.706329786765068, Avg reward: -121.7382891828347\n",
            "Episode: 165, Epsilon: 0.844880000000706, Steps: 117, Reward: -71.83935908349122, Best reward: 14.706329786765068, Avg reward: -121.63045101151691\n",
            "Episode: 166, Epsilon: 0.8441400000007093, Steps: 74, Reward: -77.67557013389327, Best reward: 14.706329786765068, Avg reward: -121.6294890975195\n",
            "Episode: 167, Epsilon: 0.8430400000007143, Steps: 110, Reward: -198.38932554815887, Best reward: 14.706329786765068, Avg reward: -121.62610427523337\n",
            "Episode: 168, Epsilon: 0.84179000000072, Steps: 125, Reward: -119.44432585580675, Best reward: 14.706329786765068, Avg reward: -121.66361762310578\n",
            "Episode: 169, Epsilon: 0.8408300000007244, Steps: 96, Reward: -108.97155844198375, Best reward: 14.706329786765068, Avg reward: -121.54186157028666\n",
            "Episode: 170, Epsilon: 0.8399700000007283, Steps: 86, Reward: -83.74866825944197, Best reward: 14.706329786765068, Avg reward: -121.1943777202022\n",
            "Episode: 171, Epsilon: 0.8387800000007337, Steps: 119, Reward: -65.18390006147214, Best reward: 14.706329786765068, Avg reward: -119.9705637758679\n",
            "Episode: 172, Epsilon: 0.8376600000007388, Steps: 112, Reward: -61.971115994272644, Best reward: 14.706329786765068, Avg reward: -119.90071111162635\n",
            "Episode: 173, Epsilon: 0.8367200000007431, Steps: 94, Reward: -312.166140972505, Best reward: 14.706329786765068, Avg reward: -120.63751230161935\n",
            "Episode: 174, Epsilon: 0.8357000000007477, Steps: 102, Reward: -215.2457469427526, Best reward: 14.706329786765068, Avg reward: -121.31502502151658\n",
            "Episode: 175, Epsilon: 0.8350200000007508, Steps: 68, Reward: -72.59186366376048, Best reward: 14.706329786765068, Avg reward: -120.58735761378402\n",
            "Episode: 176, Epsilon: 0.8338000000007564, Steps: 122, Reward: -50.32725397140334, Best reward: 14.706329786765068, Avg reward: -120.86320421417454\n",
            "Episode: 177, Epsilon: 0.8329600000007602, Steps: 84, Reward: -95.96048355255273, Best reward: 14.706329786765068, Avg reward: -120.48951064045167\n",
            "Episode: 178, Epsilon: 0.8322900000007633, Steps: 67, Reward: -111.72093928473228, Best reward: 14.706329786765068, Avg reward: -119.63742211621665\n",
            "Episode: 179, Epsilon: 0.8313400000007676, Steps: 95, Reward: -101.2883037987697, Best reward: 14.706329786765068, Avg reward: -119.77224861995586\n",
            "Episode: 180, Epsilon: 0.830370000000772, Steps: 97, Reward: -92.5118395450793, Best reward: 14.706329786765068, Avg reward: -116.74919991592235\n",
            "Episode: 181, Epsilon: 0.829710000000775, Steps: 66, Reward: -56.85372932493553, Best reward: 14.706329786765068, Avg reward: -115.78333215761033\n",
            "Episode: 182, Epsilon: 0.828400000000781, Steps: 131, Reward: -111.34774929470373, Best reward: 14.706329786765068, Avg reward: -115.04510847602768\n",
            "Episode: 183, Epsilon: 0.8275400000007849, Steps: 86, Reward: -80.4802974778585, Best reward: 14.706329786765068, Avg reward: -114.48326298250366\n",
            "Episode: 184, Epsilon: 0.8262400000007908, Steps: 130, Reward: -62.73915702295816, Best reward: 14.706329786765068, Avg reward: -113.54463721307624\n",
            "Episode: 185, Epsilon: 0.825090000000796, Steps: 115, Reward: -44.091570074316586, Best reward: 14.706329786765068, Avg reward: -112.260605735096\n",
            "Episode: 186, Epsilon: 0.8240200000008009, Steps: 107, Reward: -82.26297028594769, Best reward: 14.706329786765068, Avg reward: -111.94662794680944\n",
            "Episode: 187, Epsilon: 0.8232700000008043, Steps: 75, Reward: -48.80038867837109, Best reward: 14.706329786765068, Avg reward: -111.2760831540456\n",
            "Episode: 188, Epsilon: 0.8223400000008085, Steps: 93, Reward: -90.92118520759271, Best reward: 14.706329786765068, Avg reward: -111.13591673943698\n",
            "Episode: 189, Epsilon: 0.821150000000814, Steps: 119, Reward: -58.28743275862401, Best reward: 14.706329786765068, Avg reward: -110.69381151705848\n",
            "Episode: 190, Epsilon: 0.8199900000008192, Steps: 116, Reward: -46.49107693254268, Best reward: 14.706329786765068, Avg reward: -109.95190951044682\n",
            "Episode: 191, Epsilon: 0.8191900000008229, Steps: 80, Reward: -81.34838742400538, Best reward: 14.706329786765068, Avg reward: -109.65103926477343\n",
            "Episode: 192, Epsilon: 0.8185500000008258, Steps: 64, Reward: -72.4097897453324, Best reward: 14.706329786765068, Avg reward: -109.11905104675985\n",
            "Episode: 193, Epsilon: 0.8178000000008292, Steps: 75, Reward: 36.68347804548469, Best reward: 14.706329786765068, Avg reward: -107.8459513709725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 194, Epsilon: 0.8166200000008346, Steps: 118, Reward: -116.78569774478473, Best reward: 36.68347804548469, Avg reward: -107.83340671638345\n",
            "Episode: 195, Epsilon: 0.8158800000008379, Steps: 74, Reward: -78.76813956800225, Best reward: 36.68347804548469, Avg reward: -107.20008280561234\n",
            "Episode: 196, Epsilon: 0.8148300000008427, Steps: 105, Reward: -96.84618966799795, Best reward: 36.68347804548469, Avg reward: -106.57448357540659\n",
            "Episode: 197, Epsilon: 0.814110000000846, Steps: 72, Reward: -69.4195827367609, Best reward: 36.68347804548469, Avg reward: -106.63861667206103\n",
            "Episode: 198, Epsilon: 0.8134100000008492, Steps: 70, Reward: -69.39867779969822, Best reward: 36.68347804548469, Avg reward: -106.46020200106945\n",
            "Episode: 199, Epsilon: 0.8127200000008523, Steps: 69, Reward: -125.32441936736672, Best reward: 36.68347804548469, Avg reward: -106.77621269853867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 200, Epsilon: 0.8116000000008574, Steps: 112, Reward: -74.14395191167397, Best reward: 36.68347804548469, Avg reward: -106.8319885446204\n",
            "Episode: 201, Epsilon: 0.8106300000008618, Steps: 97, Reward: -140.96503859777732, Best reward: 36.68347804548469, Avg reward: -106.68461260697471\n",
            "Episode: 202, Epsilon: 0.8094200000008673, Steps: 121, Reward: -87.55424644946024, Best reward: 36.68347804548469, Avg reward: -106.7655244098602\n",
            "Episode: 203, Epsilon: 0.8085400000008713, Steps: 88, Reward: -95.27418316301636, Best reward: 36.68347804548469, Avg reward: -106.76928048699529\n",
            "Episode: 204, Epsilon: 0.8076400000008754, Steps: 90, Reward: -68.96274511321687, Best reward: 36.68347804548469, Avg reward: -106.1126622767166\n",
            "Episode: 205, Epsilon: 0.8070500000008781, Steps: 59, Reward: -84.57425708708543, Best reward: 36.68347804548469, Avg reward: -106.43476838604276\n",
            "Episode: 206, Epsilon: 0.8061200000008824, Steps: 93, Reward: -116.38065041505321, Best reward: 36.68347804548469, Avg reward: -106.58853157001836\n",
            "Episode: 207, Epsilon: 0.8048500000008881, Steps: 127, Reward: -10.424876400637643, Best reward: 36.68347804548469, Avg reward: -106.02459833480303\n",
            "Episode: 208, Epsilon: 0.8042500000008909, Steps: 60, Reward: -72.88783137792015, Best reward: 36.68347804548469, Avg reward: -105.80321499422989\n",
            "Episode: 209, Epsilon: 0.803340000000895, Steps: 91, Reward: -25.751492480587274, Best reward: 36.68347804548469, Avg reward: -105.4077937175061\n",
            "Episode: 210, Epsilon: 0.8020600000009008, Steps: 128, Reward: -137.66739406506557, Best reward: 36.68347804548469, Avg reward: -103.79061430176456\n",
            "Episode: 211, Epsilon: 0.8008400000009064, Steps: 122, Reward: -89.39229051534521, Best reward: 36.68347804548469, Avg reward: -103.3118727813013\n",
            "Episode: 212, Epsilon: 0.7996800000009117, Steps: 116, Reward: -76.27547687442052, Best reward: 36.68347804548469, Avg reward: -103.06931631259289\n",
            "Episode: 213, Epsilon: 0.7986300000009164, Steps: 105, Reward: -116.476582121862, Best reward: 36.68347804548469, Avg reward: -103.76126843191587\n",
            "Episode: 214, Epsilon: 0.7975700000009213, Steps: 106, Reward: -99.88349922404258, Best reward: 36.68347804548469, Avg reward: -103.34677333042485\n",
            "Episode: 215, Epsilon: 0.7967100000009252, Steps: 86, Reward: -87.5568781485606, Best reward: 36.68347804548469, Avg reward: -102.52806445410548\n",
            "Episode: 216, Epsilon: 0.7954800000009308, Steps: 123, Reward: -73.07982768167484, Best reward: 36.68347804548469, Avg reward: -102.46482999754654\n",
            "Episode: 217, Epsilon: 0.794560000000935, Steps: 92, Reward: -83.86803911576942, Best reward: 36.68347804548469, Avg reward: -101.17654441944366\n",
            "Episode: 218, Epsilon: 0.7935500000009396, Steps: 101, Reward: -144.63213907545537, Best reward: 36.68347804548469, Avg reward: -101.71169516950899\n",
            "Episode: 219, Epsilon: 0.7926800000009435, Steps: 87, Reward: -138.66559663745966, Best reward: 36.68347804548469, Avg reward: -102.29780379979105\n",
            "Episode: 220, Epsilon: 0.7918700000009472, Steps: 81, Reward: -60.10180089438167, Best reward: 36.68347804548469, Avg reward: -101.9018331772925\n",
            "Episode: 221, Epsilon: 0.7910600000009509, Steps: 81, Reward: -71.71469364236548, Best reward: 36.68347804548469, Avg reward: -101.59713223175879\n",
            "Episode: 222, Epsilon: 0.7898800000009563, Steps: 118, Reward: -90.21218246204003, Best reward: 36.68347804548469, Avg reward: -101.45596128852769\n",
            "Episode: 223, Epsilon: 0.7891500000009596, Steps: 73, Reward: -107.79499034605912, Best reward: 36.68347804548469, Avg reward: -101.37323756570889\n",
            "Episode: 224, Epsilon: 0.7880600000009645, Steps: 109, Reward: -101.63761599251161, Best reward: 36.68347804548469, Avg reward: -101.52188235543612\n",
            "Episode: 225, Epsilon: 0.78686000000097, Steps: 120, Reward: -87.5272973884584, Best reward: 36.68347804548469, Avg reward: -100.57247711240935\n",
            "Episode: 226, Epsilon: 0.7854900000009762, Steps: 137, Reward: -161.54422242278872, Best reward: 36.68347804548469, Avg reward: -101.14836971924133\n",
            "Episode: 227, Epsilon: 0.7845300000009806, Steps: 96, Reward: -104.03596378548589, Best reward: 36.68347804548469, Avg reward: -99.49474984303852\n",
            "Episode: 228, Epsilon: 0.7835500000009851, Steps: 98, Reward: -115.16957285246032, Best reward: 36.68347804548469, Avg reward: -100.1813133868946\n",
            "Episode: 229, Epsilon: 0.7824400000009901, Steps: 111, Reward: -76.87713307358928, Best reward: 36.68347804548469, Avg reward: -99.77267515270492\n",
            "Episode: 230, Epsilon: 0.7812900000009954, Steps: 115, Reward: -104.32934547233121, Best reward: 36.68347804548469, Avg reward: -100.09604417873089\n",
            "Episode: 231, Epsilon: 0.7800900000010008, Steps: 120, Reward: -81.15700000790187, Best reward: 36.68347804548469, Avg reward: -99.85987577102112\n",
            "Episode: 232, Epsilon: 0.779170000001005, Steps: 92, Reward: -77.06200021246586, Best reward: 36.68347804548469, Avg reward: -99.8020669059098\n",
            "Episode: 233, Epsilon: 0.7776500000010119, Steps: 152, Reward: -128.96575600601295, Best reward: 36.68347804548469, Avg reward: -100.53583511881276\n",
            "Episode: 234, Epsilon: 0.7765200000010171, Steps: 113, Reward: -177.0720858599836, Best reward: 36.68347804548469, Avg reward: -101.64045832153954\n",
            "Episode: 235, Epsilon: 0.7756300000010211, Steps: 89, Reward: -96.31265861271352, Best reward: 36.68347804548469, Avg reward: -101.65430377698613\n",
            "Episode: 236, Epsilon: 0.774550000001026, Steps: 108, Reward: -84.5447416548246, Best reward: 36.68347804548469, Avg reward: -101.10145359508897\n",
            "Episode: 237, Epsilon: 0.7737700000010296, Steps: 78, Reward: -75.47170283937021, Best reward: 36.68347804548469, Avg reward: -100.89711592437408\n",
            "Episode: 238, Epsilon: 0.7730000000010331, Steps: 77, Reward: -63.42856886027915, Best reward: 36.68347804548469, Avg reward: -98.26402244042691\n",
            "Episode: 239, Epsilon: 0.771710000001039, Steps: 129, Reward: -24.008253330863596, Best reward: 36.68347804548469, Avg reward: -97.11065327246425\n",
            "Episode: 240, Epsilon: 0.7706200000010439, Steps: 109, Reward: -103.70870824029174, Best reward: 36.68347804548469, Avg reward: -97.69915513537023\n",
            "Episode: 241, Epsilon: 0.7697100000010481, Steps: 91, Reward: -117.39665578823568, Best reward: 36.68347804548469, Avg reward: -97.49295940771228\n",
            "Episode: 242, Epsilon: 0.7687200000010526, Steps: 99, Reward: -31.346118458000532, Best reward: 36.68347804548469, Avg reward: -96.58621526934749\n",
            "Episode: 243, Epsilon: 0.7674900000010582, Steps: 123, Reward: -64.5987572760354, Best reward: 36.68347804548469, Avg reward: -96.39782839356226\n",
            "Episode: 244, Epsilon: 0.7666800000010618, Steps: 81, Reward: -41.93617506040819, Best reward: 36.68347804548469, Avg reward: -95.53791474258351\n",
            "Episode: 245, Epsilon: 0.7652200000010685, Steps: 146, Reward: -103.12359381586356, Best reward: 36.68347804548469, Avg reward: -92.7906051215311\n",
            "Episode: 246, Epsilon: 0.7643100000010726, Steps: 91, Reward: -114.92497354733355, Best reward: 36.68347804548469, Avg reward: -93.32394562332973\n",
            "Episode: 247, Epsilon: 0.7631700000010778, Steps: 114, Reward: -84.62511257651653, Best reward: 36.68347804548469, Avg reward: -92.89282369103681\n",
            "Episode: 248, Epsilon: 0.7621300000010826, Steps: 104, Reward: -52.278261815407774, Best reward: 36.68347804548469, Avg reward: -92.48190622698198\n",
            "Episode: 249, Epsilon: 0.7608600000010883, Steps: 127, Reward: -101.79960272423868, Best reward: 36.68347804548469, Avg reward: -92.55154015165564\n",
            "Episode: 250, Epsilon: 0.7598700000010928, Steps: 99, Reward: -26.524992577676187, Best reward: 36.68347804548469, Avg reward: -91.42896920924308\n",
            "Episode: 251, Epsilon: 0.7589800000010969, Steps: 89, Reward: -100.7783066751459, Best reward: 36.68347804548469, Avg reward: -91.4550739981422\n",
            "Episode: 252, Epsilon: 0.7577100000011027, Steps: 127, Reward: -88.7425839595812, Best reward: 36.68347804548469, Avg reward: -91.12342119007313\n",
            "Episode: 253, Epsilon: 0.7569100000011063, Steps: 80, Reward: -55.59932807504808, Best reward: 36.68347804548469, Avg reward: -90.76173279655727\n",
            "Episode: 254, Epsilon: 0.7560700000011101, Steps: 84, Reward: -42.575520198895354, Best reward: 36.68347804548469, Avg reward: -90.22525229166305\n",
            "Episode: 255, Epsilon: 0.7552000000011141, Steps: 87, Reward: -58.92363068909353, Best reward: 36.68347804548469, Avg reward: -89.68873330424212\n",
            "Episode: 256, Epsilon: 0.7540500000011193, Steps: 115, Reward: -111.96895616832973, Best reward: 36.68347804548469, Avg reward: -89.4340824274392\n",
            "Episode: 257, Epsilon: 0.753020000001124, Steps: 103, Reward: -49.07188870590815, Best reward: 36.68347804548469, Avg reward: -89.28557411570613\n",
            "Episode: 258, Epsilon: 0.7518400000011294, Steps: 118, Reward: -126.48140040865763, Best reward: 36.68347804548469, Avg reward: -89.43638038569699\n",
            "Episode: 259, Epsilon: 0.7506700000011347, Steps: 117, Reward: -45.32808409276076, Best reward: 36.68347804548469, Avg reward: -88.98240641325641\n",
            "Episode: 260, Epsilon: 0.7493800000011406, Steps: 129, Reward: -42.18127581149123, Best reward: 36.68347804548469, Avg reward: -88.40405795478824\n",
            "Episode: 261, Epsilon: 0.7485900000011442, Steps: 79, Reward: -81.90901896332946, Best reward: 36.68347804548469, Avg reward: -88.79802584617373\n",
            "Episode: 262, Epsilon: 0.747540000001149, Steps: 105, Reward: -77.31933123568089, Best reward: 36.68347804548469, Avg reward: -88.80044976169238\n",
            "Episode: 263, Epsilon: 0.7464100000011541, Steps: 113, Reward: -122.99857071521475, Best reward: 36.68347804548469, Avg reward: -88.70058026613387\n",
            "Episode: 264, Epsilon: 0.7454800000011583, Steps: 93, Reward: -92.76599964156031, Best reward: 36.68347804548469, Avg reward: -88.47220787570068\n",
            "Episode: 265, Epsilon: 0.7442200000011641, Steps: 126, Reward: 6.986441840265599, Best reward: 36.68347804548469, Avg reward: -87.68394986646312\n",
            "Episode: 266, Epsilon: 0.7435000000011673, Steps: 72, Reward: -93.6286964775002, Best reward: 36.68347804548469, Avg reward: -87.8434811298992\n",
            "Episode: 267, Epsilon: 0.742690000001171, Steps: 81, Reward: -100.66802558439262, Best reward: 36.68347804548469, Avg reward: -86.86626813026152\n",
            "Episode: 268, Epsilon: 0.7418800000011747, Steps: 81, Reward: -75.4857452285353, Best reward: 36.68347804548469, Avg reward: -86.42668232398883\n",
            "Episode: 269, Epsilon: 0.7404200000011814, Steps: 146, Reward: -353.045382805533, Best reward: 36.68347804548469, Avg reward: -88.86742056762431\n",
            "Episode: 270, Epsilon: 0.739620000001185, Steps: 80, Reward: -135.19420303721233, Best reward: 36.68347804548469, Avg reward: -89.38187591540199\n",
            "Episode: 271, Epsilon: 0.7388200000011886, Steps: 80, Reward: -109.6936861084382, Best reward: 36.68347804548469, Avg reward: -89.82697377587165\n",
            "Episode: 272, Epsilon: 0.7381300000011918, Steps: 69, Reward: -77.42880627737782, Best reward: 36.68347804548469, Avg reward: -89.98155067870267\n",
            "Episode: 273, Epsilon: 0.7372400000011958, Steps: 89, Reward: -82.30401811889715, Best reward: 36.68347804548469, Avg reward: -87.68292945016661\n",
            "Episode: 274, Epsilon: 0.7364800000011993, Steps: 76, Reward: -70.83496176498736, Best reward: 36.68347804548469, Avg reward: -86.23882159838895\n",
            "Episode: 275, Epsilon: 0.7355000000012037, Steps: 98, Reward: -99.90200709225293, Best reward: 36.68347804548469, Avg reward: -86.51192303267388\n",
            "Episode: 276, Epsilon: 0.734570000001208, Steps: 93, Reward: -59.850648244470385, Best reward: 36.68347804548469, Avg reward: -86.60715697540454\n",
            "Episode: 277, Epsilon: 0.7337800000012116, Steps: 79, Reward: -41.12816713298413, Best reward: 36.68347804548469, Avg reward: -86.05883381120886\n",
            "Episode: 278, Epsilon: 0.7330400000012149, Steps: 74, Reward: -52.81768532894479, Best reward: 36.68347804548469, Avg reward: -85.46980127165098\n",
            "Episode: 279, Epsilon: 0.7320000000012197, Steps: 104, Reward: -119.75648840700232, Best reward: 36.68347804548469, Avg reward: -85.6544831177333\n",
            "Episode: 280, Epsilon: 0.7306800000012257, Steps: 132, Reward: -117.2254830129517, Best reward: 36.68347804548469, Avg reward: -85.90161955241203\n",
            "Episode: 281, Epsilon: 0.7295500000012308, Steps: 113, Reward: -61.993592315112004, Best reward: 36.68347804548469, Avg reward: -85.95301818231378\n",
            "Episode: 282, Epsilon: 0.7285400000012354, Steps: 101, Reward: -85.26702278441329, Best reward: 36.68347804548469, Avg reward: -85.6922109172109\n",
            "Episode: 283, Epsilon: 0.7277300000012391, Steps: 81, Reward: -52.26862040661052, Best reward: 36.68347804548469, Avg reward: -85.41009414649844\n",
            "Episode: 284, Epsilon: 0.7271200000012419, Steps: 61, Reward: -72.89973227097815, Best reward: 36.68347804548469, Avg reward: -85.51169989897863\n",
            "Episode: 285, Epsilon: 0.7263900000012452, Steps: 73, Reward: -68.95382098017221, Best reward: 36.68347804548469, Avg reward: -85.7603224080372\n",
            "Episode: 286, Epsilon: 0.7256100000012488, Steps: 78, Reward: -89.98703080172261, Best reward: 36.68347804548469, Avg reward: -85.83756301319494\n",
            "Episode: 287, Epsilon: 0.7250300000012514, Steps: 58, Reward: -81.74939326889653, Best reward: 36.68347804548469, Avg reward: -86.1670530591002\n",
            "Episode: 288, Epsilon: 0.7236600000012576, Steps: 137, Reward: 19.379745820278984, Best reward: 36.68347804548469, Avg reward: -85.06404374882148\n",
            "Episode: 289, Epsilon: 0.7227500000012618, Steps: 91, Reward: -88.73741234919572, Best reward: 36.68347804548469, Avg reward: -85.3685435447272\n",
            "Episode: 290, Epsilon: 0.7217400000012664, Steps: 101, Reward: -124.17595739106294, Best reward: 36.68347804548469, Avg reward: -86.14539234931239\n",
            "Episode: 291, Epsilon: 0.720720000001271, Steps: 102, Reward: -62.14450781613129, Best reward: 36.68347804548469, Avg reward: -85.95335355323364\n",
            "Episode: 292, Epsilon: 0.7197000000012757, Steps: 102, Reward: -76.14533234456286, Best reward: 36.68347804548469, Avg reward: -85.99070897922593\n",
            "Episode: 293, Epsilon: 0.7187700000012799, Steps: 93, Reward: -79.43699380904194, Best reward: 36.68347804548469, Avg reward: -87.15191369777119\n",
            "Episode: 294, Epsilon: 0.7175000000012857, Steps: 127, Reward: -86.74961456398643, Best reward: 36.68347804548469, Avg reward: -86.85155286596324\n",
            "Episode: 295, Epsilon: 0.7161400000012919, Steps: 136, Reward: -46.63944105090938, Best reward: 36.68347804548469, Avg reward: -86.53026588079233\n",
            "Episode: 296, Epsilon: 0.7151400000012964, Steps: 100, Reward: -122.71304308159758, Best reward: 36.68347804548469, Avg reward: -86.78893441492832\n",
            "Episode: 297, Epsilon: 0.7142000000013007, Steps: 94, Reward: -68.46106826592703, Best reward: 36.68347804548469, Avg reward: -86.77934927022\n",
            "Episode: 298, Epsilon: 0.7129800000013062, Steps: 122, Reward: -70.55646408558908, Best reward: 36.68347804548469, Avg reward: -86.79092713307888\n",
            "Episode: 299, Epsilon: 0.7116900000013121, Steps: 129, Reward: -35.85468967615407, Best reward: 36.68347804548469, Avg reward: -85.89622983616675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 300, Epsilon: 0.7108200000013161, Steps: 87, Reward: -57.538304836932426, Best reward: 36.68347804548469, Avg reward: -85.73017336541935\n",
            "Episode: 301, Epsilon: 0.7095000000013221, Steps: 132, Reward: -137.1341069667628, Best reward: 36.68347804548469, Avg reward: -85.69186404910918\n",
            "Episode: 302, Epsilon: 0.7082900000013276, Steps: 121, Reward: -81.43462142984511, Best reward: 36.68347804548469, Avg reward: -85.63066779891305\n",
            "Episode: 303, Epsilon: 0.7072600000013323, Steps: 103, Reward: -82.47002910696196, Best reward: 36.68347804548469, Avg reward: -85.50262625835252\n",
            "Episode: 304, Epsilon: 0.706210000001337, Steps: 105, Reward: -118.57680689981528, Best reward: 36.68347804548469, Avg reward: -85.99876687621848\n",
            "Episode: 305, Epsilon: 0.7053100000013411, Steps: 90, Reward: -86.79761955319887, Best reward: 36.68347804548469, Avg reward: -86.02100050087962\n",
            "Episode: 306, Epsilon: 0.7039400000013474, Steps: 137, Reward: -53.61170778739227, Best reward: 36.68347804548469, Avg reward: -85.39331107460299\n",
            "Episode: 307, Epsilon: 0.7030100000013516, Steps: 93, Reward: -140.905165229013, Best reward: 36.68347804548469, Avg reward: -86.69811396288675\n",
            "Episode: 308, Epsilon: 0.7021200000013557, Steps: 89, Reward: -69.89094600005258, Best reward: 36.68347804548469, Avg reward: -86.66814510910807\n",
            "Episode: 309, Epsilon: 0.7011400000013601, Steps: 98, Reward: -110.80476585724007, Best reward: 36.68347804548469, Avg reward: -87.51867784287458\n",
            "Episode: 310, Epsilon: 0.7003200000013639, Steps: 82, Reward: -53.72891745600785, Best reward: 36.68347804548469, Avg reward: -86.67929307678402\n",
            "Episode: 311, Epsilon: 0.699200000001369, Steps: 112, Reward: -89.06829197658907, Best reward: 36.68347804548469, Avg reward: -86.67605309139645\n",
            "Episode: 312, Epsilon: 0.6979000000013749, Steps: 130, Reward: -313.1503015250337, Best reward: 36.68347804548469, Avg reward: -89.04480133790261\n",
            "Episode: 313, Epsilon: 0.6969000000013794, Steps: 100, Reward: -80.3675166404013, Best reward: 36.68347804548469, Avg reward: -88.68371068308801\n",
            "Episode: 314, Epsilon: 0.6960000000013835, Steps: 90, Reward: -60.767953175643086, Best reward: 36.68347804548469, Avg reward: -88.29255522260401\n",
            "Episode: 315, Epsilon: 0.6950500000013878, Steps: 95, Reward: -79.14158966085708, Best reward: 36.68347804548469, Avg reward: -88.20840233772698\n",
            "Episode: 316, Epsilon: 0.6941600000013919, Steps: 89, Reward: -72.7320289535646, Best reward: 36.68347804548469, Avg reward: -88.20492435044588\n",
            "Episode: 317, Epsilon: 0.6932700000013959, Steps: 89, Reward: -94.08827402670823, Best reward: 36.68347804548469, Avg reward: -88.30712669955524\n",
            "Episode: 318, Epsilon: 0.6920800000014014, Steps: 119, Reward: -60.425331264567305, Best reward: 36.68347804548469, Avg reward: -87.46505862144635\n",
            "Episode: 319, Epsilon: 0.6914300000014043, Steps: 65, Reward: -71.01466589359377, Best reward: 36.68347804548469, Avg reward: -86.78854931400771\n",
            "Episode: 320, Epsilon: 0.6903600000014092, Steps: 107, Reward: -91.71534811370518, Best reward: 36.68347804548469, Avg reward: -87.10468478620095\n",
            "Episode: 321, Epsilon: 0.6892700000014141, Steps: 109, Reward: -63.23674157057738, Best reward: 36.68347804548469, Avg reward: -87.01990526548306\n",
            "Episode: 322, Epsilon: 0.6879500000014201, Steps: 132, Reward: -101.26845171907493, Best reward: 36.68347804548469, Avg reward: -87.13046795805342\n",
            "Episode: 323, Epsilon: 0.6871400000014238, Steps: 81, Reward: -41.86487099165578, Best reward: 36.68347804548469, Avg reward: -86.4711667645094\n",
            "Episode: 324, Epsilon: 0.6862000000014281, Steps: 94, Reward: -68.91581037349887, Best reward: 36.68347804548469, Avg reward: -86.14394870831927\n",
            "Episode: 325, Epsilon: 0.6853000000014322, Steps: 90, Reward: -71.81319664069984, Best reward: 36.68347804548469, Avg reward: -85.98680770084167\n",
            "Episode: 326, Epsilon: 0.6841900000014373, Steps: 111, Reward: -60.06717078274548, Best reward: 36.68347804548469, Avg reward: -84.97203718444125\n",
            "Episode: 327, Epsilon: 0.6833200000014412, Steps: 87, Reward: -92.18244440138596, Best reward: 36.68347804548469, Avg reward: -84.85350199060025\n",
            "Episode: 328, Epsilon: 0.682260000001446, Steps: 106, Reward: -96.99190498769836, Best reward: 36.68347804548469, Avg reward: -84.67172531195263\n",
            "Episode: 329, Epsilon: 0.6812000000014509, Steps: 106, Reward: -78.07176675294049, Best reward: 36.68347804548469, Avg reward: -84.68367164874613\n",
            "Episode: 330, Epsilon: 0.6796500000014579, Steps: 155, Reward: -189.10407238810413, Best reward: 36.68347804548469, Avg reward: -85.53141891790386\n",
            "Episode: 331, Epsilon: 0.6784600000014633, Steps: 119, Reward: 24.743261314835237, Best reward: 36.68347804548469, Avg reward: -84.47241630467649\n",
            "Episode: 332, Epsilon: 0.6775700000014674, Steps: 89, Reward: -62.89946502507462, Best reward: 36.68347804548469, Avg reward: -84.33079095280256\n",
            "Episode: 333, Epsilon: 0.676770000001471, Steps: 80, Reward: -58.97571079976045, Best reward: 36.68347804548469, Avg reward: -83.63089050074004\n",
            "Episode: 334, Epsilon: 0.6757800000014755, Steps: 99, Reward: -112.82039704357221, Best reward: 36.68347804548469, Avg reward: -82.98837361257594\n",
            "Episode: 335, Epsilon: 0.6750000000014791, Steps: 78, Reward: -86.41895930275237, Best reward: 36.68347804548469, Avg reward: -82.88943661947633\n",
            "Episode: 336, Epsilon: 0.673920000001484, Steps: 108, Reward: -25.21139182959726, Best reward: 36.68347804548469, Avg reward: -82.29610312122406\n",
            "Episode: 337, Epsilon: 0.673050000001488, Steps: 87, Reward: -11.97310610628196, Best reward: 36.68347804548469, Avg reward: -81.66111715389317\n",
            "Episode: 338, Epsilon: 0.6722800000014915, Steps: 77, Reward: -35.08171933854257, Best reward: 36.68347804548469, Avg reward: -81.37764865867581\n",
            "Episode: 339, Epsilon: 0.6711100000014968, Steps: 117, Reward: -84.33548687715842, Best reward: 36.68347804548469, Avg reward: -81.98092099413876\n",
            "Episode: 340, Epsilon: 0.6703100000015004, Steps: 80, Reward: -115.28344848916934, Best reward: 36.68347804548469, Avg reward: -82.09666839662754\n",
            "Episode: 341, Epsilon: 0.6692200000015054, Steps: 109, Reward: -111.11320869225077, Best reward: 36.68347804548469, Avg reward: -82.0338339256677\n",
            "Episode: 342, Epsilon: 0.66821000000151, Steps: 101, Reward: -46.924608903446654, Best reward: 36.68347804548469, Avg reward: -82.18961883012214\n",
            "Episode: 343, Epsilon: 0.6668500000015162, Steps: 136, Reward: -48.81621030940285, Best reward: 36.68347804548469, Avg reward: -82.03179336045581\n",
            "Episode: 344, Epsilon: 0.6654200000015227, Steps: 143, Reward: -96.81018459467128, Best reward: 36.68347804548469, Avg reward: -82.58053345579845\n",
            "Episode: 345, Epsilon: 0.6647300000015258, Steps: 69, Reward: -71.5640095604858, Best reward: 36.68347804548469, Avg reward: -82.26493761324465\n",
            "Episode: 346, Epsilon: 0.6637800000015301, Steps: 95, Reward: 15.70284368178919, Best reward: 36.68347804548469, Avg reward: -80.95865944095344\n",
            "Episode: 347, Epsilon: 0.6630000000015337, Steps: 78, Reward: 6.9007465688783896, Best reward: 36.68347804548469, Avg reward: -80.04340084949949\n",
            "Episode: 348, Epsilon: 0.6618500000015389, Steps: 115, Reward: -83.5097115016998, Best reward: 36.68347804548469, Avg reward: -80.35571534636242\n",
            "Episode: 349, Epsilon: 0.6607100000015441, Steps: 114, Reward: -277.9391464744766, Best reward: 36.68347804548469, Avg reward: -82.11711078386479\n",
            "Episode: 350, Epsilon: 0.6597000000015487, Steps: 101, Reward: -101.19462530168228, Best reward: 36.68347804548469, Avg reward: -82.86380711110485\n",
            "Episode: 351, Epsilon: 0.6583700000015548, Steps: 133, Reward: -44.87057893968212, Best reward: 36.68347804548469, Avg reward: -82.3047298337502\n",
            "Episode: 352, Epsilon: 0.6569600000015612, Steps: 141, Reward: -100.0294653500515, Best reward: 36.68347804548469, Avg reward: -82.41759864765493\n",
            "Episode: 353, Epsilon: 0.6558800000015661, Steps: 108, Reward: -120.78395633314094, Best reward: 36.68347804548469, Avg reward: -83.06944493023585\n",
            "Episode: 354, Epsilon: 0.6550700000015698, Steps: 81, Reward: -54.660555275563496, Best reward: 36.68347804548469, Avg reward: -83.19029528100253\n",
            "Episode: 355, Epsilon: 0.6540700000015743, Steps: 100, Reward: -92.91710149018031, Best reward: 36.68347804548469, Avg reward: -83.53022998901339\n",
            "Episode: 356, Epsilon: 0.6533100000015778, Steps: 76, Reward: -61.041904959790585, Best reward: 36.68347804548469, Avg reward: -83.020959476928\n",
            "Episode: 357, Epsilon: 0.6524000000015819, Steps: 91, Reward: -89.13723889730439, Best reward: 36.68347804548469, Avg reward: -83.42161297884199\n",
            "Episode: 358, Epsilon: 0.6509700000015884, Steps: 143, Reward: -15.658356656446983, Best reward: 36.68347804548469, Avg reward: -82.31338254131988\n",
            "Episode: 359, Epsilon: 0.6501200000015923, Steps: 85, Reward: -67.78310901277851, Best reward: 36.68347804548469, Avg reward: -82.53793279052005\n",
            "Episode: 360, Epsilon: 0.6488000000015983, Steps: 132, Reward: -217.85152332751198, Best reward: 36.68347804548469, Avg reward: -84.29463526568024\n",
            "Episode: 361, Epsilon: 0.6476400000016036, Steps: 116, Reward: -60.45336593791302, Best reward: 36.68347804548469, Avg reward: -84.08007873542607\n",
            "Episode: 362, Epsilon: 0.6467900000016075, Steps: 85, Reward: -56.74433897642587, Best reward: 36.68347804548469, Avg reward: -83.87432881283354\n",
            "Episode: 363, Epsilon: 0.6458100000016119, Steps: 98, Reward: -162.34885697923994, Best reward: 36.68347804548469, Avg reward: -84.26783167547381\n",
            "Episode: 364, Epsilon: 0.6445100000016178, Steps: 130, Reward: -163.56192753614195, Best reward: 36.68347804548469, Avg reward: -84.97579095441961\n",
            "Episode: 365, Epsilon: 0.6437800000016212, Steps: 73, Reward: -94.25427753474347, Best reward: 36.68347804548469, Avg reward: -85.98819814816969\n",
            "Episode: 366, Epsilon: 0.6428200000016255, Steps: 96, Reward: -85.74547262696956, Best reward: 36.68347804548469, Avg reward: -85.90936590966439\n",
            "Episode: 367, Epsilon: 0.6418200000016301, Steps: 100, Reward: -17.660281321753843, Best reward: 36.68347804548469, Avg reward: -85.079288467038\n",
            "Episode: 368, Epsilon: 0.6403900000016366, Steps: 143, Reward: -293.9193235517452, Best reward: 36.68347804548469, Avg reward: -87.2636242502701\n",
            "Episode: 369, Epsilon: 0.6392300000016419, Steps: 116, Reward: -80.58058867730166, Best reward: 36.68347804548469, Avg reward: -84.53897630898778\n",
            "Episode: 370, Epsilon: 0.6379500000016477, Steps: 128, Reward: -51.47737761158298, Best reward: 36.68347804548469, Avg reward: -83.7018080547315\n",
            "Episode: 371, Epsilon: 0.6372500000016509, Steps: 70, Reward: -61.399647576801044, Best reward: 36.68347804548469, Avg reward: -83.21886766941512\n",
            "Episode: 372, Epsilon: 0.6361600000016558, Steps: 109, Reward: -38.7158258493602, Best reward: 36.68347804548469, Avg reward: -82.83173786513493\n",
            "Episode: 373, Epsilon: 0.6354400000016591, Steps: 72, Reward: -27.25134681380608, Best reward: 36.68347804548469, Avg reward: -82.28121115208401\n",
            "Episode: 374, Epsilon: 0.6347100000016624, Steps: 73, Reward: -63.74927682819508, Best reward: 36.68347804548469, Avg reward: -82.2103543027161\n",
            "Episode: 375, Epsilon: 0.633720000001667, Steps: 99, Reward: -38.63165991371849, Best reward: 36.68347804548469, Avg reward: -81.59765083093075\n",
            "Episode: 376, Epsilon: 0.6329300000016705, Steps: 79, Reward: -48.9565552549373, Best reward: 36.68347804548469, Avg reward: -81.48870990103542\n",
            "Episode: 377, Epsilon: 0.6316400000016764, Steps: 129, Reward: -54.37422574207832, Best reward: 36.68347804548469, Avg reward: -81.62117048712636\n",
            "Episode: 378, Epsilon: 0.6309600000016795, Steps: 68, Reward: -71.6299504056158, Best reward: 36.68347804548469, Avg reward: -81.80929313789308\n",
            "Episode: 379, Epsilon: 0.6296500000016855, Steps: 131, Reward: -23.138932408636407, Best reward: 36.68347804548469, Avg reward: -80.84311757790942\n",
            "Episode: 380, Epsilon: 0.628870000001689, Steps: 78, Reward: -66.40467592208752, Best reward: 36.68347804548469, Avg reward: -80.33490950700079\n",
            "Episode: 381, Epsilon: 0.6275400000016951, Steps: 133, Reward: -224.63534984949416, Best reward: 36.68347804548469, Avg reward: -81.9613270823446\n",
            "Episode: 382, Epsilon: 0.6263600000017004, Steps: 118, Reward: -30.92305631606999, Best reward: 36.68347804548469, Avg reward: -81.41788741766118\n",
            "Episode: 383, Epsilon: 0.6251800000017058, Steps: 118, Reward: -78.72416494503446, Best reward: 36.68347804548469, Avg reward: -81.68244286304541\n",
            "Episode: 384, Epsilon: 0.6241600000017105, Steps: 102, Reward: -37.33062050227226, Best reward: 36.68347804548469, Avg reward: -81.32675174535835\n",
            "Episode: 385, Epsilon: 0.623390000001714, Steps: 77, Reward: -60.21140855221938, Best reward: 36.68347804548469, Avg reward: -81.23932762107881\n",
            "Episode: 386, Epsilon: 0.6222600000017191, Steps: 113, Reward: -47.41776552695271, Best reward: 36.68347804548469, Avg reward: -80.81363496833112\n",
            "Episode: 387, Epsilon: 0.6207900000017258, Steps: 147, Reward: -95.92443483748538, Best reward: 36.68347804548469, Avg reward: -80.95538538401699\n",
            "Episode: 388, Epsilon: 0.6199000000017298, Steps: 89, Reward: -70.8960931076363, Best reward: 36.68347804548469, Avg reward: -81.85814377329615\n",
            "Episode: 389, Epsilon: 0.6187300000017352, Steps: 117, Reward: -43.75708368532597, Best reward: 36.68347804548469, Avg reward: -81.40834048665744\n",
            "Episode: 390, Epsilon: 0.6177600000017396, Steps: 97, Reward: -66.68507266171765, Best reward: 36.68347804548469, Avg reward: -80.833431639364\n",
            "Episode: 391, Epsilon: 0.6168400000017438, Steps: 92, Reward: -39.58558794363137, Best reward: 36.68347804548469, Avg reward: -80.607842440639\n",
            "Episode: 392, Epsilon: 0.6154400000017501, Steps: 140, Reward: -9.193752032757303, Best reward: 36.68347804548469, Avg reward: -79.93832663752094\n",
            "Episode: 393, Epsilon: 0.6141300000017561, Steps: 131, Reward: -43.94936376411535, Best reward: 36.68347804548469, Avg reward: -79.5834503370717\n",
            "Episode: 394, Epsilon: 0.6128900000017617, Steps: 124, Reward: 9.356335262927317, Best reward: 36.68347804548469, Avg reward: -78.62239083880256\n",
            "Episode: 395, Epsilon: 0.6118900000017663, Steps: 100, Reward: -102.80444782628594, Best reward: 36.68347804548469, Avg reward: -79.18404090655633\n",
            "Episode: 396, Epsilon: 0.6107100000017717, Steps: 118, Reward: -71.51004741444238, Best reward: 36.68347804548469, Avg reward: -78.67201094988478\n",
            "Episode: 397, Epsilon: 0.6098000000017758, Steps: 91, Reward: -92.46222249580495, Best reward: 36.68347804548469, Avg reward: -78.91202249218355\n",
            "Episode: 398, Epsilon: 0.6088600000017801, Steps: 94, Reward: -19.360653476341525, Best reward: 36.68347804548469, Avg reward: -78.40006438609107\n",
            "Episode: 399, Epsilon: 0.6077600000017851, Steps: 110, Reward: -67.72792514728442, Best reward: 36.68347804548469, Avg reward: -78.71879674080238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 400, Epsilon: 0.6069200000017889, Steps: 84, Reward: -56.64343346379795, Best reward: 36.68347804548469, Avg reward: -78.70984802707103\n",
            "Episode: 401, Epsilon: 0.606240000001792, Steps: 68, Reward: -65.0573066613638, Best reward: 36.68347804548469, Avg reward: -77.98908002401704\n",
            "Episode: 402, Epsilon: 0.6053200000017962, Steps: 92, Reward: -61.51341401418895, Best reward: 36.68347804548469, Avg reward: -77.7898679498605\n",
            "Episode: 403, Epsilon: 0.6043800000018005, Steps: 94, Reward: -134.4675195548169, Best reward: 36.68347804548469, Avg reward: -78.30984285433904\n",
            "Episode: 404, Epsilon: 0.6035100000018044, Steps: 87, Reward: -73.30111200830679, Best reward: 36.68347804548469, Avg reward: -77.85708590542396\n",
            "Episode: 405, Epsilon: 0.6024300000018094, Steps: 108, Reward: -122.02839609438021, Best reward: 36.68347804548469, Avg reward: -78.20939367083577\n",
            "Episode: 406, Epsilon: 0.6014400000018139, Steps: 99, Reward: -89.6839729442661, Best reward: 36.68347804548469, Avg reward: -78.57011632240449\n",
            "Episode: 407, Epsilon: 0.6001100000018199, Steps: 133, Reward: -30.57064955726611, Best reward: 36.68347804548469, Avg reward: -77.46677116568702\n",
            "Episode: 408, Epsilon: 0.5991400000018243, Steps: 97, Reward: -35.51238044731281, Best reward: 36.68347804548469, Avg reward: -77.12298551015964\n",
            "Episode: 409, Epsilon: 0.5980600000018292, Steps: 108, Reward: -0.24606044160924512, Best reward: 36.68347804548469, Avg reward: -76.01739845600333\n",
            "Episode: 410, Epsilon: 0.5969400000018343, Steps: 112, Reward: -21.015130082882493, Best reward: 36.68347804548469, Avg reward: -75.69026058227207\n",
            "Episode: 411, Epsilon: 0.5958000000018395, Steps: 114, Reward: -82.34911005110084, Best reward: 36.68347804548469, Avg reward: -75.62306876301719\n",
            "Episode: 412, Epsilon: 0.5943000000018464, Steps: 150, Reward: -133.93458301380534, Best reward: 36.68347804548469, Avg reward: -73.83091157790491\n",
            "Episode: 413, Epsilon: 0.5932500000018511, Steps: 105, Reward: -108.32369788931324, Best reward: 36.68347804548469, Avg reward: -74.11047339039402\n",
            "Episode: 414, Epsilon: 0.5918700000018574, Steps: 138, Reward: -20.459393689967342, Best reward: 36.68347804548469, Avg reward: -73.70738779553727\n",
            "Episode: 415, Epsilon: 0.5905200000018636, Steps: 135, Reward: 9.985165463423442, Best reward: 36.68347804548469, Avg reward: -72.81612024429447\n",
            "Episode: 416, Epsilon: 0.5895800000018678, Steps: 94, Reward: -42.88594796599998, Best reward: 36.68347804548469, Avg reward: -72.51765943441882\n",
            "Episode: 417, Epsilon: 0.5883600000018734, Steps: 122, Reward: -38.03935080102289, Best reward: 36.68347804548469, Avg reward: -71.95717020216198\n",
            "Episode: 418, Epsilon: 0.5871600000018788, Steps: 120, Reward: -81.77657428674783, Best reward: 36.68347804548469, Avg reward: -72.17068263238376\n",
            "Episode: 419, Epsilon: 0.5857400000018853, Steps: 142, Reward: 5.743184028853435, Best reward: 36.68347804548469, Avg reward: -71.40310413315929\n",
            "Episode: 420, Epsilon: 0.5849700000018888, Steps: 77, Reward: -64.28904208369264, Best reward: 36.68347804548469, Avg reward: -71.12884107285917\n",
            "Episode: 421, Epsilon: 0.5833700000018961, Steps: 160, Reward: -16.665982116177716, Best reward: 36.68347804548469, Avg reward: -70.66313347831517\n",
            "Episode: 422, Epsilon: 0.5821500000019016, Steps: 122, Reward: -29.74620082534028, Best reward: 36.68347804548469, Avg reward: -69.94791096937783\n",
            "Episode: 423, Epsilon: 0.5813600000019052, Steps: 79, Reward: -74.671446978815, Best reward: 36.68347804548469, Avg reward: -70.27597672924942\n",
            "Episode: 424, Epsilon: 0.5799700000019116, Steps: 139, Reward: -72.88582121927992, Best reward: 36.68347804548469, Avg reward: -70.31567683770723\n",
            "Episode: 425, Epsilon: 0.5788100000019168, Steps: 116, Reward: -162.1525057444849, Best reward: 36.68347804548469, Avg reward: -71.21906992874509\n",
            "Episode: 426, Epsilon: 0.5776900000019219, Steps: 112, Reward: -17.696974301245646, Best reward: 36.68347804548469, Avg reward: -70.79536796393009\n",
            "Episode: 427, Epsilon: 0.5766500000019267, Steps: 104, Reward: -55.82603041199233, Best reward: 36.68347804548469, Avg reward: -70.43180382403615\n",
            "Episode: 428, Epsilon: 0.5755300000019318, Steps: 112, Reward: -54.114848189290065, Best reward: 36.68347804548469, Avg reward: -70.00303325605206\n",
            "Episode: 429, Epsilon: 0.5744500000019367, Steps: 108, Reward: -66.6888367668971, Best reward: 36.68347804548469, Avg reward: -69.88920395619162\n",
            "Episode: 430, Epsilon: 0.5734900000019411, Steps: 96, Reward: -13.285818049202206, Best reward: 36.68347804548469, Avg reward: -68.1310214128026\n",
            "Episode: 431, Epsilon: 0.5725200000019455, Steps: 97, Reward: -45.29762889878885, Best reward: 36.68347804548469, Avg reward: -68.83143031493886\n",
            "Episode: 432, Epsilon: 0.5714000000019506, Steps: 112, Reward: -67.68389601015096, Best reward: 36.68347804548469, Avg reward: -68.87927462478962\n",
            "Episode: 433, Epsilon: 0.5703500000019553, Steps: 105, Reward: -72.7510097494935, Best reward: 36.68347804548469, Avg reward: -69.01702761428696\n",
            "Episode: 434, Epsilon: 0.5692300000019604, Steps: 112, Reward: -19.096423676607472, Best reward: 36.68347804548469, Avg reward: -68.0797878806173\n",
            "Episode: 435, Epsilon: 0.5683200000019646, Steps: 91, Reward: -89.78337336348416, Best reward: 36.68347804548469, Avg reward: -68.11343202122461\n",
            "Episode: 436, Epsilon: 0.5669500000019708, Steps: 137, Reward: -69.07090209619298, Best reward: 36.68347804548469, Avg reward: -68.55202712389058\n",
            "Episode: 437, Epsilon: 0.5658000000019761, Steps: 115, Reward: -34.63526432619079, Best reward: 36.68347804548469, Avg reward: -68.77864870608965\n",
            "Episode: 438, Epsilon: 0.5607400000019991, Steps: 506, Reward: -162.64712518024712, Best reward: 36.68347804548469, Avg reward: -70.0543027645067\n",
            "Episode: 439, Epsilon: 0.5598000000020034, Steps: 94, Reward: -93.24712155383817, Best reward: 36.68347804548469, Avg reward: -70.1434191112735\n",
            "Episode: 440, Epsilon: 0.5583000000020102, Steps: 150, Reward: -12.35547049060618, Best reward: 36.68347804548469, Avg reward: -69.11413933128786\n",
            "Episode: 441, Epsilon: 0.5567300000020173, Steps: 157, Reward: -94.50289707568942, Best reward: 36.68347804548469, Avg reward: -68.94803621512226\n",
            "Episode: 442, Epsilon: 0.5553600000020236, Steps: 137, Reward: -61.78005370659476, Best reward: 36.68347804548469, Avg reward: -69.09659066315373\n",
            "Episode: 443, Epsilon: 0.5542000000020288, Steps: 116, Reward: -102.59649675212293, Best reward: 36.68347804548469, Avg reward: -69.63439352758095\n",
            "Episode: 444, Epsilon: 0.5533500000020327, Steps: 85, Reward: -365.6993361543224, Best reward: 36.68347804548469, Avg reward: -72.32328504317746\n",
            "Episode: 445, Epsilon: 0.5521000000020384, Steps: 125, Reward: -52.22868452526486, Best reward: 36.68347804548469, Avg reward: -72.12993179282525\n",
            "Episode: 446, Epsilon: 0.5507900000020444, Steps: 131, Reward: -121.70981603361159, Best reward: 36.68347804548469, Avg reward: -73.50405838997925\n",
            "Episode: 447, Epsilon: 0.5498400000020487, Steps: 95, Reward: -40.995596909226805, Best reward: 36.68347804548469, Avg reward: -73.98302182476031\n",
            "Episode: 448, Epsilon: 0.5483600000020554, Steps: 148, Reward: -5.828165868030624, Best reward: 36.68347804548469, Avg reward: -73.20620636842361\n",
            "Episode: 449, Epsilon: 0.5473000000020602, Steps: 106, Reward: 13.724266435031225, Best reward: 36.68347804548469, Avg reward: -70.28957223932854\n",
            "Episode: 450, Epsilon: 0.5463400000020646, Steps: 96, Reward: 10.088502807647558, Best reward: 36.68347804548469, Avg reward: -69.17674095823523\n",
            "Episode: 451, Epsilon: 0.5452300000020697, Steps: 111, Reward: -11.67119285973915, Best reward: 36.68347804548469, Avg reward: -68.8447470974358\n",
            "Episode: 452, Epsilon: 0.5440000000020753, Steps: 123, Reward: -60.48621312578067, Best reward: 36.68347804548469, Avg reward: -68.4493145751931\n",
            "Episode: 453, Epsilon: 0.5423500000020828, Steps: 165, Reward: 2.8586124203815046, Best reward: 36.68347804548469, Avg reward: -67.21288888765787\n",
            "Episode: 454, Epsilon: 0.5413700000020872, Steps: 98, Reward: -52.8878918697304, Best reward: 36.68347804548469, Avg reward: -67.19516225359953\n",
            "Episode: 455, Epsilon: 0.5402300000020924, Steps: 114, Reward: -99.41597446720003, Best reward: 36.68347804548469, Avg reward: -67.26015098336973\n",
            "Episode: 456, Epsilon: 0.5390700000020977, Steps: 116, Reward: 18.246858908938577, Best reward: 36.68347804548469, Avg reward: -66.46726334468245\n",
            "Episode: 457, Epsilon: 0.537910000002103, Steps: 116, Reward: -112.72851746974081, Best reward: 36.68347804548469, Avg reward: -66.7031761304068\n",
            "Episode: 458, Epsilon: 0.5367300000021084, Steps: 118, Reward: -24.41649314454601, Best reward: 36.68347804548469, Avg reward: -66.79075749528779\n",
            "Episode: 459, Epsilon: 0.5354400000021142, Steps: 129, Reward: -58.14072863297371, Best reward: 36.68347804548469, Avg reward: -66.69433369148976\n",
            "Episode: 460, Epsilon: 0.5337900000021217, Steps: 165, Reward: -6.925651861190474, Best reward: 36.68347804548469, Avg reward: -64.58507497682653\n",
            "Episode: 461, Epsilon: 0.5327900000021263, Steps: 100, Reward: 25.780617336453943, Best reward: 36.68347804548469, Avg reward: -63.72273514408286\n",
            "Episode: 462, Epsilon: 0.5312300000021334, Steps: 156, Reward: -39.62274525865605, Best reward: 36.68347804548469, Avg reward: -63.55151920690517\n",
            "Episode: 463, Epsilon: 0.5298600000021396, Steps: 137, Reward: -23.69349578127516, Best reward: 36.68347804548469, Avg reward: -62.164965594925526\n",
            "Episode: 464, Epsilon: 0.5286200000021453, Steps: 124, Reward: -18.782423512929284, Best reward: 36.68347804548469, Avg reward: -60.717170554693396\n",
            "Episode: 465, Epsilon: 0.5270000000021526, Steps: 162, Reward: -18.48563293985552, Best reward: 36.68347804548469, Avg reward: -59.95948410874452\n",
            "Episode: 466, Epsilon: 0.5256200000021589, Steps: 138, Reward: -18.628750123399016, Best reward: 36.68347804548469, Avg reward: -59.28831688370881\n",
            "Episode: 467, Epsilon: 0.5244200000021644, Steps: 120, Reward: -65.24895785067602, Best reward: 36.68347804548469, Avg reward: -59.76420364899803\n",
            "Episode: 468, Epsilon: 0.52319000000217, Steps: 123, Reward: -32.6027325835087, Best reward: 36.68347804548469, Avg reward: -57.15103773931566\n",
            "Episode: 469, Epsilon: 0.5222900000021741, Steps: 90, Reward: -36.04964540534735, Best reward: 36.68347804548469, Avg reward: -56.70572830659612\n",
            "Episode: 470, Epsilon: 0.5122900000022196, Steps: 1000, Reward: 66.49234204341262, Best reward: 36.68347804548469, Avg reward: -55.52603111004616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 471, Epsilon: 0.5022900000022651, Steps: 1000, Reward: -31.882409229030245, Best reward: 66.49234204341262, Avg reward: -55.23085872656846\n",
            "Episode: 472, Epsilon: 0.5010200000022709, Steps: 127, Reward: 64.28201382178975, Best reward: 66.49234204341262, Avg reward: -54.200880329856965\n",
            "Episode: 473, Epsilon: 0.4996700000022752, Steps: 135, Reward: -22.59623974672307, Best reward: 66.49234204341262, Avg reward: -54.154329259186134\n",
            "Episode: 474, Epsilon: 0.498510000002274, Steps: 116, Reward: -53.869949077213825, Best reward: 66.49234204341262, Avg reward: -54.05553598167632\n",
            "Episode: 475, Epsilon: 0.49704000000227255, Steps: 147, Reward: -99.34579363732428, Best reward: 66.49234204341262, Avg reward: -54.66267731891237\n",
            "Episode: 476, Epsilon: 0.4947600000022703, Steps: 228, Reward: -22.187194647113927, Best reward: 66.49234204341262, Avg reward: -54.39498371283414\n",
            "Episode: 477, Epsilon: 0.4933700000022689, Steps: 139, Reward: -98.15110444905517, Best reward: 66.49234204341262, Avg reward: -54.83275249990391\n",
            "Episode: 478, Epsilon: 0.4927000000022682, Steps: 67, Reward: -41.54848189577196, Best reward: 66.49234204341262, Avg reward: -54.53193781480548\n",
            "Episode: 479, Epsilon: 0.49092000000226643, Steps: 178, Reward: -41.45562610721694, Best reward: 66.49234204341262, Avg reward: -54.71510475179128\n",
            "Episode: 480, Epsilon: 0.48092000000225643, Steps: 1000, Reward: -64.62880144574858, Best reward: 66.49234204341262, Avg reward: -54.69734600702789\n",
            "Episode: 481, Epsilon: 0.4799900000022555, Steps: 93, Reward: 10.503793961772061, Best reward: 66.49234204341262, Avg reward: -52.345954568915225\n",
            "Episode: 482, Epsilon: 0.4787100000022542, Steps: 128, Reward: -21.01053304522489, Best reward: 66.49234204341262, Avg reward: -52.24682933620677\n",
            "Episode: 483, Epsilon: 0.4777800000022533, Steps: 93, Reward: -86.77531991428691, Best reward: 66.49234204341262, Avg reward: -52.327340885899304\n",
            "Episode: 484, Epsilon: 0.4766700000022522, Steps: 111, Reward: -69.42953127584997, Best reward: 66.49234204341262, Avg reward: -52.64832999363509\n",
            "Episode: 485, Epsilon: 0.4751800000022507, Steps: 149, Reward: 9.359447639047275, Best reward: 66.49234204341262, Avg reward: -51.952621431722406\n",
            "Episode: 486, Epsilon: 0.4739900000022495, Steps: 119, Reward: 7.283421068936448, Best reward: 66.49234204341262, Avg reward: -51.40560956576353\n",
            "Episode: 487, Epsilon: 0.47203000000224754, Steps: 196, Reward: 54.730328171178144, Best reward: 66.49234204341262, Avg reward: -49.899061935676876\n",
            "Episode: 488, Epsilon: 0.47046000000224597, Steps: 157, Reward: -193.94928027786412, Best reward: 66.49234204341262, Avg reward: -51.12959380737916\n",
            "Episode: 489, Epsilon: 0.46046000000223597, Steps: 1000, Reward: 69.77596247459333, Best reward: 66.49234204341262, Avg reward: -49.99426334577997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 490, Epsilon: 0.45856000000223407, Steps: 190, Reward: -27.63588711574188, Best reward: 69.77596247459333, Avg reward: -49.603771490320206\n",
            "Episode: 491, Epsilon: 0.45694000000223245, Steps: 162, Reward: -144.74638015519065, Best reward: 69.77596247459333, Avg reward: -50.655379412435806\n",
            "Episode: 492, Epsilon: 0.44694000000222245, Steps: 1000, Reward: 55.25017998654518, Best reward: 69.77596247459333, Avg reward: -50.010940092242784\n",
            "Episode: 493, Epsilon: 0.43694000000221245, Steps: 1000, Reward: -8.160053057633599, Best reward: 69.77596247459333, Avg reward: -49.65304698517795\n",
            "Episode: 494, Epsilon: 0.4351100000022106, Steps: 183, Reward: -26.894936505513925, Best reward: 69.77596247459333, Avg reward: -50.01555970286236\n",
            "Episode: 495, Epsilon: 0.4251100000022006, Steps: 1000, Reward: 111.09055116379537, Best reward: 69.77596247459333, Avg reward: -47.87660971296155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 496, Epsilon: 0.42384000000219935, Steps: 127, Reward: -32.272804140664235, Best reward: 111.09055116379537, Avg reward: -47.48423728022378\n",
            "Episode: 497, Epsilon: 0.4226000000021981, Steps: 124, Reward: -91.23471994220345, Best reward: 111.09055116379537, Avg reward: -47.47196225468777\n",
            "Episode: 498, Epsilon: 0.42106000000219657, Steps: 154, Reward: -0.8203527845621181, Best reward: 111.09055116379537, Avg reward: -47.286559247769965\n",
            "Episode: 499, Epsilon: 0.41953000000219504, Steps: 153, Reward: -43.05481125128692, Best reward: 111.09055116379537, Avg reward: -47.039828108809985\n",
            "Episode: 500, Epsilon: 0.40953000000218504, Steps: 1000, Reward: 100.86455974307793, Best reward: 111.09055116379537, Avg reward: -45.46474817674123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 501, Epsilon: 0.4086100000021841, Steps: 92, Reward: -23.17287108474413, Best reward: 111.09055116379537, Avg reward: -45.045903820975035\n",
            "Episode: 502, Epsilon: 0.40726000000218276, Steps: 135, Reward: -0.8092249625633059, Best reward: 111.09055116379537, Avg reward: -44.43886193045878\n",
            "Episode: 503, Epsilon: 0.4062800000021818, Steps: 98, Reward: -46.2359921891832, Best reward: 111.09055116379537, Avg reward: -43.556546656802446\n",
            "Episode: 504, Epsilon: 0.3962800000021718, Steps: 1000, Reward: 73.8656242497532, Best reward: 111.09055116379537, Avg reward: -42.084879294221835\n",
            "Episode: 505, Epsilon: 0.3862800000021618, Steps: 1000, Reward: 70.5391774464996, Best reward: 111.09055116379537, Avg reward: -40.15920355881305\n",
            "Episode: 506, Epsilon: 0.3852100000021607, Steps: 107, Reward: -39.98712199244079, Best reward: 111.09055116379537, Avg reward: -39.66223504929479\n",
            "Episode: 507, Epsilon: 0.3752100000021507, Steps: 1000, Reward: 114.63946890305046, Best reward: 111.09055116379537, Avg reward: -38.21013386469163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 508, Epsilon: 0.36837000000214387, Steps: 684, Reward: 237.73117237581158, Best reward: 114.63946890305046, Avg reward: -35.477698336460385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 509, Epsilon: 0.3657000000021412, Steps: 267, Reward: -24.38432380740582, Best reward: 237.73117237581158, Avg reward: -35.719080970118355\n",
            "Episode: 510, Epsilon: 0.3557000000021312, Steps: 1000, Reward: 123.81447124525083, Best reward: 237.73117237581158, Avg reward: -34.27078495683702\n",
            "Episode: 511, Epsilon: 0.3457000000021212, Steps: 1000, Reward: 123.11958918766645, Best reward: 237.73117237581158, Avg reward: -32.21609796444934\n",
            "Episode: 512, Epsilon: 0.34386000000211936, Steps: 184, Reward: -115.36482315671174, Best reward: 237.73117237581158, Avg reward: -32.03040036587841\n",
            "Episode: 513, Epsilon: 0.34226000000211776, Steps: 160, Reward: 3.985924646545797, Best reward: 237.73117237581158, Avg reward: -30.907304140519813\n",
            "Episode: 514, Epsilon: 0.34057000000211607, Steps: 169, Reward: 10.778085922418597, Best reward: 237.73117237581158, Avg reward: -30.59492934439596\n",
            "Episode: 515, Epsilon: 0.3359200000021114, Steps: 465, Reward: -111.73673587805746, Best reward: 237.73117237581158, Avg reward: -31.812148357810766\n",
            "Episode: 516, Epsilon: 0.3259200000021014, Steps: 1000, Reward: 78.00399231410213, Best reward: 237.73117237581158, Avg reward: -30.60324895500975\n",
            "Episode: 517, Epsilon: 0.32307000000209857, Steps: 285, Reward: -120.62381289240625, Best reward: 237.73117237581158, Avg reward: -31.42909357592358\n",
            "Episode: 518, Epsilon: 0.3177900000020933, Steps: 528, Reward: -277.614922674325, Best reward: 237.73117237581158, Avg reward: -33.38747705979935\n",
            "Episode: 519, Epsilon: 0.31425000000208975, Steps: 354, Reward: -94.17810981064463, Best reward: 237.73117237581158, Avg reward: -34.38668999819433\n",
            "Episode: 520, Epsilon: 0.30425000000207975, Steps: 1000, Reward: 116.86044263013827, Best reward: 237.73117237581158, Avg reward: -32.57519515105602\n",
            "Episode: 521, Epsilon: 0.29425000000206974, Steps: 1000, Reward: -14.871212387771038, Best reward: 237.73117237581158, Avg reward: -32.557247453771964\n",
            "Episode: 522, Epsilon: 0.2907800000020663, Steps: 347, Reward: -346.97023350827175, Best reward: 237.73117237581158, Avg reward: -35.72948778060127\n",
            "Episode: 523, Epsilon: 0.2807800000020563, Steps: 1000, Reward: 112.77220119307849, Best reward: 237.73117237581158, Avg reward: -33.85505129888234\n",
            "Episode: 524, Epsilon: 0.279480000002055, Steps: 130, Reward: -46.058103510390424, Best reward: 237.73117237581158, Avg reward: -33.58677412179344\n",
            "Episode: 525, Epsilon: 0.27847000000205396, Steps: 101, Reward: -31.6816197438896, Best reward: 237.73117237581158, Avg reward: -32.28206526178749\n",
            "Episode: 526, Epsilon: 0.26847000000204396, Steps: 1000, Reward: 81.70173709228563, Best reward: 237.73117237581158, Avg reward: -31.288078147852175\n",
            "Episode: 527, Epsilon: 0.2638100000020393, Steps: 466, Reward: -73.12617523603159, Best reward: 237.73117237581158, Avg reward: -31.461079596092567\n",
            "Episode: 528, Epsilon: 0.2622200000020377, Steps: 159, Reward: -35.21397280888962, Best reward: 237.73117237581158, Avg reward: -31.272070842288564\n",
            "Episode: 529, Epsilon: 0.25957000000203506, Steps: 265, Reward: -290.3094030899865, Best reward: 237.73117237581158, Avg reward: -33.508276505519454\n",
            "Episode: 530, Epsilon: 0.24957000000202506, Steps: 1000, Reward: 84.18697907556057, Best reward: 237.73117237581158, Avg reward: -32.53354853427183\n",
            "Episode: 531, Epsilon: 0.2481000000020236, Steps: 147, Reward: -59.069410773947936, Best reward: 237.73117237581158, Avg reward: -32.67126635302342\n",
            "Episode: 532, Epsilon: 0.24573000000202122, Steps: 237, Reward: 4.346702043365411, Best reward: 237.73117237581158, Avg reward: -31.950960372488257\n",
            "Episode: 533, Epsilon: 0.2407000000020162, Steps: 503, Reward: -195.32506136269296, Best reward: 237.73117237581158, Avg reward: -33.176700888620246\n",
            "Episode: 534, Epsilon: 0.2307000000020062, Steps: 1000, Reward: 111.48381415120647, Best reward: 237.73117237581158, Avg reward: -31.87089851034211\n",
            "Episode: 535, Epsilon: 0.2207000000019962, Steps: 1000, Reward: 43.526854288327044, Best reward: 237.73117237581158, Avg reward: -30.537796233824\n",
            "Episode: 536, Epsilon: 0.21538000000199087, Steps: 532, Reward: -93.72686155210319, Best reward: 237.73117237581158, Avg reward: -30.78435582838311\n",
            "Episode: 537, Epsilon: 0.20538000000198087, Steps: 1000, Reward: 34.743215481851024, Best reward: 237.73117237581158, Avg reward: -30.09057103030269\n",
            "Episode: 538, Epsilon: 0.20174000000197723, Steps: 364, Reward: -242.51086538899375, Best reward: 237.73117237581158, Avg reward: -30.889208432390156\n",
            "Episode: 539, Epsilon: 0.19374000000196923, Steps: 800, Reward: -168.25261012426762, Best reward: 237.73117237581158, Avg reward: -31.639263318094446\n",
            "Episode: 540, Epsilon: 0.18374000000195922, Steps: 1000, Reward: -104.44052224356575, Best reward: 237.73117237581158, Avg reward: -32.56011383562404\n",
            "Episode: 541, Epsilon: 0.17374000000194922, Steps: 1000, Reward: -23.75373674738904, Best reward: 237.73117237581158, Avg reward: -31.85262223234104\n",
            "Episode: 542, Epsilon: 0.16374000000193922, Steps: 1000, Reward: 81.6780550730119, Best reward: 237.73117237581158, Avg reward: -30.41804114454497\n",
            "Episode: 543, Epsilon: 0.15374000000192922, Steps: 1000, Reward: -26.89923571078083, Best reward: 237.73117237581158, Avg reward: -29.661068534131555\n",
            "Episode: 544, Epsilon: 0.14374000000191922, Steps: 1000, Reward: -73.08957801809122, Best reward: 237.73117237581158, Avg reward: -26.734970952769245\n",
            "Episode: 545, Epsilon: 0.13374000000190922, Steps: 1000, Reward: -84.92331928030445, Best reward: 237.73117237581158, Avg reward: -27.06191730031963\n",
            "Episode: 546, Epsilon: 0.12716000000190264, Steps: 658, Reward: 196.38211626243373, Best reward: 237.73117237581158, Avg reward: -23.880997977359183\n",
            "Episode: 547, Epsilon: 0.12046000000190224, Steps: 670, Reward: 171.50964125048375, Best reward: 237.73117237581158, Avg reward: -21.75594559576208\n",
            "Episode: 548, Epsilon: 0.11322000000190505, Steps: 724, Reward: 239.67828367560554, Best reward: 237.73117237581158, Avg reward: -19.300881100325718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 549, Epsilon: 0.10627000000190774, Steps: 695, Reward: 218.6736915554568, Best reward: 239.67828367560554, Avg reward: -17.25138684912146\n",
            "Episode: 550, Epsilon: 0.1, Steps: 809, Reward: 207.9102593393255, Best reward: 239.67828367560554, Avg reward: -15.27316928380468\n",
            "Episode: 551, Epsilon: 0.1, Steps: 1000, Reward: 98.26992825297552, Best reward: 239.67828367560554, Avg reward: -14.173758072677533\n",
            "Episode: 552, Epsilon: 0.1, Steps: 660, Reward: 208.72225097846018, Best reward: 239.67828367560554, Avg reward: -11.481673431635123\n",
            "Episode: 553, Epsilon: 0.1, Steps: 905, Reward: 100.72455370466716, Best reward: 239.67828367560554, Avg reward: -10.503014018792266\n",
            "Episode: 554, Epsilon: 0.1, Steps: 296, Reward: -51.374605995967116, Best reward: 239.67828367560554, Avg reward: -10.487881160054632\n",
            "Episode: 555, Epsilon: 0.1, Steps: 882, Reward: 248.66165952094283, Best reward: 239.67828367560554, Avg reward: -7.007104820173202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 556, Epsilon: 0.1, Steps: 506, Reward: 262.2915203200543, Best reward: 248.66165952094283, Avg reward: -4.566658206062045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 557, Epsilon: 0.1, Steps: 432, Reward: 280.1116527237653, Best reward: 262.2915203200543, Avg reward: -0.6382565041269856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 558, Epsilon: 0.1, Steps: 1000, Reward: -26.66116646288792, Best reward: 280.1116527237653, Avg reward: -0.6607032373104044\n",
            "Episode: 559, Epsilon: 0.1, Steps: 684, Reward: 196.62947715552593, Best reward: 280.1116527237653, Avg reward: 1.8869988205745931\n",
            "Episode: 560, Epsilon: 0.1, Steps: 1000, Reward: -36.73986013725411, Best reward: 280.1116527237653, Avg reward: 1.5888567378139558\n",
            "Episode: 561, Epsilon: 0.1, Steps: 1000, Reward: 32.61767211595754, Best reward: 280.1116527237653, Avg reward: 1.6572272856089902\n",
            "Episode: 562, Epsilon: 0.1, Steps: 912, Reward: 115.57010699819475, Best reward: 280.1116527237653, Avg reward: 3.2091558081775005\n",
            "Episode: 563, Epsilon: 0.1, Steps: 1000, Reward: -31.856413829704785, Best reward: 280.1116527237653, Avg reward: 3.1275266276932028\n",
            "Episode: 564, Epsilon: 0.1, Steps: 672, Reward: 160.00325911804794, Best reward: 280.1116527237653, Avg reward: 4.915383454002976\n",
            "Episode: 565, Epsilon: 0.1, Steps: 938, Reward: -201.66518877117835, Best reward: 280.1116527237653, Avg reward: 3.083587895689746\n",
            "Episode: 566, Epsilon: 0.1, Steps: 1000, Reward: 38.81283387333452, Best reward: 280.1116527237653, Avg reward: 3.658003735657086\n",
            "Episode: 567, Epsilon: 0.1, Steps: 647, Reward: 161.69469998763765, Best reward: 280.1116527237653, Avg reward: 5.9274403140402185\n",
            "Episode: 568, Epsilon: 0.1, Steps: 482, Reward: -64.63209486045284, Best reward: 280.1116527237653, Avg reward: 5.607146691270779\n",
            "Episode: 569, Epsilon: 0.1, Steps: 586, Reward: 118.849307531694, Best reward: 280.1116527237653, Avg reward: 7.156136220641191\n",
            "Episode: 570, Epsilon: 0.1, Steps: 969, Reward: 169.81383308447073, Best reward: 280.1116527237653, Avg reward: 8.189351131051772\n",
            "Episode: 571, Epsilon: 0.1, Steps: 281, Reward: -33.85671013803221, Best reward: 280.1116527237653, Avg reward: 8.169608121961753\n",
            "Episode: 572, Epsilon: 0.1, Steps: 373, Reward: 243.6914457386252, Best reward: 280.1116527237653, Avg reward: 9.963702441130108\n",
            "Episode: 573, Epsilon: 0.1, Steps: 348, Reward: 290.6562753770295, Best reward: 280.1116527237653, Avg reward: 13.096227592367633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 574, Epsilon: 0.1, Steps: 1000, Reward: 93.13091338890918, Best reward: 290.6562753770295, Avg reward: 14.56623621702886\n",
            "Episode: 575, Epsilon: 0.1, Steps: 620, Reward: 225.13171495354433, Best reward: 290.6562753770295, Avg reward: 17.81101130293755\n",
            "Episode: 576, Epsilon: 0.1, Steps: 494, Reward: 221.45123804308042, Best reward: 290.6562753770295, Avg reward: 20.24739562983949\n",
            "Episode: 577, Epsilon: 0.1, Steps: 861, Reward: 174.659236750344, Best reward: 290.6562753770295, Avg reward: 22.975499041833483\n",
            "Episode: 578, Epsilon: 0.1, Steps: 1000, Reward: 93.46444298418554, Best reward: 290.6562753770295, Avg reward: 24.32562829063306\n",
            "Episode: 579, Epsilon: 0.1, Steps: 466, Reward: 235.1867213879093, Best reward: 290.6562753770295, Avg reward: 27.09205176558432\n",
            "Episode: 580, Epsilon: 0.1, Steps: 551, Reward: 215.12960821956693, Best reward: 290.6562753770295, Avg reward: 29.889635862237473\n",
            "Episode: 581, Epsilon: 0.1, Steps: 896, Reward: 98.35250058374751, Best reward: 290.6562753770295, Avg reward: 30.768122928457224\n",
            "Episode: 582, Epsilon: 0.1, Steps: 1000, Reward: -34.029073757543294, Best reward: 290.6562753770295, Avg reward: 30.63793752133404\n",
            "Episode: 583, Epsilon: 0.1, Steps: 517, Reward: 180.88011359567057, Best reward: 290.6562753770295, Avg reward: 33.31449185643362\n",
            "Episode: 584, Epsilon: 0.1, Steps: 577, Reward: 235.59383533862135, Best reward: 290.6562753770295, Avg reward: 36.364725522578325\n",
            "Episode: 585, Epsilon: 0.1, Steps: 784, Reward: 155.53474727793306, Best reward: 290.6562753770295, Avg reward: 37.82647851896719\n",
            "Episode: 586, Epsilon: 0.1, Steps: 817, Reward: 145.75838415481257, Best reward: 290.6562753770295, Avg reward: 39.211228149825956\n",
            "Episode: 587, Epsilon: 0.1, Steps: 785, Reward: 229.7492664605349, Best reward: 290.6562753770295, Avg reward: 40.96141753271952\n",
            "Episode: 588, Epsilon: 0.1, Steps: 759, Reward: 242.3956985040041, Best reward: 290.6562753770295, Avg reward: 45.3248673205382\n",
            "Episode: 589, Epsilon: 0.1, Steps: 473, Reward: 249.01744947639406, Best reward: 290.6562753770295, Avg reward: 47.11728219055621\n",
            "Episode: 590, Epsilon: 0.1, Steps: 781, Reward: 141.21978512167948, Best reward: 290.6562753770295, Avg reward: 48.80583891293043\n",
            "Episode: 591, Epsilon: 0.1, Steps: 576, Reward: 214.66342159040767, Best reward: 290.6562753770295, Avg reward: 52.39993693038641\n",
            "Episode: 592, Epsilon: 0.1, Steps: 373, Reward: -58.91506155995701, Best reward: 290.6562753770295, Avg reward: 51.258284514921385\n",
            "Episode: 593, Epsilon: 0.1, Steps: 350, Reward: 257.69179600231445, Best reward: 290.6562753770295, Avg reward: 53.91680300552087\n",
            "Episode: 594, Epsilon: 0.1, Steps: 1000, Reward: 104.02947248434081, Best reward: 290.6562753770295, Avg reward: 55.22604709541941\n",
            "Episode: 595, Epsilon: 0.1, Steps: 486, Reward: 246.3958714949203, Best reward: 290.6562753770295, Avg reward: 56.579100298730665\n",
            "Episode: 596, Epsilon: 0.1, Steps: 515, Reward: 265.7511561412306, Best reward: 290.6562753770295, Avg reward: 59.55933990154961\n",
            "Episode: 597, Epsilon: 0.1, Steps: 460, Reward: 208.08772247691718, Best reward: 290.6562753770295, Avg reward: 62.55256432574081\n",
            "Episode: 598, Epsilon: 0.1, Steps: 631, Reward: 199.6125271986969, Best reward: 290.6562753770295, Avg reward: 64.55689312557341\n",
            "Episode: 599, Epsilon: 0.1, Steps: 563, Reward: 152.26090504621214, Best reward: 290.6562753770295, Avg reward: 66.5100502885484\n",
            "Episode: 600, Epsilon: 0.1, Steps: 553, Reward: 277.6239553728657, Best reward: 290.6562753770295, Avg reward: 68.27764424484627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 601, Epsilon: 0.1, Steps: 455, Reward: -67.1310813724539, Best reward: 290.6562753770295, Avg reward: 67.83806214196917\n",
            "Episode: 602, Epsilon: 0.1, Steps: 540, Reward: 151.03818805703494, Best reward: 290.6562753770295, Avg reward: 69.35653627216517\n",
            "Episode: 603, Epsilon: 0.1, Steps: 544, Reward: -205.04720961172578, Best reward: 290.6562753770295, Avg reward: 67.76842409793974\n",
            "Episode: 604, Epsilon: 0.1, Steps: 299, Reward: 250.66198060136563, Best reward: 290.6562753770295, Avg reward: 69.53638766145586\n",
            "Episode: 605, Epsilon: 0.1, Steps: 567, Reward: 247.43751462164784, Best reward: 290.6562753770295, Avg reward: 71.30537103320735\n",
            "Episode: 606, Epsilon: 0.1, Steps: 1000, Reward: 75.02397357984213, Best reward: 290.6562753770295, Avg reward: 72.45548198893017\n",
            "Episode: 607, Epsilon: 0.1, Steps: 412, Reward: 235.34964077628425, Best reward: 290.6562753770295, Avg reward: 73.66258370766252\n",
            "Episode: 608, Epsilon: 0.1, Steps: 530, Reward: 257.6794422871805, Best reward: 290.6562753770295, Avg reward: 73.86206640677621\n",
            "Episode: 609, Epsilon: 0.1, Steps: 629, Reward: 145.5309560249936, Best reward: 290.6562753770295, Avg reward: 75.5612192051002\n",
            "Episode: 610, Epsilon: 0.1, Steps: 307, Reward: 263.0764195991552, Best reward: 290.6562753770295, Avg reward: 76.95383868863924\n",
            "Episode: 611, Epsilon: 0.1, Steps: 499, Reward: 156.55341884097516, Best reward: 290.6562753770295, Avg reward: 77.28817698517233\n",
            "Episode: 612, Epsilon: 0.1, Steps: 721, Reward: 280.21089271546015, Best reward: 290.6562753770295, Avg reward: 81.24393414389405\n",
            "Episode: 613, Epsilon: 0.1, Steps: 625, Reward: 248.65654436955006, Best reward: 290.6562753770295, Avg reward: 83.69064034112411\n",
            "Episode: 614, Epsilon: 0.1, Steps: 600, Reward: 151.52107457539503, Best reward: 290.6562753770295, Avg reward: 85.09807022765386\n",
            "Episode: 615, Epsilon: 0.1, Steps: 613, Reward: 217.22085987653304, Best reward: 290.6562753770295, Avg reward: 88.38764618519977\n",
            "Episode: 616, Epsilon: 0.1, Steps: 441, Reward: 279.7402651775835, Best reward: 290.6562753770295, Avg reward: 90.40500891383459\n",
            "Episode: 617, Epsilon: 0.1, Steps: 292, Reward: 255.2229119660879, Best reward: 290.6562753770295, Avg reward: 94.16347616241951\n",
            "Episode: 618, Epsilon: 0.1, Steps: 715, Reward: 210.10776361129217, Best reward: 290.6562753770295, Avg reward: 99.04070302527568\n",
            "Episode: 619, Epsilon: 0.1, Steps: 1000, Reward: 25.325870956097226, Best reward: 290.6562753770295, Avg reward: 100.23574283294309\n",
            "Episode: 620, Epsilon: 0.1, Steps: 718, Reward: 202.38003159005396, Best reward: 290.6562753770295, Avg reward: 101.09093872254226\n",
            "Episode: 621, Epsilon: 0.1, Steps: 364, Reward: 264.53673836555697, Best reward: 290.6562753770295, Avg reward: 103.88501823007553\n",
            "Episode: 622, Epsilon: 0.1, Steps: 402, Reward: 261.042265131764, Best reward: 290.6562753770295, Avg reward: 109.9651432164759\n",
            "Episode: 623, Epsilon: 0.1, Steps: 804, Reward: 202.95684845031326, Best reward: 290.6562753770295, Avg reward: 110.86698968904824\n",
            "Episode: 624, Epsilon: 0.1, Steps: 1000, Reward: -69.58828975423538, Best reward: 290.6562753770295, Avg reward: 110.63168782660979\n",
            "Episode: 625, Epsilon: 0.1, Steps: 376, Reward: 261.2860371939273, Best reward: 290.6562753770295, Avg reward: 113.56136439598795\n",
            "Episode: 626, Epsilon: 0.1, Steps: 1000, Reward: 85.58529038543203, Best reward: 290.6562753770295, Avg reward: 113.60019992891941\n",
            "Episode: 627, Epsilon: 0.1, Steps: 851, Reward: 104.80652904613203, Best reward: 290.6562753770295, Avg reward: 115.37952697174107\n",
            "Episode: 628, Epsilon: 0.1, Steps: 268, Reward: 249.94698879798912, Best reward: 290.6562753770295, Avg reward: 118.23113658780986\n",
            "Episode: 629, Epsilon: 0.1, Steps: 401, Reward: 233.26394786421622, Best reward: 290.6562753770295, Avg reward: 123.46687009735187\n",
            "Episode: 630, Epsilon: 0.1, Steps: 1000, Reward: 131.10165454365932, Best reward: 290.6562753770295, Avg reward: 123.93601685203286\n",
            "Episode: 631, Epsilon: 0.1, Steps: 437, Reward: 256.1076074943244, Best reward: 290.6562753770295, Avg reward: 127.08778703471559\n",
            "Episode: 632, Epsilon: 0.1, Steps: 1000, Reward: 168.1385251029436, Best reward: 290.6562753770295, Avg reward: 128.72570526531138\n",
            "Episode: 633, Epsilon: 0.1, Steps: 629, Reward: 174.8584131058176, Best reward: 290.6562753770295, Avg reward: 132.4275400099965\n",
            "Episode: 634, Epsilon: 0.1, Steps: 725, Reward: 237.49105814105397, Best reward: 290.6562753770295, Avg reward: 133.68761244989494\n",
            "Episode: 635, Epsilon: 0.1, Steps: 421, Reward: 250.06468945070242, Best reward: 290.6562753770295, Avg reward: 135.7529908015187\n",
            "Episode: 636, Epsilon: 0.1, Steps: 266, Reward: 243.94262723013424, Best reward: 290.6562753770295, Avg reward: 139.12968568934104\n",
            "Episode: 637, Epsilon: 0.1, Steps: 667, Reward: 244.33599618260175, Best reward: 290.6562753770295, Avg reward: 141.2256134963486\n",
            "Episode: 638, Epsilon: 0.1, Steps: 794, Reward: 131.36750193695627, Best reward: 290.6562753770295, Avg reward: 144.9643971696081\n",
            "Episode: 639, Epsilon: 0.1, Steps: 408, Reward: 238.61998052958046, Best reward: 290.6562753770295, Avg reward: 149.03312307614658\n",
            "Episode: 640, Epsilon: 0.1, Steps: 572, Reward: 235.51957082406662, Best reward: 290.6562753770295, Avg reward: 152.4327240068229\n",
            "Episode: 641, Epsilon: 0.1, Steps: 232, Reward: -150.45334874424935, Best reward: 290.6562753770295, Avg reward: 151.16572788685428\n",
            "Episode: 642, Epsilon: 0.1, Steps: 288, Reward: 268.5270842496835, Best reward: 290.6562753770295, Avg reward: 153.034218178621\n",
            "Episode: 643, Epsilon: 0.1, Steps: 263, Reward: 266.98478233432724, Best reward: 290.6562753770295, Avg reward: 155.9730583590721\n",
            "Episode: 644, Epsilon: 0.1, Steps: 742, Reward: 281.0368573245413, Best reward: 290.6562753770295, Avg reward: 159.51432271249843\n",
            "Episode: 645, Epsilon: 0.1, Steps: 362, Reward: 267.0915766037223, Best reward: 290.6562753770295, Avg reward: 163.03447167133868\n",
            "Episode: 646, Epsilon: 0.1, Steps: 956, Reward: 111.5367203357451, Best reward: 290.6562753770295, Avg reward: 162.18601771207182\n",
            "Episode: 647, Epsilon: 0.1, Steps: 442, Reward: 206.09230511074895, Best reward: 290.6562753770295, Avg reward: 162.53184435067445\n",
            "Episode: 648, Epsilon: 0.1, Steps: 351, Reward: 286.63262940502926, Best reward: 290.6562753770295, Avg reward: 163.00138780796868\n",
            "Episode: 649, Epsilon: 0.1, Steps: 609, Reward: 198.07311283195216, Best reward: 290.6562753770295, Avg reward: 162.79538202073365\n",
            "Episode: 650, Epsilon: 0.1, Steps: 449, Reward: 266.6393557847251, Best reward: 290.6562753770295, Avg reward: 163.38267298518767\n",
            "Episode: 651, Epsilon: 0.1, Steps: 501, Reward: 224.6193270571947, Best reward: 290.6562753770295, Avg reward: 164.64616697322984\n",
            "Episode: 652, Epsilon: 0.1, Steps: 513, Reward: 225.41830165657086, Best reward: 290.6562753770295, Avg reward: 164.81312748001096\n",
            "Episode: 653, Epsilon: 0.1, Steps: 629, Reward: 175.83215499682078, Best reward: 290.6562753770295, Avg reward: 165.56420349293248\n",
            "Episode: 654, Epsilon: 0.1, Steps: 515, Reward: 217.9946041578643, Best reward: 290.6562753770295, Avg reward: 168.2578955944708\n",
            "Episode: 655, Epsilon: 0.1, Steps: 838, Reward: 165.2633223984285, Best reward: 290.6562753770295, Avg reward: 167.42391222324565\n",
            "Episode: 656, Epsilon: 0.1, Steps: 782, Reward: 143.34771532316765, Best reward: 290.6562753770295, Avg reward: 166.2344741732768\n",
            "Episode: 657, Epsilon: 0.1, Steps: 513, Reward: 218.05278377643643, Best reward: 290.6562753770295, Avg reward: 165.61388548380347\n",
            "Episode: 658, Epsilon: 0.1, Steps: 151, Reward: -199.50737854925083, Best reward: 290.6562753770295, Avg reward: 163.88542336293983\n",
            "Episode: 659, Epsilon: 0.1, Steps: 273, Reward: 253.551063299899, Best reward: 290.6562753770295, Avg reward: 164.45463922438353\n",
            "Episode: 660, Epsilon: 0.1, Steps: 350, Reward: 296.8337942618309, Best reward: 290.6562753770295, Avg reward: 167.7903757683744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 661, Epsilon: 0.1, Steps: 726, Reward: 217.3651419119579, Best reward: 296.8337942618309, Avg reward: 169.6378504663344\n",
            "Episode: 662, Epsilon: 0.1, Steps: 514, Reward: 231.49053871376776, Best reward: 296.8337942618309, Avg reward: 170.79705478349015\n",
            "Episode: 663, Epsilon: 0.1, Steps: 288, Reward: 247.86779735009833, Best reward: 296.8337942618309, Avg reward: 173.5942968952882\n",
            "Episode: 664, Epsilon: 0.1, Steps: 572, Reward: 237.4916980088972, Best reward: 296.8337942618309, Avg reward: 174.3691812841967\n",
            "Episode: 665, Epsilon: 0.1, Steps: 386, Reward: 228.21189525169822, Best reward: 296.8337942618309, Avg reward: 178.66795212442543\n",
            "Episode: 666, Epsilon: 0.1, Steps: 503, Reward: 238.38281519210707, Best reward: 296.8337942618309, Avg reward: 180.66365193761317\n",
            "Episode: 667, Epsilon: 0.1, Steps: 322, Reward: 286.5737511629003, Best reward: 296.8337942618309, Avg reward: 181.91244244936584\n",
            "Episode: 668, Epsilon: 0.1, Steps: 297, Reward: 229.91699440472908, Best reward: 296.8337942618309, Avg reward: 184.85793334201765\n",
            "Episode: 669, Epsilon: 0.1, Steps: 342, Reward: 231.6220445962209, Best reward: 296.8337942618309, Avg reward: 185.98566071266293\n",
            "Episode: 670, Epsilon: 0.1, Steps: 176, Reward: -218.130810162091, Best reward: 296.8337942618309, Avg reward: 182.10621428019724\n",
            "Episode: 671, Epsilon: 0.1, Steps: 510, Reward: 253.96703874043237, Best reward: 296.8337942618309, Avg reward: 184.98445176898193\n",
            "Episode: 672, Epsilon: 0.1, Steps: 431, Reward: 249.7422072084528, Best reward: 296.8337942618309, Avg reward: 185.04495938368018\n",
            "Episode: 673, Epsilon: 0.1, Steps: 331, Reward: 245.6361593452653, Best reward: 296.8337942618309, Avg reward: 184.5947582233625\n",
            "Episode: 674, Epsilon: 0.1, Steps: 265, Reward: 238.30091715577828, Best reward: 296.8337942618309, Avg reward: 186.04645826103126\n",
            "Episode: 675, Epsilon: 0.1, Steps: 396, Reward: 296.11311161976516, Best reward: 296.8337942618309, Avg reward: 186.75627222769344\n",
            "Episode: 676, Epsilon: 0.1, Steps: 353, Reward: 300.28417064175994, Best reward: 296.8337942618309, Avg reward: 187.54460155368025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "... models saved successfully ...\n",
            "Episode: 677, Epsilon: 0.1, Steps: 335, Reward: 278.9925284426955, Best reward: 300.28417064175994, Avg reward: 188.58793447060378\n",
            "Episode: 678, Epsilon: 0.1, Steps: 277, Reward: 231.35234132939692, Best reward: 300.28417064175994, Avg reward: 189.96681345405588\n",
            "Episode: 679, Epsilon: 0.1, Steps: 373, Reward: 254.69392917148846, Best reward: 300.28417064175994, Avg reward: 190.1618855318917\n",
            "Episode: 680, Epsilon: 0.1, Steps: 328, Reward: 277.66679372412506, Best reward: 300.28417064175994, Avg reward: 190.78725738693728\n",
            "Episode: 681, Epsilon: 0.1, Steps: 478, Reward: 275.2000182844415, Best reward: 300.28417064175994, Avg reward: 192.55573256394422\n",
            "Episode: 682, Epsilon: 0.1, Steps: 558, Reward: 252.00695808673805, Best reward: 300.28417064175994, Avg reward: 195.41609288238703\n",
            "Episode: 683, Epsilon: 0.1, Steps: 407, Reward: 254.247599436908, Best reward: 300.28417064175994, Avg reward: 196.14976774079943\n",
            "Episode: 684, Epsilon: 0.1, Steps: 280, Reward: 262.48393220692765, Best reward: 300.28417064175994, Avg reward: 196.41866870948243\n",
            "Episode: 685, Epsilon: 0.1, Steps: 431, Reward: 249.26859896879046, Best reward: 300.28417064175994, Avg reward: 197.356007226391\n",
            "Episode: 686, Epsilon: 0.1, Steps: 378, Reward: 253.88779442398675, Best reward: 300.28417064175994, Avg reward: 198.43730132908274\n",
            "Episode: 687, Epsilon: 0.1, Steps: 585, Reward: 244.47910731703158, Best reward: 300.28417064175994, Avg reward: 198.5845997376477\n",
            "Episode: 688, Epsilon: 0.1, Steps: 410, Reward: 256.72915530964104, Best reward: 300.28417064175994, Avg reward: 198.72793430570403\n",
            "Episode: 689, Epsilon: 0.1, Steps: 663, Reward: 232.70638262648293, Best reward: 300.28417064175994, Avg reward: 198.56482363720494\n",
            "Episode: 690, Epsilon: 0.1, Steps: 389, Reward: 200.90414151290656, Best reward: 300.28417064175994, Avg reward: 199.16166720111724\n",
            "Episode: 691, Epsilon: 0.1, Steps: 433, Reward: 241.1916976268623, Best reward: 300.28417064175994, Avg reward: 199.42694996148177\n",
            "Episode: 692, Epsilon: 0.1, Steps: 246, Reward: 291.5654921550249, Best reward: 300.28417064175994, Avg reward: 202.93175549863162\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "import signal\n",
        "import time\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "env = make_env(\"LunarLander-v2\", \"videos/\", 50)\n",
        "action_space = [_ for _ in range(env.action_space.n)]\n",
        "record = True\n",
        "print( env.observation_space.shape)\n",
        "\n",
        "trainer_params = {\n",
        "    \"noe\": 1000, \n",
        "    \"max_steps\": 10000,\n",
        "    \"max_eps\": 1,\n",
        "    \"min_eps\": 0.1,\n",
        "    \"eps_decay_rate\": 1e-5,\n",
        "    \"eps\": 1,\n",
        "    \"action_space\": action_space,\n",
        "    \"is_tg\": True,\n",
        "    \"tg_bot_freq_epi\": 10,\n",
        "    \"record\": record,\n",
        "    \"gamma\": 0.99, \n",
        "    \"lr\": 0.001, \n",
        "    \"input_dims\": env.observation_space.shape,\n",
        "    \"mem_size\" : 100000,\n",
        "    \"batch_size\" : 32,\n",
        "    \"replace\" : 500,\n",
        "    \"algo\" : \"dueling_dqn\",\n",
        "    \"env_name\" : \"lunarlander\",\n",
        "    \"n_actions\" : len(action_space),\n",
        "    \"chkpt_dir\": \"tmp/dueling_dqn/\",\n",
        "    \"actions\": action_space,\n",
        "    \"target_score\": 200,\n",
        "    \"video_prefix\": \"dueling_dqn\",\n",
        "    \"checkpoint\": False\n",
        "}\n",
        "\n",
        "    \n",
        "if __name__ == \"__main__\": \n",
        "    \n",
        "    try: \n",
        "        manage_memory()\n",
        "       \n",
        "        trainer = Trainer(env, trainer_params)\n",
        "        episode_rewards, epsilon_history, avg_rewards, best_reward = trainer.train_rl_model()\n",
        "        \n",
        "        with open(\"dueling_dqn_episode_rewards.obj\", \"wb\") as f: \n",
        "            pickle.dump(episode_rewards, f)\n",
        "        \n",
        "        with open(\"dueling_dqn_epsilon_history.obj\", \"wb\") as f: \n",
        "            pickle.dump(epsilon_history, f)\n",
        "        \n",
        "        with open(\"dueling_dqn_avg_rewards.obj\", \"wb\") as f: \n",
        "            pickle.dump(avg_rewards, f)\n",
        "            \n",
        "        plot_learning_curve(episode_rewards, epsilon_history, \"dueling_dqn\")\n",
        "        \n",
        "    except Exception as error: \n",
        "        raise error\n",
        "        \n",
        "   # eval_model(env, \"keras model\", \"videos/\", fps=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NSSgq3Bd1MbR"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import imageio\n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "class Eval: \n",
        "\n",
        "    def __init__(self, env, model_path, action_space, number_of_episode=50, test_video_path=\"test_videos/\"):\n",
        "        self.env = env \n",
        "        self.model = tf.keras.models.load_model(model_path)\n",
        "        self.recorder = RecordVideo('dueling_dqn', test_video_path, 15)\n",
        "        self.number_of_episode = number_of_episode\n",
        "        self.action_space = action_space\n",
        "        \n",
        "    def test(self): \n",
        "        rewards = []\n",
        "        steps = []\n",
        "        for episode in range(self.number_of_episode): \n",
        "            done = False\n",
        "            reward = 0\n",
        "            step = 0\n",
        "            state = env.reset(seed=random.randint(0,500))\n",
        "            if episode % 1 == 0: \n",
        "                img = env.render()\n",
        "                self.recorder.add_image(img) \n",
        "\n",
        "            while not done:\n",
        "                if type(state) == tuple: \n",
        "                    state = state[0]\n",
        "                    \n",
        "                action =  greedy_policy(state, self.model, self.action_space)\n",
        "                state, reward_prob, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated \n",
        "                reward += reward_prob\n",
        "                step += 1 \n",
        "                if episode % 1 == 0:\n",
        "                    img = env.render()\n",
        "                    self.recorder.add_image(img)\n",
        "            \n",
        "            rewards.append(reward)\n",
        "            steps.append(step)\n",
        "            self.recorder.save(1) if episode % 1 == 0 else None\n",
        "        \n",
        "        return rewards, steps                                                                                                                                                        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vzKyDpDA1MbR"
      },
      "outputs": [],
      "source": [
        "env = make_env(\"LunarLander-v2\", \"videos/\", 50)\n",
        "action_space = [_ for _ in range(env.action_space.n)]\n",
        "model_path = \"/content/tmp/dueling_dqn/lunarlander_dueling_dqn_q_value\"\n",
        "\n",
        "eval = Eval(env, model_path, action_space, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGLc60iR1MbS",
        "outputId": "c808a7ca-1e59-4381-e827-a84493aa8706"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ],
      "source": [
        "test_rewards, steps = eval.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SbXIJbE1MbT",
        "outputId": "17e7d8ac-41ba-4e78-e299-f752e043686c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[138.8108153174976,\n",
              " 225.6415642677828,\n",
              " 278.19812668001833,\n",
              " 265.09468033915107,\n",
              " 257.81140068169685,\n",
              " 244.87992382366315,\n",
              " 225.04083347881598,\n",
              " 249.0798702548756,\n",
              " 233.8856473910113,\n",
              " 248.3651946892898]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqcx855vYiUG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
