{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-04T02:38:42.330381Z","iopub.status.busy":"2023-04-04T02:38:42.330021Z","iopub.status.idle":"2023-04-04T02:40:17.100731Z","shell.execute_reply":"2023-04-04T02:40:17.099572Z","shell.execute_reply.started":"2023-04-04T02:38:42.330348Z"},"id":"Nb4_TB9Xkzp3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install gymnasium[atari] --quiet\n","!pip install gymnasium --quiet\n","!pip install -U gymnasium[atari] --quiet\n","!pip install imageio_ffmpeg --quiet\n","!pip install npy_append_array --quiet\n","!pip install pyTelegramBotAPI --quiet\n","!pip install gymnasium[accept-rom-license] --quiet\n","!pip install gymnasium[box2d] --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T02:46:13.093654Z","iopub.status.busy":"2023-04-04T02:46:13.093255Z","iopub.status.idle":"2023-04-04T02:46:13.108885Z","shell.execute_reply":"2023-04-04T02:46:13.107612Z","shell.execute_reply.started":"2023-04-04T02:46:13.093612Z"},"id":"OV8e-jcakzqH","trusted":true},"outputs":[],"source":["import numpy as np \n","\n","class ExperienceReplayBuffer: \n","    def __init__(self, max_memory, input_shape, batch_size, n_actions, cer=False): \n","        self.mem_size = max_memory\n","        self.mem_counter = 0\n","        self.state_memory = np.zeros((self.mem_size, *input_shape),\n","                                     dtype=np.float32)\n","        self.next_state_memory = np.zeros((self.mem_size, *input_shape),\n","                                         dtype=np.float32)\n","\n","        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=np.float32)\n","        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n","        self.batch_size = batch_size\n","        self.cer = cer\n","\n","    def store_experience(self, state, action, reward, next_state, done): \n","        index = self.mem_counter % self.mem_size \n","\n","        self.state_memory[index] = state\n","        self.next_state_memory[index] = next_state\n","        self.reward_memory[index] = reward\n","        self.action_memory[index] = action\n","        self.terminal_memory[index] = done\n","        self.mem_counter += 1\n","\n","    def sample_experience(self, batch_size):\n","        # used to get the last transition\n","        offset = 1 if self.cer else 0\n","\n","        max_mem = min(self.mem_counter, self.mem_size) - offset\n","        batch_index = np.random.choice(max_mem, batch_size - offset, replace=False)\n","\n","        states = self.state_memory[batch_index]\n","        next_states = self.next_state_memory[batch_index]\n","        rewards = self.reward_memory[batch_index]\n","        actions = self.action_memory[batch_index]\n","        terminals = self.terminal_memory[batch_index]\n","\n","        if self.cer: \n","            last_index = self.mem_counter % self.mem_size - 1\n","            last_state = self.state_memory[last_index]\n","            last_action = self.action_memory[last_index]\n","            last_terminal = self.terminal_memory[last_index]\n","            last_next_state = self.next_state_memory[last_index]\n","            last_reward = self.reward_memory[last_index]\n","\n","            # for 2d and 3d use vstack to append, for 1d array use append() to append the data\n","            states = np.vstack((self.state_memory[batch_index], last_state))\n","            next_states = np.vstack((self.next_state_memory[batch_index], last_next_state))\n","\n","            actions = np.append(actions, last_action)\n","            terminals = np.append(terminals, last_terminal)\n","            rewards = np.append(rewards, last_reward)\n","    \n","        return states, actions, rewards, next_states, terminals\n","    \n","    \n","    def is_sufficient(self): \n","        return self.mem_counter > self.batch_size\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZ29uYM2kzqK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:06:26.253365Z","iopub.status.busy":"2023-04-04T03:06:26.252951Z","iopub.status.idle":"2023-04-04T03:06:26.264941Z","shell.execute_reply":"2023-04-04T03:06:26.263028Z","shell.execute_reply.started":"2023-04-04T03:06:26.253331Z"},"id":"rAV6eDXlkzqK","trusted":true},"outputs":[],"source":["import tensorflow as tf \n","from tensorflow.keras.layers import Dense, Conv2D, Input, Lambda, concatenate\n"," \n","class ActorNetwork(tf.keras.Model):\n","    def __init__(self, input_dims, action_bound, action_dims, name):\n","        super(ActorNetwork, self).__init__()\n","        self.fc1 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        self.fc3 = Dense(16, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        \n","        self.out = Dense(action_dims, activation='tanh')\n","\n","    def call(self, x):\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        x = self.out(x)\n","        return x \n","\n","\n","class CriticNetwork(tf.keras.Model):\n","    def __init__(self, input_dims, action_dims, name): \n","        super(CriticNetwork, self).__init__()\n","        self.fc1 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        self.fc2 = Dense(32, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        self.fc3 = Dense(16, activation=\"relu\", kernel_initializer=\"he_uniform\")\n","        self.out = Dense(1, activation='linear')\n","\n","    def call(self, state, action):\n","        x = self.fc1(tf.concat([state, action], axis=1))\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        x = self.out(x)\n","        return x "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmIY8RP0kzqM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:48.902717Z","iopub.status.busy":"2023-04-04T03:09:48.902266Z","iopub.status.idle":"2023-04-04T03:09:48.939727Z","shell.execute_reply":"2023-04-04T03:09:48.937561Z","shell.execute_reply.started":"2023-04-04T03:09:48.902680Z"},"id":"cwt8tDsRkzqO","trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","import tensorflow as tf \n","import tensorflow_probability as tfp\n","import numpy as np\n","\n","class TD3Agent:\n","  \n","    def __init__(self, input_dims, n_actions, gamma, alpha, beta, \n","                                batch_size, mem_size, soft_update, \n","                                tau, min_action, max_action, noise, eval_noise_scale): \n","        self.gamma = gamma \n","        self.explore_noise = noise\n","        self.n_actions = n_actions\n","        self.soft_update = soft_update\n","        self.tau = tau\n","        self.fname = \"models/ddpg/\"\n","        self.min_action = min_action\n","        self.max_action = max_action\n","        self.batch_size = batch_size\n","        self.eval_noise_scale = eval_noise_scale\n","        self.Normal = tfp.distributions.Normal\n","\n","\n","        self.memory = ExperienceReplayBuffer(mem_size, input_dims, batch_size, n_actions, cer=False)\n","        self.actor = ActorNetwork(input_dims[0], max_action, n_actions, \"actor\")\n","        self.target_actor = ActorNetwork(input_dims[0], max_action, n_actions, \"target_actor\")\n","        self.critic_1 = CriticNetwork(input_dims, 1, \"critic_1\")\n","        self.critic_2 = CriticNetwork(input_dims, 1, \"critic_2\")\n","        self.target_critic_1 = CriticNetwork(input_dims, 1, \"target_critic_1\")\n","        self.target_critic_2 = CriticNetwork(input_dims, 1, \"target_critic_2\")\n","\n","        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n","        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n","        self.critic_1.compile(optimizer=Adam(learning_rate=beta))\n","        self.critic_2.compile(optimizer=Adam(learning_rate=beta))\n","        self.target_critic_1.compile(optimizer=Adam(learning_rate=beta))\n","        self.target_critic_2.compile(optimizer=Adam(learning_rate=beta))\n","\n","      #  self.update_target_networks()\n","    \n","    def evaluate(self, state, eval_noise_scale):\n","        \"\"\" \n","        Generating action by the target actor, to pass it into the target critic\n","        \"\"\"\n","        action = self.target_actor(state)\n","        action = self.max_action * action\n","\n","        # add noise\n","        normal = self.Normal(0, 1)\n","        eval_noise_clip = 2 * eval_noise_scale\n","        noise = normal.sample(action.shape) * eval_noise_scale\n","        noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)\n","        action = action + noise\n","        return action\n","        \n","    def get_action(self, state, greedy=False): \n","        state = tf.convert_to_tensor([state], dtype=tf.float32)\n","        mu = self.actor(state)[0]\n","        mu_prime = mu\n","        if not greedy: \n","            mu_prime = mu_prime + np.random.normal(scale=self.explore_noise)\n","        mu_prime = tf.clip_by_value(mu_prime, self.min_action, self.max_action)\n","        \n","        return mu_prime\n","\n","    def sample_action(self):\n","        \"\"\" generate random actions for exploration \"\"\"\n","        mu = np.random.normal(scale=self.explore_noise, size=(self.n_actions,))\n","        mu_prime = mu + np.random.normal(scale=self.explore_noise)\n","        mu_prime = tf.clip_by_value(mu_prime, self.min_action, self.max_action)\n","\n","        return mu_prime\n","\n","    def store_experience(self, state, action, reward, state_, done):\n","        self.memory.store_experience(state, action, reward, state_, done)\n","\n","    def sample_experience(self):\n","        state, action, reward, new_state, done = \\\n","                                  self.memory.sample_experience(self.batch_size)\n","        states = tf.convert_to_tensor(state)\n","        rewards = tf.convert_to_tensor(reward)\n","        dones = tf.convert_to_tensor(done)\n","        actions = tf.convert_to_tensor(action)\n","        states_ = tf.convert_to_tensor(new_state)\n","        return states, actions, rewards, states_, dones\n"," \n","    def save_models(self):\n","        self.actor.save(self.fname + \"td3_actor_network\")\n","        self.target_actor.save(self.fname + \"td3_target_actor_network\")\n","        self.critic_1.save(self.fname  + \"td3_critic_1_network\")\n","        self.target_critic_1.save(self.fname  + \"td3_target_critic_1_network\")\n","        self.critic_2.save(self.fname  + \"td3_critic_2_network\")\n","        self.target_critic_2.save(self.fname  + \"td3_target_critic_2_network\")\n","        print(\"[+] Saving the models\") \n","\n","    def load_models(self):\n","        self.actor = tf.keras.models.load_model(self.fname + \"_\" + \"td3_actor_network\") \n","        self.target_actor = tf.keras.models.load_model(self.fname + \"_\" + \"td3_target_actor_network\") \n","        self.critic_1 = tf.keras.models.load_model(self.fname + \"_\" + \"td3_critic_1_network\") \n","        self.target_critic_1 = tf.keras.models.load_model(self.fname + \"_\" + \"td3_target_critic_1_network\") \n","        self.critic_2 = tf.keras.models.load_model(self.fname + \"_\" + \"td3_critic_2_network\") \n","        self.target_critic_2 = tf.keras.models.load_model(self.fname + \"_\" + \"td3_target_critic_2_network\") \n","        print(\"[+] Loading the models\")\n","  \n","    def learn(self, update_actor=False): \n","        if not self.memory.is_sufficient():\n","            return\n","        states, actions, rewards, next_states, dones = self.sample_experience()\n","       \n","        target_actions = self.target_actor(next_states)\n","        target_actions = target_actions + tf.clip_by_value(np.random.normal(scale=0.2), -0.5, 0.5)\n","        target_actions = tf.clip_by_value(target_actions, self.min_action,\n","                                                                  self.max_action)\n","        one_step_lookahead_vals_1 = self.target_critic_1(next_states, target_actions)\n","        one_step_lookahead_vals_2 = self.target_critic_2(next_states, target_actions)\n","        one_step_lookahead_vals_1 = tf.squeeze(one_step_lookahead_vals_1, 1)\n","        one_step_lookahead_vals_2 = tf.squeeze(one_step_lookahead_vals_2, 1)\n","        one_step_lookahead_vals = tf.math.minimum(one_step_lookahead_vals_1, one_step_lookahead_vals_2)\n","        \n","        with tf.GradientTape() as tape:\n","            q_vals_1 = self.critic_1(states, actions)\n","            q_vals_1 = tf.squeeze(q_vals_1, 1)\n","            target_q_vals = rewards + self.gamma * one_step_lookahead_vals * ([1 - int(d) for d in dones])\n","            critic_1_loss = self.critic_loss(q_vals_1, target_q_vals)\n","      \n","        critic_1_params = self.critic_1.trainable_variables\n","        critic_1_grads = tape.gradient(critic_1_loss, critic_1_params)\n","        self.critic_1.optimizer.apply_gradients(zip(critic_1_grads, critic_1_params))\n","        \n","        with tf.GradientTape() as tape:\n","            q_vals_2 = self.critic_2(states, actions)\n","            q_vals_2 = tf.squeeze(q_vals_2, 1)\n","            target_q_vals = rewards + self.gamma * one_step_lookahead_vals * ([1 - int(d) for d in dones])\n","            critic_2_loss = self.critic_loss(q_vals_2, target_q_vals)\n","      \n","        critic_2_params = self.critic_2.trainable_variables\n","        critic_2_grads = tape.gradient(critic_2_loss, critic_2_params)\n","        self.critic_2.optimizer.apply_gradients(zip(critic_2_grads, critic_2_params))\n","\n","\n","        if not update_actor: \n","            return \n","        \n","        with tf.GradientTape() as tape:\n","            pred_actions = self.actor(states)\n","            q_vals = self.critic_1(states, pred_actions)\n","            actor_loss = self.actor_loss(q_vals)\n","\n","        actor_params = self.actor.trainable_variables\n","        actor_grads = tape.gradient(actor_loss, actor_params)\n","        self.actor.optimizer.apply_gradients(zip(actor_grads, actor_params))\n","        self.update_target_networks()\n","    \n","    def critic_loss(self, q_vals, target_q_vals): \n","        loss = tf.keras.losses.MSE(q_vals, target_q_vals)\n","        return loss\n","    \n","    def actor_loss(self, q_vals):\n","        return -tf.math.reduce_mean(q_vals)\n","\n","    def update_target_networks(self):\n","        actor_weights = self.actor.get_weights()\n","        t_actor_weights = self.target_actor.get_weights()\n","        critic_1_weights = self.critic_1.get_weights()\n","        t_critic_1_weights = self.target_critic_1.get_weights()\n","        critic_2_weights = self.critic_2.get_weights()\n","        t_critic_2_weights = self.target_critic_2.get_weights()\n","        if self.soft_update: \n","            for i in range(len(actor_weights)):\n","                t_actor_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * t_actor_weights[i]\n","\n","            for i in range(len(critic_1_weights)):\n","                t_critic_1_weights[i] = self.tau * critic_1_weights[i] + (1 - self.tau) * t_critic_1_weights[i]\n","\n","            for i in range(len(critic_2_weights)):\n","                t_critic_2_weights[i] = self.tau * critic_2_weights[i] + (1 - self.tau) * t_critic_2_weights[i]\n","\n","        self.target_actor.set_weights(t_actor_weights)\n","        self.target_critic_1.set_weights(t_critic_1_weights)\n","        self.target_critic_2.set_weights(t_critic_2_weights)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTd9476DkzqV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:49.247536Z","iopub.status.busy":"2023-04-04T03:09:49.247122Z","iopub.status.idle":"2023-04-04T03:09:49.256889Z","shell.execute_reply":"2023-04-04T03:09:49.255514Z","shell.execute_reply.started":"2023-04-04T03:09:49.247474Z"},"id":"f4wBkd2BkzqW","trusted":true},"outputs":[],"source":["import time\n","from telebot import TeleBot\n","import datetime\n","import telebot\n","\n","token = \"6238487424:AAG0jRhvbiVa90qUcf2fAirQr_-quPMs7cU\"\n","chat_id = \"1055055706\"\n","bot = TeleBot(token=token) \n","\n","def telegram_send(message, bot):\n","    chat_id = \"1055055706\"\n","    bot.send_message(chat_id=chat_id, text=message)\n","\n","def welcome_msg(multi_step, double_dqn, dueling):\n","    st = 'Hi! Starting learning with DQN Multi-step = %d, Double DQN = %r, Dueling DQN = %r' % (multi_step, double_dqn, dueling)\n","    telegram_send(st, bot)\n","    \n","def info_msg(episode, max_episode, reward, best_score, loss): \n","    st = f\"Current Episode: {episode}, Current Reward: {reward}, Max Episode: {max_episode}, Best Score: {best_score}, loss: {loss}\"\n","    telegram_send(st, bot)\n","\n","def end_msg(learning_time):\n","    st = 'Finished! Learning time: ' + str(datetime.timedelta(seconds=int(learning_time)))\n","    telegram_send(st, bot)\n","    print(st)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMVFBvd4kzqX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:49.582729Z","iopub.status.busy":"2023-04-04T03:09:49.582158Z","iopub.status.idle":"2023-04-04T03:09:49.596335Z","shell.execute_reply":"2023-04-04T03:09:49.594792Z","shell.execute_reply.started":"2023-04-04T03:09:49.582695Z"},"id":"UiUa-abukzqY","trusted":true},"outputs":[],"source":["import collections\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gymnasium as gym\n","import tensorflow as tf\n","from gymnasium.wrappers import *\n","\n","\n","def manage_memory():\n","    gpus = tf.config.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","        except RuntimeError as e:\n","            print(e)\n","\n","\n","def plot_learning_curve(scores, epsilons, filename, lines=None):\n","    x = [_ for _ in range(len(scores))]\n","    fig=plt.figure()\n","    ax=fig.add_subplot(111, label=\"1\")\n","    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n","\n","    ax.plot(x, epsilons, color=\"C0\")\n","    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n","    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n","    ax.tick_params(axis='x', colors=\"C0\")\n","    ax.tick_params(axis='y', colors=\"C0\")\n","\n","    N = len(scores)\n","    running_avg = np.empty(N)\n","    for t in range(N):\n","\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n","\n","    ax2.scatter(x, running_avg, color=\"C1\")\n","    ax2.axes.get_xaxis().set_visible(False)\n","    ax2.yaxis.tick_right()\n","    ax2.set_ylabel('Score', color=\"C1\")\n","    ax2.yaxis.set_label_position('right')\n","    ax2.tick_params(axis='y', colors=\"C1\")\n","\n","    if lines is not None:\n","        for line in lines:\n","            plt.axvline(x=line)\n","\n","    plt.savefig(filename)\n","\n","\n","def make_env(env_name, video_file_name, episode_freq_fo_video): \n","    env = gym.make(env_name, render_mode=\"rgb_array\")\n","    \n","    if len(env.observation_space.shape) >= 3: \n","        #env = AtariPreprocessing(env, 10, 4, 84, False, True)\n","        env = ResizeObservation(env, 84)\n","        env = GrayScaleObservation(env, keep_dim=False)\n","        env = FrameStack(env, 4, lz4_compress=False)\n","        env = NormalizeObservation(env)\n","\n","    return env"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:49.757360Z","iopub.status.busy":"2023-04-04T03:09:49.757005Z","iopub.status.idle":"2023-04-04T03:09:49.763808Z","shell.execute_reply":"2023-04-04T03:09:49.762687Z","shell.execute_reply.started":"2023-04-04T03:09:49.757330Z"},"id":"5-AbLVMjkzqa","trusted":true},"outputs":[],"source":["class Writer:\n","    def __init__(self, fname): \n","        self.fname = fname \n","\n","    def write_to_file(self, content): \n","        with open(self.fname, \"a\") as file: \n","            file.write(content + \"\\n\")\n","\n","    def read_file(self, fname):\n","        with open(fname, \"r\") as file: \n","            return file.read()\n","            "]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:49.927448Z","iopub.status.busy":"2023-04-04T03:09:49.927054Z","iopub.status.idle":"2023-04-04T03:09:49.936107Z","shell.execute_reply":"2023-04-04T03:09:49.934774Z","shell.execute_reply.started":"2023-04-04T03:09:49.927415Z"},"id":"DGl6XdHYkzqa","trusted":true},"outputs":[],"source":["import numpy as np \n","import imageio\n","\n","\n","class RecordVideo: \n","    \n","    def __init__(self, prefix_fname,  out_directory=\"videos/\", fps=10): \n","        self.prefix_fname = prefix_fname\n","        self.out_directory = out_directory\n","        self.fps = fps\n","        self.images = []\n","        \n","    def add_image(self, image): \n","        self.images.append(image)\n","    \n","    def save(self, episode_no): \n","        name = self.out_directory + self.prefix_fname + \"_\" + str(episode_no) + \".mp4\"\n","        imageio.mimsave(name, [np.array(img) for i, img in enumerate(self.images)], fps=self.fps)\n","        self.images = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N779hhrAkzqb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T03:09:50.309307Z","iopub.status.busy":"2023-04-04T03:09:50.308948Z","iopub.status.idle":"2023-04-04T03:09:50.328125Z","shell.execute_reply":"2023-04-04T03:09:50.326071Z","shell.execute_reply.started":"2023-04-04T03:09:50.309274Z"},"id":"chnNSwb6kzqc","trusted":true},"outputs":[],"source":["from npy_append_array import NpyAppendArray\n","import numpy as np\n","\n","class Trainer:   \n","    def __init__(self, env, gamma, alpha, beta, batch_size, \n","                                     tau, sot_update, noe, max_steps, \n","                                     is_tg, tg_bot_freq_epi, record, \n","                                     mem_size, noise, explore_steps=1000, \n","                                     policy_network_update_freq=3, eval_noise_scale=0.5): \n","       \n","        self.env = env \n","        self.target_score = 280\n","        self.explore_steps = explore_steps\n","        self.policy_network_update_freq = policy_network_update_freq\n","        self.noe = noe\n","        self.max_steps = max_steps\n","        self.is_tg = is_tg\n","        self.tg_bot_freq_epi = tg_bot_freq_epi\n","        self.record = record \n","        self.writer = Writer(\"model_training_results.txt\")\n","        self.recorder = RecordVideo(\"ddpg\", \"videos/\", 20)\n","        self.agent = TD3Agent(env.observation_space.shape, \n","                               env.action_space.shape[0], gamma, alpha,\n","                               beta, batch_size, mem_size,\n","                               soft_update, tau, env.action_space.low[0],\n","                               env.action_space.high[0], noise, eval_noise_scale\n","                            )\n","    def train_rl_model(self): \n","        avg_rewards = []\n","        best_reward = float(\"-inf\")\n","        steps_idx = 0\n","        episode_rewards = []\n","\n","        for episode in range(self.noe): \n","            n_steps = 0          \n","            state = self.env.reset()\n","            reward = 0 \n","\n","            if record and episode % 50 == 0:\n","                img = self.env.render()\n","                self.recorder.add_image(img)\n","\n","            for step in range(self.max_steps): \n","\n","                if type(state) == tuple: \n","                    state = state[0]\n","\n","                if steps_idx > self.explore_steps:\n","                    action = self.agent.get_action(state, greedy=False)\n","\n","                else: \n","                    action = self.agent.sample_action()\n","\n","                next_info = self.env.step(action)\n","                next_state, reward_prob, terminated, truncated, _ = next_info\n","                done = truncated or terminated\n","                reward += reward_prob\n","\n","                self.agent.store_experience(state, action, reward_prob, next_state, done)\n","\n","                # updating agent for every n steps\n","                update_agent = False \n","                if step % self.policy_network_update_freq == 0:\n","                    update_agent = True \n","                self.agent.learn(update_agent)\n","\n","                # record\n","                if record and episode % 50 == 0:\n","                    img = self.env.render()\n","                    self.recorder.add_image(img)\n","\n","                # next state\n","                state = next_state\n","                n_steps += 1    \n","                steps_idx += 1            \n","                if done: \n","                    break\n","            \n","            episode_rewards.append(reward)\n","            avg_reward = np.mean(episode_rewards[-100:])\n","            avg_rewards.append(avg_reward)\n","\n","            result = f\"Episode: {episode}, Steps: {n_steps}, Reward: {reward}, Best reward: {best_reward}, Avg reward: {avg_reward}\"\n","            self.writer.write_to_file(result)\n","            print(result)\n","\n","            # Recording.\n","            if record and episode % 50 == 0:\n","                self.recorder.save(episode)\n","            \n","            # Saving Best Model\n","            if reward > best_reward: \n","                best_reward = reward\n","                self.agent.save_models()\n","                \n","            # Telegram bot\n","            if self.is_tg and episode % self.tg_bot_freq_epi == 0: \n","                info_msg(episode+1, self.noe, reward, best_reward, \"d\")\n","                \n","            # Eatly Stopping\n","            if episode > 100 and np.mean(episode_rewards[-50:]) >= self.target_score: \n","                break\n","                \n","                \n","        return episode_rewards, avg_rewards, best_reward\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1e9cBjAkzqe"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-04T03:09:50.607454Z","iopub.status.busy":"2023-04-04T03:09:50.607084Z","iopub.status.idle":"2023-04-04T09:53:54.654418Z","shell.execute_reply":"2023-04-04T09:53:54.652165Z","shell.execute_reply.started":"2023-04-04T03:09:50.607422Z"},"id":"xAyEbgDIkzqf","outputId":"e7d43afb-032b-49f6-ea0c-5b76087a7aac","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Box(-1.0, 1.0, (2,), float32)\n","Episode: 0, Steps: 121, Reward: -85.7920611581755, Best reward: -inf, Avg reward: -85.7920611581755\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5ec9a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["[+] Saving the models\n","Episode: 1, Steps: 95, Reward: -201.9269544407556, Best reward: -85.7920611581755, Avg reward: -143.85950779946555\n","Episode: 2, Steps: 106, Reward: -303.7181263031381, Best reward: -85.7920611581755, Avg reward: -197.14571396735641\n","Episode: 3, Steps: 64, Reward: -36.88672676612599, Best reward: -85.7920611581755, Avg reward: -157.0809671670488\n","[+] Saving the models\n","Episode: 4, Steps: 98, Reward: -293.1118251026838, Best reward: -36.88672676612599, Avg reward: -184.2871387541758\n","Episode: 5, Steps: 138, Reward: -265.6360749308625, Best reward: -36.88672676612599, Avg reward: -197.84529478362356\n","Episode: 6, Steps: 113, Reward: -105.69592777424819, Best reward: -36.88672676612599, Avg reward: -184.68109949656994\n","Episode: 7, Steps: 133, Reward: -207.1898456422914, Best reward: -36.88672676612599, Avg reward: -187.49469276478516\n","Episode: 8, Steps: 96, Reward: -261.0154022716815, Best reward: -36.88672676612599, Avg reward: -195.66366048777365\n","Episode: 9, Steps: 108, Reward: -9.4680663266044, Best reward: -36.88672676612599, Avg reward: -177.0441010716567\n","[+] Saving the models\n","Episode: 10, Steps: 138, Reward: -1324.5369494163544, Best reward: -9.4680663266044, Avg reward: -281.36163273935654\n","Episode: 11, Steps: 89, Reward: -869.98227252762, Best reward: -9.4680663266044, Avg reward: -330.4133527217118\n","Episode: 12, Steps: 119, Reward: -1287.3028964053237, Best reward: -9.4680663266044, Avg reward: -404.0202406973743\n","Episode: 13, Steps: 86, Reward: -790.2732526678835, Best reward: -9.4680663266044, Avg reward: -431.60974155241064\n","Episode: 14, Steps: 136, Reward: -1434.4502142029419, Best reward: -9.4680663266044, Avg reward: -498.46577306244603\n","Episode: 15, Steps: 115, Reward: -712.6520997055106, Best reward: -9.4680663266044, Avg reward: -511.85241847763757\n","Episode: 16, Steps: 412, Reward: -324.5861612272703, Best reward: -9.4680663266044, Avg reward: -500.83675628643954\n","Episode: 17, Steps: 153, Reward: -436.3813750100079, Best reward: -9.4680663266044, Avg reward: -497.25590177108216\n","Episode: 18, Steps: 62, Reward: -84.69847145202972, Best reward: -9.4680663266044, Avg reward: -475.54235280692154\n","Episode: 19, Steps: 104, Reward: -59.50263807970559, Best reward: -9.4680663266044, Avg reward: -454.74036707056075\n","Episode: 20, Steps: 136, Reward: -444.5945905495913, Best reward: -9.4680663266044, Avg reward: -454.2572348552765\n","Episode: 21, Steps: 91, Reward: -104.63261829848761, Best reward: -9.4680663266044, Avg reward: -438.3652068299679\n","Episode: 22, Steps: 117, Reward: -249.88133816850868, Best reward: -9.4680663266044, Avg reward: -430.17025601860007\n","Episode: 23, Steps: 210, Reward: -131.6556313256044, Best reward: -9.4680663266044, Avg reward: -417.732146656392\n","Episode: 24, Steps: 110, Reward: -177.880012394985, Best reward: -9.4680663266044, Avg reward: -408.1380612859357\n","Episode: 25, Steps: 139, Reward: -172.16374637889498, Best reward: -9.4680663266044, Avg reward: -399.06212609720336\n","Episode: 26, Steps: 74, Reward: -120.54748841519624, Best reward: -9.4680663266044, Avg reward: -388.7467691460179\n","Episode: 27, Steps: 92, Reward: -96.26007390003375, Best reward: -9.4680663266044, Avg reward: -378.3008157443756\n","Episode: 28, Steps: 76, Reward: -138.02691675045097, Best reward: -9.4680663266044, Avg reward: -370.0155088825161\n","Episode: 29, Steps: 75, Reward: -111.48046440119884, Best reward: -9.4680663266044, Avg reward: -361.3976740664722\n","Episode: 30, Steps: 68, Reward: -110.14690838617908, Best reward: -9.4680663266044, Avg reward: -353.2928106574305\n","Episode: 31, Steps: 104, Reward: -55.929579766327464, Best reward: -9.4680663266044, Avg reward: -344.00020969208356\n","Episode: 32, Steps: 79, Reward: -120.44669662512646, Best reward: -9.4680663266044, Avg reward: -337.22586081126667\n","Episode: 33, Steps: 94, Reward: -113.03004600431069, Best reward: -9.4680663266044, Avg reward: -330.6318662581209\n","Episode: 34, Steps: 87, Reward: -101.90432841914426, Best reward: -9.4680663266044, Avg reward: -324.09679374843586\n","Episode: 35, Steps: 76, Reward: -56.6385082455558, Best reward: -9.4680663266044, Avg reward: -316.66739692891144\n","Episode: 36, Steps: 139, Reward: 43.94670130959105, Best reward: -9.4680663266044, Avg reward: -306.92106994949245\n","[+] Saving the models\n","Episode: 37, Steps: 96, Reward: -127.46564398671256, Best reward: 43.94670130959105, Avg reward: -302.1985587399456\n","Episode: 38, Steps: 84, Reward: -88.30936334460378, Best reward: 43.94670130959105, Avg reward: -296.7142203964753\n","Episode: 39, Steps: 104, Reward: -87.39258775112377, Best reward: 43.94670130959105, Avg reward: -291.4811795803415\n","Episode: 40, Steps: 139, Reward: -34.97149470958328, Best reward: 43.94670130959105, Avg reward: -285.22484580300596\n","Episode: 41, Steps: 88, Reward: -43.14972245304385, Best reward: 43.94670130959105, Avg reward: -279.4611523899116\n","Episode: 42, Steps: 66, Reward: -57.04958048964497, Best reward: 43.94670130959105, Avg reward: -274.2887902526961\n","Episode: 43, Steps: 77, Reward: -71.03083892650305, Best reward: 43.94670130959105, Avg reward: -269.669291358919\n","Episode: 44, Steps: 93, Reward: -59.800323574994785, Best reward: 43.94670130959105, Avg reward: -265.00553651927623\n","Episode: 45, Steps: 77, Reward: -62.313946537516124, Best reward: 43.94670130959105, Avg reward: -260.5991976066292\n","Episode: 46, Steps: 112, Reward: 16.14696042473983, Best reward: 43.94670130959105, Avg reward: -254.71098147830224\n","Episode: 47, Steps: 144, Reward: 40.28727822655887, Best reward: 43.94670130959105, Avg reward: -248.56518440111765\n","Episode: 48, Steps: 504, Reward: -196.2901305536967, Best reward: 43.94670130959105, Avg reward: -247.49834656749678\n","Episode: 49, Steps: 891, Reward: 150.30938321492698, Best reward: 43.94670130959105, Avg reward: -239.54219197184833\n","[+] Saving the models\n","Episode: 50, Steps: 156, Reward: -16.88417354334814, Best reward: 150.30938321492698, Avg reward: -235.1763484732503\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5dbaa00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 51, Steps: 109, Reward: 48.29906642859308, Best reward: 150.30938321492698, Avg reward: -229.7248981866764\n","Episode: 52, Steps: 1000, Reward: 132.26582581827552, Best reward: 150.30938321492698, Avg reward: -222.89488452620557\n","Episode: 53, Steps: 163, Reward: -15.978210661298164, Best reward: 150.30938321492698, Avg reward: -219.06309426944804\n","Episode: 54, Steps: 158, Reward: 14.250628285281607, Best reward: 150.30938321492698, Avg reward: -214.8210265866348\n","Episode: 55, Steps: 176, Reward: 34.320677523628945, Best reward: 150.30938321492698, Avg reward: -210.37206758466579\n","Episode: 56, Steps: 119, Reward: 21.39325185092818, Best reward: 150.30938321492698, Avg reward: -206.3060093489536\n","Episode: 57, Steps: 169, Reward: 13.01498309829033, Best reward: 150.30938321492698, Avg reward: -202.52461292744943\n","Episode: 58, Steps: 163, Reward: 10.965804426065361, Best reward: 150.30938321492698, Avg reward: -198.90613127738985\n","Episode: 59, Steps: 125, Reward: 40.974176038789096, Best reward: 150.30938321492698, Avg reward: -194.90812615545352\n","Episode: 60, Steps: 179, Reward: 18.353844109047543, Best reward: 150.30938321492698, Avg reward: -191.41202828226497\n","Episode: 61, Steps: 129, Reward: 50.2134847750481, Best reward: 150.30938321492698, Avg reward: -187.5148425877922\n","Episode: 62, Steps: 143, Reward: -48.81209354943233, Best reward: 150.30938321492698, Avg reward: -185.31321165067536\n","Episode: 63, Steps: 183, Reward: -25.652623206916317, Best reward: 150.30938321492698, Avg reward: -182.81851495624164\n","Episode: 64, Steps: 180, Reward: -21.908858700505746, Best reward: 150.30938321492698, Avg reward: -180.34298178307648\n","Episode: 65, Steps: 111, Reward: 37.671604684339684, Best reward: 150.30938321492698, Avg reward: -177.03973047296412\n","Episode: 66, Steps: 210, Reward: -42.98990121944509, Best reward: 150.30938321492698, Avg reward: -175.03898675276236\n","Episode: 67, Steps: 171, Reward: -16.825152749580297, Best reward: 150.30938321492698, Avg reward: -172.71231272330377\n","Episode: 68, Steps: 1000, Reward: 107.46796133253936, Best reward: 150.30938321492698, Avg reward: -168.65172904133505\n","Episode: 69, Steps: 136, Reward: 1.9526175033760325, Best reward: 150.30938321492698, Avg reward: -166.21452409069633\n","Episode: 70, Steps: 328, Reward: -23.7366958440533, Best reward: 150.30938321492698, Avg reward: -164.2077941153915\n","Episode: 71, Steps: 207, Reward: -1.622115521182593, Best reward: 150.30938321492698, Avg reward: -161.9496596904719\n","Episode: 72, Steps: 166, Reward: 56.32390312510671, Best reward: 150.30938321492698, Avg reward: -158.95961088477904\n","Episode: 73, Steps: 220, Reward: 12.699870343163028, Best reward: 150.30938321492698, Avg reward: -156.63988816548252\n","Episode: 74, Steps: 184, Reward: 16.630590275433946, Best reward: 150.30938321492698, Avg reward: -154.32961511960363\n","Episode: 75, Steps: 188, Reward: 63.0446069055285, Best reward: 150.30938321492698, Avg reward: -151.469427987694\n","Episode: 76, Steps: 1000, Reward: 31.70581952851581, Best reward: 150.30938321492698, Avg reward: -149.09052866930165\n","Episode: 77, Steps: 137, Reward: 18.52487588810682, Best reward: 150.30938321492698, Avg reward: -146.94161322625797\n","Episode: 78, Steps: 281, Reward: 26.11456860529303, Best reward: 150.30938321492698, Avg reward: -144.75102864611173\n","Episode: 79, Steps: 243, Reward: -5.545420082809798, Best reward: 150.30938321492698, Avg reward: -143.0109585390705\n","Episode: 80, Steps: 207, Reward: 37.188583431739374, Best reward: 150.30938321492698, Avg reward: -140.78627283572715\n","Episode: 81, Steps: 192, Reward: -10.61395451291098, Best reward: 150.30938321492698, Avg reward: -139.19880553910744\n","Episode: 82, Steps: 204, Reward: 4.48172810352105, Best reward: 150.30938321492698, Avg reward: -137.46771477232878\n","Episode: 83, Steps: 230, Reward: 25.685410474223545, Best reward: 150.30938321492698, Avg reward: -135.52541566225077\n","Episode: 84, Steps: 248, Reward: 22.319142939248536, Best reward: 150.30938321492698, Avg reward: -133.66842085517433\n","Episode: 85, Steps: 310, Reward: -8.848245121030146, Best reward: 150.30938321492698, Avg reward: -132.2170234629168\n","Episode: 86, Steps: 192, Reward: 16.07605694073156, Best reward: 150.30938321492698, Avg reward: -130.51250529735765\n","Episode: 87, Steps: 189, Reward: 41.78168299768916, Best reward: 150.30938321492698, Avg reward: -128.55461679400486\n","Episode: 88, Steps: 273, Reward: -18.37007311183328, Best reward: 150.30938321492698, Avg reward: -127.31658821330629\n","Episode: 89, Steps: 230, Reward: -27.187634620025833, Best reward: 150.30938321492698, Avg reward: -126.20404428449207\n","Episode: 90, Steps: 230, Reward: -29.942499139654146, Best reward: 150.30938321492698, Avg reward: -125.14622510707626\n","Episode: 91, Steps: 207, Reward: -11.462214749974919, Best reward: 150.30938321492698, Avg reward: -123.91052934232515\n","Episode: 92, Steps: 245, Reward: -44.90814374298384, Best reward: 150.30938321492698, Avg reward: -123.06104132512793\n","Episode: 93, Steps: 172, Reward: 37.2558289632027, Best reward: 150.30938321492698, Avg reward: -121.3555427050393\n","Episode: 94, Steps: 241, Reward: -43.547151427193015, Best reward: 150.30938321492698, Avg reward: -120.53650700737775\n","Episode: 95, Steps: 230, Reward: -64.09336476998516, Best reward: 150.30938321492698, Avg reward: -119.9485576090716\n","Episode: 96, Steps: 245, Reward: -72.51881623817735, Best reward: 150.30938321492698, Avg reward: -119.45959120318611\n","Episode: 97, Steps: 214, Reward: -52.67471220686452, Best reward: 150.30938321492698, Avg reward: -118.77811284608077\n","Episode: 98, Steps: 190, Reward: -27.420416492474047, Best reward: 150.30938321492698, Avg reward: -117.85530783240799\n","Episode: 99, Steps: 208, Reward: 24.46483869286338, Best reward: 150.30938321492698, Avg reward: -116.43210636715527\n","Episode: 100, Steps: 348, Reward: -233.9545624345404, Best reward: 150.30938321492698, Avg reward: -117.91373137991891\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x608fa00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 101, Steps: 181, Reward: 27.706508011557233, Best reward: 150.30938321492698, Avg reward: -115.61739675539579\n","Episode: 102, Steps: 290, Reward: -193.29250038773637, Best reward: 150.30938321492698, Avg reward: -114.51314049624177\n","Episode: 103, Steps: 331, Reward: -231.53539602498776, Best reward: 150.30938321492698, Avg reward: -116.45962718883042\n","Episode: 104, Steps: 223, Reward: -45.57045078451151, Best reward: 150.30938321492698, Avg reward: -113.98421344564868\n","Episode: 105, Steps: 199, Reward: 21.349024826393318, Best reward: 150.30938321492698, Avg reward: -111.1143624480761\n","Episode: 106, Steps: 205, Reward: 16.959141947114034, Best reward: 150.30938321492698, Avg reward: -109.88781175086247\n","Episode: 107, Steps: 262, Reward: -42.214880533004724, Best reward: 150.30938321492698, Avg reward: -108.2380620997696\n","Episode: 108, Steps: 266, Reward: -68.8634981398063, Best reward: 150.30938321492698, Avg reward: -106.31654305845086\n","Episode: 109, Steps: 1000, Reward: 65.07042863229933, Best reward: 150.30938321492698, Avg reward: -105.57115810886182\n","Episode: 110, Steps: 167, Reward: 33.512681805093194, Best reward: 150.30938321492698, Avg reward: -91.99066179664734\n","Episode: 111, Steps: 183, Reward: 31.09648104706477, Best reward: 150.30938321492698, Avg reward: -82.97987426090047\n","Episode: 112, Steps: 322, Reward: -80.92060411121783, Best reward: 150.30938321492698, Avg reward: -70.91605133795943\n","Episode: 113, Steps: 152, Reward: 2.2723697879436173, Best reward: 150.30938321492698, Avg reward: -62.99059511340115\n","Episode: 114, Steps: 1000, Reward: 111.78074349437398, Best reward: 150.30938321492698, Avg reward: -47.528285536427994\n","Episode: 115, Steps: 1000, Reward: 72.82470347759005, Best reward: 150.30938321492698, Avg reward: -39.673517504596994\n","Episode: 116, Steps: 220, Reward: 24.533634488710348, Best reward: 150.30938321492698, Avg reward: -36.18231954743719\n","Episode: 117, Steps: 219, Reward: 22.640311509286093, Best reward: 150.30938321492698, Avg reward: -31.59210268224425\n","Episode: 118, Steps: 121, Reward: 3.734909463766499, Best reward: 150.30938321492698, Avg reward: -30.707768873086287\n","Episode: 119, Steps: 199, Reward: -37.29034652289346, Best reward: 150.30938321492698, Avg reward: -30.485645957518162\n","Episode: 120, Steps: 148, Reward: -42.16422574873494, Best reward: 150.30938321492698, Avg reward: -26.4613423095096\n","Episode: 121, Steps: 316, Reward: 19.78560793281467, Best reward: 150.30938321492698, Avg reward: -25.217160047196575\n","Episode: 122, Steps: 1000, Reward: 57.02710380309019, Best reward: 150.30938321492698, Avg reward: -22.14807562748059\n","Episode: 123, Steps: 228, Reward: -47.43792514362968, Best reward: 150.30938321492698, Avg reward: -21.305898565660847\n","Episode: 124, Steps: 189, Reward: -75.33584850685813, Best reward: 150.30938321492698, Avg reward: -20.28045692677957\n","Episode: 125, Steps: 210, Reward: -51.94738889511852, Best reward: 150.30938321492698, Avg reward: -19.07829335194181\n","Episode: 126, Steps: 205, Reward: -44.66113664613476, Best reward: 150.30938321492698, Avg reward: -18.319429834251196\n","Episode: 127, Steps: 211, Reward: -110.14384717043481, Best reward: 150.30938321492698, Avg reward: -18.458267566955204\n","Episode: 128, Steps: 210, Reward: -9.608158995142546, Best reward: 150.30938321492698, Avg reward: -17.17407998940212\n","Episode: 129, Steps: 207, Reward: -85.95520631009414, Best reward: 150.30938321492698, Avg reward: -16.918827408491072\n","Episode: 130, Steps: 204, Reward: 17.802686614826214, Best reward: 150.30938321492698, Avg reward: -15.639331458481022\n","Episode: 131, Steps: 166, Reward: -70.64206243408213, Best reward: 150.30938321492698, Avg reward: -15.786456285158568\n","Episode: 132, Steps: 220, Reward: -40.99344852462116, Best reward: 150.30938321492698, Avg reward: -14.99192380415352\n","Episode: 133, Steps: 195, Reward: -64.49495890980006, Best reward: 150.30938321492698, Avg reward: -14.506572933208412\n","Episode: 134, Steps: 194, Reward: 2.2658280474483234, Best reward: 150.30938321492698, Avg reward: -13.464871368542486\n","Episode: 135, Steps: 204, Reward: -83.0931372472304, Best reward: 150.30938321492698, Avg reward: -13.729417658559232\n","Episode: 136, Steps: 239, Reward: -25.422098257158012, Best reward: 150.30938321492698, Avg reward: -14.423105654226722\n","Episode: 137, Steps: 187, Reward: -32.86133385868432, Best reward: 150.30938321492698, Avg reward: -13.477062552946439\n","Episode: 138, Steps: 201, Reward: -57.79995831003204, Best reward: 150.30938321492698, Avg reward: -13.171968502600722\n","Episode: 139, Steps: 211, Reward: -106.55946411265771, Best reward: 150.30938321492698, Avg reward: -13.36363726621606\n","Episode: 140, Steps: 179, Reward: -120.12675946311113, Best reward: 150.30938321492698, Avg reward: -14.215189913751338\n","Episode: 141, Steps: 225, Reward: -33.95133835321688, Best reward: 150.30938321492698, Avg reward: -14.123206072753069\n","Episode: 142, Steps: 184, Reward: -93.6159763360477, Best reward: 150.30938321492698, Avg reward: -14.488870031217095\n","Episode: 143, Steps: 181, Reward: -21.03371578041164, Best reward: 150.30938321492698, Avg reward: -13.988898799756184\n","Episode: 144, Steps: 193, Reward: 3.451413493083777, Best reward: 150.30938321492698, Avg reward: -13.356381429075398\n","Episode: 145, Steps: 209, Reward: -1.483742876463495, Best reward: 150.30938321492698, Avg reward: -12.748079392464874\n","Episode: 146, Steps: 186, Reward: -113.07388919575311, Best reward: 150.30938321492698, Avg reward: -14.040287888669802\n","Episode: 147, Steps: 179, Reward: -54.46139794182564, Best reward: 150.30938321492698, Avg reward: -14.987774650353646\n","Episode: 148, Steps: 254, Reward: 10.567548175962713, Best reward: 150.30938321492698, Avg reward: -12.919197863057054\n","Episode: 149, Steps: 195, Reward: -30.43474704149888, Best reward: 150.30938321492698, Avg reward: -14.72663916562131\n","Episode: 150, Steps: 214, Reward: -32.90390484466856, Best reward: 150.30938321492698, Avg reward: -14.886836478634516\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5e60a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 151, Steps: 195, Reward: -151.7386061871189, Best reward: 150.30938321492698, Avg reward: -16.88721320479163\n","Episode: 152, Steps: 192, Reward: -20.520696474051107, Best reward: 150.30938321492698, Avg reward: -18.4150784277149\n","Episode: 153, Steps: 239, Reward: -1.5581032668744967, Best reward: 150.30938321492698, Avg reward: -18.27087735377066\n","Episode: 154, Steps: 1000, Reward: 146.49702923841355, Best reward: 150.30938321492698, Avg reward: -16.948413344239345\n","Episode: 155, Steps: 197, Reward: -27.805604630100476, Best reward: 150.30938321492698, Avg reward: -17.569676165776638\n","Episode: 156, Steps: 205, Reward: -71.78978265696377, Best reward: 150.30938321492698, Avg reward: -18.501506510855556\n","Episode: 157, Steps: 179, Reward: -44.96209197328977, Best reward: 150.30938321492698, Avg reward: -19.081277261571355\n","Episode: 158, Steps: 191, Reward: -24.50343070537653, Best reward: 150.30938321492698, Avg reward: -19.435969612885778\n","Episode: 159, Steps: 184, Reward: -20.968282353281182, Best reward: 150.30938321492698, Avg reward: -20.05539419680648\n","Episode: 160, Steps: 156, Reward: -98.6989061727592, Best reward: 150.30938321492698, Avg reward: -21.225921699624546\n","Episode: 161, Steps: 177, Reward: 40.35213345696195, Best reward: 150.30938321492698, Avg reward: -21.324535212805408\n","Episode: 162, Steps: 192, Reward: -74.28525469951157, Best reward: 150.30938321492698, Avg reward: -21.579266824306195\n","Episode: 163, Steps: 189, Reward: 10.170926987599827, Best reward: 150.30938321492698, Avg reward: -21.221031322361043\n","Episode: 164, Steps: 177, Reward: -50.736062448586246, Best reward: 150.30938321492698, Avg reward: -21.509303359841848\n","Episode: 165, Steps: 223, Reward: 54.48204702890527, Best reward: 150.30938321492698, Avg reward: -21.341198936396196\n","Episode: 166, Steps: 162, Reward: -82.92142803868505, Best reward: 150.30938321492698, Avg reward: -21.740514204588592\n","Episode: 167, Steps: 237, Reward: -15.639157847575504, Best reward: 150.30938321492698, Avg reward: -21.728654255568543\n","Episode: 168, Steps: 189, Reward: -152.17632305173896, Best reward: 150.30938321492698, Avg reward: -24.325097099411327\n","Episode: 169, Steps: 246, Reward: -74.50669209733425, Best reward: 150.30938321492698, Avg reward: -25.089690195418424\n","Episode: 170, Steps: 174, Reward: -81.67701699150903, Best reward: 150.30938321492698, Avg reward: -25.669093406892983\n","Episode: 171, Steps: 229, Reward: -99.50923367560952, Best reward: 150.30938321492698, Avg reward: -26.647964588437254\n","Episode: 172, Steps: 224, Reward: -11.991551988975232, Best reward: 150.30938321492698, Avg reward: -27.331119139578067\n","Episode: 173, Steps: 1000, Reward: 117.39833484528721, Best reward: 150.30938321492698, Avg reward: -26.284134494556827\n","Episode: 174, Steps: 1000, Reward: 132.665083401611, Best reward: 150.30938321492698, Avg reward: -25.123789563295063\n","Episode: 175, Steps: 1000, Reward: 14.676019354536592, Best reward: 150.30938321492698, Avg reward: -25.607475438804983\n","Episode: 176, Steps: 785, Reward: -255.34516725433585, Best reward: 150.30938321492698, Avg reward: -28.4779853066335\n","Episode: 177, Steps: 1000, Reward: -157.55783969542873, Best reward: 150.30938321492698, Avg reward: -30.238812462468854\n","Episode: 178, Steps: 1000, Reward: -129.65910754927842, Best reward: 150.30938321492698, Avg reward: -31.79654922401457\n","Episode: 179, Steps: 1000, Reward: -100.92026633449257, Best reward: 150.30938321492698, Avg reward: -32.7502976865314\n","Episode: 180, Steps: 1000, Reward: 117.50570342258771, Best reward: 150.30938321492698, Avg reward: -31.94712648662291\n","Episode: 181, Steps: 1000, Reward: 45.585590007092975, Best reward: 150.30938321492698, Avg reward: -31.38513104142287\n","Episode: 182, Steps: 1000, Reward: 67.7003301915737, Best reward: 150.30938321492698, Avg reward: -30.75294502054234\n","Episode: 183, Steps: 149, Reward: 26.10715430562888, Best reward: 150.30938321492698, Avg reward: -30.748727582228288\n","Episode: 184, Steps: 1000, Reward: 141.06991169427303, Best reward: 150.30938321492698, Avg reward: -29.56121989467805\n","Episode: 185, Steps: 165, Reward: -1.4594750197723414, Best reward: 150.30938321492698, Avg reward: -29.48733219366547\n","Episode: 186, Steps: 544, Reward: -238.62856552697258, Best reward: 150.30938321492698, Avg reward: -32.03437841834252\n","Episode: 187, Steps: 1000, Reward: 171.1722467345924, Best reward: 150.30938321492698, Avg reward: -30.740472780973473\n","[+] Saving the models\n","Episode: 188, Steps: 1000, Reward: 156.89027821018564, Best reward: 171.1722467345924, Avg reward: -28.98786926775329\n","Episode: 189, Steps: 1000, Reward: -40.68510913437872, Best reward: 171.1722467345924, Avg reward: -29.122844012896813\n","Episode: 190, Steps: 1000, Reward: -11.027872050167426, Best reward: 171.1722467345924, Avg reward: -28.933697742001947\n","Episode: 191, Steps: 1000, Reward: 58.691553057131884, Best reward: 171.1722467345924, Avg reward: -28.232160063930884\n","Episode: 192, Steps: 1000, Reward: 0.46153789330593753, Best reward: 171.1722467345924, Avg reward: -27.778463247567984\n","Episode: 193, Steps: 1000, Reward: 40.12840200383588, Best reward: 171.1722467345924, Avg reward: -27.74973751716165\n","Episode: 194, Steps: 229, Reward: 22.29685686826427, Best reward: 171.1722467345924, Avg reward: -27.09129743420708\n","Episode: 195, Steps: 1000, Reward: -98.18340082876549, Best reward: 171.1722467345924, Avg reward: -27.432197794794888\n","Episode: 196, Steps: 1000, Reward: -66.00794082815312, Best reward: 171.1722467345924, Avg reward: -27.367089040694637\n","Episode: 197, Steps: 213, Reward: -44.65570417711395, Best reward: 171.1722467345924, Avg reward: -27.28689896039714\n","Episode: 198, Steps: 197, Reward: -33.07975034207969, Best reward: 171.1722467345924, Avg reward: -27.34349229889319\n","Episode: 199, Steps: 200, Reward: 17.748532713809922, Best reward: 171.1722467345924, Avg reward: -27.410655358683726\n","Episode: 200, Steps: 1000, Reward: 82.34285153582675, Best reward: 171.1722467345924, Avg reward: -24.24768121898005\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5fa7a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 201, Steps: 1000, Reward: 155.88625086083465, Best reward: 171.1722467345924, Avg reward: -22.965883790487283\n","Episode: 202, Steps: 1000, Reward: 40.28604554385778, Best reward: 171.1722467345924, Avg reward: -20.63009833117134\n","Episode: 203, Steps: 827, Reward: 126.39147682529858, Best reward: 171.1722467345924, Avg reward: -17.05082960266847\n","Episode: 204, Steps: 1000, Reward: -70.31059104372916, Best reward: 171.1722467345924, Avg reward: -17.298231005260646\n","Episode: 205, Steps: 1000, Reward: -62.14634504693565, Best reward: 171.1722467345924, Avg reward: -18.133184703993937\n","Episode: 206, Steps: 1000, Reward: -61.47178460447232, Best reward: 171.1722467345924, Avg reward: -18.917493969509803\n","Episode: 207, Steps: 1000, Reward: -59.5186700144834, Best reward: 171.1722467345924, Avg reward: -19.09053186432459\n","Episode: 208, Steps: 1000, Reward: -82.84604443770651, Best reward: 171.1722467345924, Avg reward: -19.23035732730359\n","Episode: 209, Steps: 1000, Reward: -205.4973876828546, Best reward: 171.1722467345924, Avg reward: -21.93603549045513\n","Episode: 210, Steps: 494, Reward: -215.46271410069062, Best reward: 171.1722467345924, Avg reward: -24.42578944951297\n","Episode: 211, Steps: 446, Reward: -251.25211337032755, Best reward: 171.1722467345924, Avg reward: -27.249275393686894\n","Episode: 212, Steps: 1000, Reward: -142.53826861437048, Best reward: 171.1722467345924, Avg reward: -27.865452038718423\n","Episode: 213, Steps: 1000, Reward: -45.181900127191156, Best reward: 171.1722467345924, Avg reward: -28.33999473786976\n","Episode: 214, Steps: 225, Reward: -209.37115437331778, Best reward: 171.1722467345924, Avg reward: -31.551513716546683\n","Episode: 215, Steps: 380, Reward: -219.41471053394133, Best reward: 171.1722467345924, Avg reward: -34.473907856661995\n","Episode: 216, Steps: 263, Reward: -158.5788252460273, Best reward: 171.1722467345924, Avg reward: -36.30503245400937\n","Episode: 217, Steps: 1000, Reward: -40.34079703051202, Best reward: 171.1722467345924, Avg reward: -36.93484353940735\n","Episode: 218, Steps: 299, Reward: -164.85979068158116, Best reward: 171.1722467345924, Avg reward: -38.62079054086083\n","Episode: 219, Steps: 413, Reward: -215.4069416074633, Best reward: 171.1722467345924, Avg reward: -40.401956491706535\n","Episode: 220, Steps: 616, Reward: -105.36755504417131, Best reward: 171.1722467345924, Avg reward: -41.03398978466089\n","Episode: 221, Steps: 379, Reward: -175.57855855738154, Best reward: 171.1722467345924, Avg reward: -42.98763144956285\n","Episode: 222, Steps: 1000, Reward: 3.637700877967874, Best reward: 171.1722467345924, Avg reward: -43.52152547881406\n","Episode: 223, Steps: 1000, Reward: -47.93735504000513, Best reward: 171.1722467345924, Avg reward: -43.52651977777783\n","Episode: 224, Steps: 1000, Reward: -25.563014239677972, Best reward: 171.1722467345924, Avg reward: -43.02879143510602\n","Episode: 225, Steps: 1000, Reward: -54.92776085275409, Best reward: 171.1722467345924, Avg reward: -43.05859515468237\n","Episode: 226, Steps: 1000, Reward: -32.723564035214075, Best reward: 171.1722467345924, Avg reward: -42.93921942857318\n","Episode: 227, Steps: 1000, Reward: -122.50726081797659, Best reward: 171.1722467345924, Avg reward: -43.06285356504859\n","Episode: 228, Steps: 1000, Reward: -48.42225052437645, Best reward: 171.1722467345924, Avg reward: -43.450994480340924\n","Episode: 229, Steps: 1000, Reward: 28.76223177158618, Best reward: 171.1722467345924, Avg reward: -42.30382009952414\n","Episode: 230, Steps: 967, Reward: -229.07247320245557, Best reward: 171.1722467345924, Avg reward: -44.77257169769695\n","Episode: 231, Steps: 914, Reward: -219.0607673028767, Best reward: 171.1722467345924, Avg reward: -46.256758746384904\n","Episode: 232, Steps: 729, Reward: -159.43405644252545, Best reward: 171.1722467345924, Avg reward: -47.44116482556396\n","Episode: 233, Steps: 1000, Reward: -119.24777988502696, Best reward: 171.1722467345924, Avg reward: -47.98869303531622\n","Episode: 234, Steps: 1000, Reward: -54.969990428595615, Best reward: 171.1722467345924, Avg reward: -48.56105122007665\n","Episode: 235, Steps: 543, Reward: 270.06904296486215, Best reward: 171.1722467345924, Avg reward: -45.029429417955726\n","[+] Saving the models\n","Episode: 236, Steps: 1000, Reward: -46.63334948921716, Best reward: 270.06904296486215, Avg reward: -45.24154193027632\n","Episode: 237, Steps: 1000, Reward: -63.418289858371146, Best reward: 270.06904296486215, Avg reward: -45.54711149027318\n","Episode: 238, Steps: 1000, Reward: 141.58742437182272, Best reward: 270.06904296486215, Avg reward: -43.55323766345464\n","Episode: 239, Steps: 657, Reward: 103.08742812351363, Best reward: 270.06904296486215, Avg reward: -41.45676874109292\n","Episode: 240, Steps: 1000, Reward: 139.21495785452277, Best reward: 270.06904296486215, Avg reward: -38.86335156791658\n","Episode: 241, Steps: 102, Reward: -130.29300374694355, Best reward: 270.06904296486215, Avg reward: -39.826768221853854\n","Episode: 242, Steps: 1000, Reward: -42.386228279272075, Best reward: 270.06904296486215, Avg reward: -39.314470741286094\n","Episode: 243, Steps: 1000, Reward: 101.59700783322585, Best reward: 270.06904296486215, Avg reward: -38.08816350514972\n","Episode: 244, Steps: 516, Reward: 193.448304389999, Best reward: 270.06904296486215, Avg reward: -36.18819459618057\n","Episode: 245, Steps: 1000, Reward: 121.07619750757806, Best reward: 270.06904296486215, Avg reward: -34.96259519234015\n","Episode: 246, Steps: 220, Reward: 9.462279578674838, Best reward: 270.06904296486215, Avg reward: -33.73723350459588\n","Episode: 247, Steps: 650, Reward: -94.0580893145692, Best reward: 270.06904296486215, Avg reward: -34.133200418323305\n","Episode: 248, Steps: 1000, Reward: 78.7296957253183, Best reward: 270.06904296486215, Avg reward: -33.45157894282975\n","Episode: 249, Steps: 208, Reward: 20.081104124844828, Best reward: 270.06904296486215, Avg reward: -32.94642043116631\n","Episode: 250, Steps: 1000, Reward: 182.10057311167822, Best reward: 270.06904296486215, Avg reward: -30.796375651602848\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5e1ba00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 251, Steps: 1000, Reward: 104.47050518338948, Best reward: 270.06904296486215, Avg reward: -28.23428453789776\n","Episode: 252, Steps: 407, Reward: 178.0477573048347, Best reward: 270.06904296486215, Avg reward: -26.248600000108908\n","Episode: 253, Steps: 1000, Reward: 171.99786922697257, Best reward: 270.06904296486215, Avg reward: -24.51304027517044\n","Episode: 254, Steps: 1000, Reward: 81.25018376035658, Best reward: 270.06904296486215, Avg reward: -25.165508729951007\n","Episode: 255, Steps: 1000, Reward: 136.2845580970045, Best reward: 270.06904296486215, Avg reward: -23.524607102679955\n","Episode: 256, Steps: 1000, Reward: 142.64832254362196, Best reward: 270.06904296486215, Avg reward: -21.380226050674104\n","Episode: 257, Steps: 1000, Reward: 174.21533798552483, Best reward: 270.06904296486215, Avg reward: -19.18845175108595\n","Episode: 258, Steps: 1000, Reward: 156.41387764728802, Best reward: 270.06904296486215, Avg reward: -17.379278667559305\n","Episode: 259, Steps: 1000, Reward: 80.83654855776959, Best reward: 270.06904296486215, Avg reward: -16.361230358448797\n","Episode: 260, Steps: 1000, Reward: 153.31951794829905, Best reward: 270.06904296486215, Avg reward: -13.841046117238214\n","Episode: 261, Steps: 1000, Reward: 88.7139186577022, Best reward: 270.06904296486215, Avg reward: -13.357428265230812\n","Episode: 262, Steps: 1000, Reward: 70.02596012952148, Best reward: 270.06904296486215, Avg reward: -11.914316116940482\n","Episode: 263, Steps: 1000, Reward: 87.29852397407778, Best reward: 270.06904296486215, Avg reward: -11.143040147075704\n","Episode: 264, Steps: 1000, Reward: 137.12424261731886, Best reward: 270.06904296486215, Avg reward: -9.264437096416652\n","Episode: 265, Steps: 1000, Reward: 110.86100465166128, Best reward: 270.06904296486215, Avg reward: -8.700647520189092\n","Episode: 266, Steps: 1000, Reward: 87.0284194795131, Best reward: 270.06904296486215, Avg reward: -7.001149045007108\n","Episode: 267, Steps: 1000, Reward: 29.240615438765904, Best reward: 270.06904296486215, Avg reward: -6.552351312143697\n","Episode: 268, Steps: 1000, Reward: 13.771317050282647, Best reward: 270.06904296486215, Avg reward: -4.892874911123479\n","Episode: 269, Steps: 509, Reward: -173.45198943038122, Best reward: 270.06904296486215, Avg reward: -5.882327884453948\n","Episode: 270, Steps: 796, Reward: -266.20794193186214, Best reward: 270.06904296486215, Avg reward: -7.727637133857481\n","Episode: 271, Steps: 692, Reward: 49.68349029524556, Best reward: 270.06904296486215, Avg reward: -6.235709894148931\n","Episode: 272, Steps: 552, Reward: -160.48741722645045, Best reward: 270.06904296486215, Avg reward: -7.720668546523684\n","Episode: 273, Steps: 1000, Reward: 83.1739361852946, Best reward: 270.06904296486215, Avg reward: -8.062912533123612\n","Episode: 274, Steps: 1000, Reward: 34.71361115092808, Best reward: 270.06904296486215, Avg reward: -9.042427255630441\n","Episode: 275, Steps: 653, Reward: -61.47017557592978, Best reward: 270.06904296486215, Avg reward: -9.803889204935103\n","Episode: 276, Steps: 1000, Reward: -144.16548285761846, Best reward: 270.06904296486215, Avg reward: -8.692092360967932\n","Episode: 277, Steps: 366, Reward: -128.11995048587772, Best reward: 270.06904296486215, Avg reward: -8.397713468872418\n","Episode: 278, Steps: 1000, Reward: 19.525680201403663, Best reward: 270.06904296486215, Avg reward: -6.905865591365596\n","Episode: 279, Steps: 1000, Reward: 22.221941178336298, Best reward: 270.06904296486215, Avg reward: -5.6744435162373055\n","Episode: 280, Steps: 1000, Reward: -17.753060524315835, Best reward: 270.06904296486215, Avg reward: -7.027031155706343\n","Episode: 281, Steps: 1000, Reward: -45.25214718721833, Best reward: 270.06904296486215, Avg reward: -7.935408527649454\n","Episode: 282, Steps: 1000, Reward: 170.10840432302834, Best reward: 270.06904296486215, Avg reward: -6.911327786334908\n","Episode: 283, Steps: 1000, Reward: 57.23196450110058, Best reward: 270.06904296486215, Avg reward: -6.600079684380193\n","Episode: 284, Steps: 1000, Reward: 65.98737896586897, Best reward: 270.06904296486215, Avg reward: -7.350905011664232\n","Episode: 285, Steps: 977, Reward: -245.38029112856339, Best reward: 270.06904296486215, Avg reward: -9.790113172752143\n","Episode: 286, Steps: 762, Reward: -132.95022025071037, Best reward: 270.06904296486215, Avg reward: -8.73332971998952\n","Episode: 287, Steps: 1000, Reward: -49.66980033446736, Best reward: 270.06904296486215, Avg reward: -10.94175019068012\n","Episode: 288, Steps: 1000, Reward: -36.73905011350399, Best reward: 270.06904296486215, Avg reward: -12.878043473917014\n","Episode: 289, Steps: 250, Reward: -55.90419310887464, Best reward: 270.06904296486215, Avg reward: -13.030234313661971\n","Episode: 290, Steps: 1000, Reward: -37.06446805956178, Best reward: 270.06904296486215, Avg reward: -13.290600273755915\n","Episode: 291, Steps: 1000, Reward: -68.53533192471907, Best reward: 270.06904296486215, Avg reward: -14.562869123574423\n","Episode: 292, Steps: 1000, Reward: -61.629253684280656, Best reward: 270.06904296486215, Avg reward: -15.183777039350291\n","Episode: 293, Steps: 1000, Reward: -69.98727356408222, Best reward: 270.06904296486215, Avg reward: -16.284933795029474\n","Episode: 294, Steps: 998, Reward: -177.66283512047875, Best reward: 270.06904296486215, Avg reward: -18.284530714916905\n","Episode: 295, Steps: 1000, Reward: 111.74845577920053, Best reward: 270.06904296486215, Avg reward: -16.185212148837245\n","Episode: 296, Steps: 448, Reward: -187.55119108095778, Best reward: 270.06904296486215, Avg reward: -17.40064465136529\n","Episode: 297, Steps: 287, Reward: -83.34295228424014, Best reward: 270.06904296486215, Avg reward: -17.787517132436555\n","Episode: 298, Steps: 620, Reward: -208.4005042362837, Best reward: 270.06904296486215, Avg reward: -19.540724671378598\n","Episode: 299, Steps: 590, Reward: -185.2029668875834, Best reward: 270.06904296486215, Avg reward: -21.570239667392528\n","Episode: 300, Steps: 152, Reward: -136.69631245943629, Best reward: 270.06904296486215, Avg reward: -23.76063130734516\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x5a09a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 301, Steps: 582, Reward: -182.49875179645926, Best reward: 270.06904296486215, Avg reward: -27.144481333918097\n","Episode: 302, Steps: 376, Reward: -97.87219496481451, Best reward: 270.06904296486215, Avg reward: -28.526063739004822\n","Episode: 303, Steps: 1000, Reward: 56.54930921668309, Best reward: 270.06904296486215, Avg reward: -29.22448541509097\n","Episode: 304, Steps: 1000, Reward: 140.07637302455956, Best reward: 270.06904296486215, Avg reward: -27.120615774408087\n","Episode: 305, Steps: 536, Reward: -138.15908970342312, Best reward: 270.06904296486215, Avg reward: -27.880743220972963\n","Episode: 306, Steps: 1000, Reward: 55.27302714160878, Best reward: 270.06904296486215, Avg reward: -26.713295103512152\n","Episode: 307, Steps: 1000, Reward: 74.47603312328778, Best reward: 270.06904296486215, Avg reward: -25.373348072134437\n","Episode: 308, Steps: 603, Reward: -137.37374938202612, Best reward: 270.06904296486215, Avg reward: -25.918625121577634\n","Episode: 309, Steps: 580, Reward: -180.38314410234062, Best reward: 270.06904296486215, Avg reward: -25.667482685772498\n","Episode: 310, Steps: 493, Reward: -163.66859470534902, Best reward: 270.06904296486215, Avg reward: -25.149541491819082\n","Episode: 311, Steps: 629, Reward: -178.61460850317218, Best reward: 270.06904296486215, Avg reward: -24.42316644314753\n","Episode: 312, Steps: 652, Reward: -223.34134026579068, Best reward: 270.06904296486215, Avg reward: -25.231197159661733\n","Episode: 313, Steps: 530, Reward: -170.94675084612788, Best reward: 270.06904296486215, Avg reward: -26.4888456668511\n","Episode: 314, Steps: 772, Reward: 158.54173053044184, Best reward: 270.06904296486215, Avg reward: -22.809716817813502\n","Episode: 315, Steps: 1000, Reward: 111.61915344685701, Best reward: 270.06904296486215, Avg reward: -19.499378178005514\n","Episode: 316, Steps: 1000, Reward: 32.429220230621816, Best reward: 270.06904296486215, Avg reward: -17.589297723239024\n","Episode: 317, Steps: 1000, Reward: 106.34881863849014, Best reward: 270.06904296486215, Avg reward: -16.122401566549\n","Episode: 318, Steps: 1000, Reward: 125.42681270701274, Best reward: 270.06904296486215, Avg reward: -13.219535532663063\n","Episode: 319, Steps: 1000, Reward: 114.56821204942857, Best reward: 270.06904296486215, Avg reward: -9.919783996094141\n","Episode: 320, Steps: 1000, Reward: 101.00717450982758, Best reward: 270.06904296486215, Avg reward: -7.856036700554154\n","Episode: 321, Steps: 1000, Reward: 58.92539767360903, Best reward: 270.06904296486215, Avg reward: -5.510997138244249\n","Episode: 322, Steps: 395, Reward: 152.56425330735055, Best reward: 270.06904296486215, Avg reward: -4.021731613950423\n","Episode: 323, Steps: 1000, Reward: 163.9730828389641, Best reward: 270.06904296486215, Avg reward: -1.902627235160731\n","Episode: 324, Steps: 1000, Reward: 141.79592681670908, Best reward: 270.06904296486215, Avg reward: -0.22903782459685942\n","Episode: 325, Steps: 1000, Reward: 137.23475358823137, Best reward: 270.06904296486215, Avg reward: 1.6925873198129957\n","Episode: 326, Steps: 1000, Reward: 88.49978994585328, Best reward: 270.06904296486215, Avg reward: 2.9048208596236704\n","Episode: 327, Steps: 1000, Reward: -10.23150911718269, Best reward: 270.06904296486215, Avg reward: 4.027578376631606\n","Episode: 328, Steps: 850, Reward: 114.80708166845506, Best reward: 270.06904296486215, Avg reward: 5.659871698559921\n","Episode: 329, Steps: 228, Reward: 222.97197592263345, Best reward: 270.06904296486215, Avg reward: 7.601969140070394\n","Episode: 330, Steps: 1000, Reward: 54.423227787287914, Best reward: 270.06904296486215, Avg reward: 10.436926149967828\n","Episode: 331, Steps: 1000, Reward: 114.9826388149799, Best reward: 270.06904296486215, Avg reward: 13.777360211146394\n","Episode: 332, Steps: 1000, Reward: 112.86559390367401, Best reward: 270.06904296486215, Avg reward: 16.500356714608387\n","Episode: 333, Steps: 1000, Reward: -31.92586207635759, Best reward: 270.06904296486215, Avg reward: 17.37357589269508\n","Episode: 334, Steps: 966, Reward: 243.9398954712867, Best reward: 270.06904296486215, Avg reward: 20.362674751693902\n","Episode: 335, Steps: 1000, Reward: 41.494050833847794, Best reward: 270.06904296486215, Avg reward: 18.076924830383756\n","Episode: 336, Steps: 1000, Reward: -43.707636010559206, Best reward: 270.06904296486215, Avg reward: 18.10618196517034\n","Episode: 337, Steps: 1000, Reward: 124.8273167089248, Best reward: 270.06904296486215, Avg reward: 19.988638030843298\n","Episode: 338, Steps: 1000, Reward: -41.15994007538482, Best reward: 270.06904296486215, Avg reward: 18.161164386371226\n","Episode: 339, Steps: 1000, Reward: 138.99543093283768, Best reward: 270.06904296486215, Avg reward: 18.520244414464464\n","Episode: 340, Steps: 1000, Reward: 35.301968905051275, Best reward: 270.06904296486215, Avg reward: 17.481114524969755\n","Episode: 341, Steps: 1000, Reward: -61.64992858732011, Best reward: 270.06904296486215, Avg reward: 18.16754527656598\n","Episode: 342, Steps: 1000, Reward: -23.984030564936855, Best reward: 270.06904296486215, Avg reward: 18.351567253709337\n","Episode: 343, Steps: 1000, Reward: 133.35331248210554, Best reward: 270.06904296486215, Avg reward: 18.669130300198134\n","Episode: 344, Steps: 1000, Reward: -56.095092996472545, Best reward: 270.06904296486215, Avg reward: 16.17369632633342\n","Episode: 345, Steps: 1000, Reward: -53.48582115995993, Best reward: 270.06904296486215, Avg reward: 14.428076139658042\n","Episode: 346, Steps: 1000, Reward: 114.3665875742548, Best reward: 270.06904296486215, Avg reward: 15.47711921961384\n","Episode: 347, Steps: 1000, Reward: 53.632873798993664, Best reward: 270.06904296486215, Avg reward: 16.95402885074947\n","Episode: 348, Steps: 1000, Reward: 94.71293394237105, Best reward: 270.06904296486215, Avg reward: 17.113861232919998\n","Episode: 349, Steps: 587, Reward: -177.28576328378148, Best reward: 270.06904296486215, Avg reward: 15.140192558833732\n","Episode: 350, Steps: 632, Reward: 138.9527615638345, Best reward: 270.06904296486215, Avg reward: 14.708714443355296\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x6e05a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 351, Steps: 285, Reward: -126.68110423424349, Best reward: 270.06904296486215, Avg reward: 12.397198349178968\n","Episode: 352, Steps: 199, Reward: 41.979134948071106, Best reward: 270.06904296486215, Avg reward: 11.03651212561133\n","Episode: 353, Steps: 1000, Reward: 173.46040764238253, Best reward: 270.06904296486215, Avg reward: 11.051137509765432\n","Episode: 354, Steps: 257, Reward: -94.5213350066885, Best reward: 270.06904296486215, Avg reward: 9.29342232209498\n","Episode: 355, Steps: 273, Reward: -22.598958423786215, Best reward: 270.06904296486215, Avg reward: 7.704587156887075\n","Episode: 356, Steps: 318, Reward: 249.98698064923826, Best reward: 270.06904296486215, Avg reward: 8.777973737943235\n","Episode: 357, Steps: 1000, Reward: 38.37651343873393, Best reward: 270.06904296486215, Avg reward: 7.419585492475328\n","Episode: 358, Steps: 1000, Reward: 166.022160014957, Best reward: 270.06904296486215, Avg reward: 7.515668316152016\n","Episode: 359, Steps: 1000, Reward: 83.51758610253765, Best reward: 270.06904296486215, Avg reward: 7.542478691599696\n","Episode: 360, Steps: 451, Reward: 212.76402021437747, Best reward: 270.06904296486215, Avg reward: 8.136923714260481\n","Episode: 361, Steps: 1000, Reward: 150.59939780067637, Best reward: 270.06904296486215, Avg reward: 8.755778505690222\n","Episode: 362, Steps: 1000, Reward: 130.32197945633135, Best reward: 270.06904296486215, Avg reward: 9.358738698958318\n","Episode: 363, Steps: 1000, Reward: 88.32317035126766, Best reward: 270.06904296486215, Avg reward: 9.368985162730219\n","Episode: 364, Steps: 1000, Reward: 122.48054808898293, Best reward: 270.06904296486215, Avg reward: 9.22254821744686\n","Episode: 365, Steps: 1000, Reward: 175.51627348701547, Best reward: 270.06904296486215, Avg reward: 9.869100905800403\n","Episode: 366, Steps: 268, Reward: -144.06890947002242, Best reward: 270.06904296486215, Avg reward: 7.558127616305047\n","Episode: 367, Steps: 1000, Reward: 77.17769365451538, Best reward: 270.06904296486215, Avg reward: 8.037498398462542\n","Episode: 368, Steps: 232, Reward: -101.33065219838257, Best reward: 270.06904296486215, Avg reward: 6.886478705975889\n","Episode: 369, Steps: 1000, Reward: 94.72300557215046, Best reward: 270.06904296486215, Avg reward: 9.568228656001207\n","Episode: 370, Steps: 225, Reward: -106.69276781749588, Best reward: 270.06904296486215, Avg reward: 11.16338039714487\n","Episode: 371, Steps: 1000, Reward: 157.22692816352196, Best reward: 270.06904296486215, Avg reward: 12.238814775827635\n","Episode: 372, Steps: 1000, Reward: 136.94552678929645, Best reward: 270.06904296486215, Avg reward: 15.213144215985103\n","Episode: 373, Steps: 1000, Reward: 154.03942350971016, Best reward: 270.06904296486215, Avg reward: 15.921799089229257\n","Episode: 374, Steps: 232, Reward: -116.96042961398344, Best reward: 270.06904296486215, Avg reward: 14.405058681580142\n","Episode: 375, Steps: 1000, Reward: 55.896413171346964, Best reward: 270.06904296486215, Avg reward: 15.578724569052909\n","Episode: 376, Steps: 1000, Reward: 48.45731914823797, Best reward: 270.06904296486215, Avg reward: 17.504952589111472\n","Episode: 377, Steps: 1000, Reward: 107.13413240994322, Best reward: 270.06904296486215, Avg reward: 19.85749341806968\n","Episode: 378, Steps: 551, Reward: 171.77271721658593, Best reward: 270.06904296486215, Avg reward: 21.379963788221502\n","Episode: 379, Steps: 1000, Reward: -76.7249746262534, Best reward: 270.06904296486215, Avg reward: 20.390494630175606\n","Episode: 380, Steps: 449, Reward: -85.33192233288241, Best reward: 270.06904296486215, Avg reward: 19.714706012089945\n","Episode: 381, Steps: 1000, Reward: 128.52945154785493, Best reward: 270.06904296486215, Avg reward: 21.452521999440677\n","Episode: 382, Steps: 1000, Reward: 8.430336555740855, Best reward: 270.06904296486215, Avg reward: 19.8357413217678\n","Episode: 383, Steps: 1000, Reward: 30.353018686832932, Best reward: 270.06904296486215, Avg reward: 19.566951863625125\n","Episode: 384, Steps: 1000, Reward: 121.0379132358384, Best reward: 270.06904296486215, Avg reward: 20.117457206324822\n","Episode: 385, Steps: 1000, Reward: -30.87141245551938, Best reward: 270.06904296486215, Avg reward: 22.26254599305526\n","Episode: 386, Steps: 1000, Reward: -73.63090917894168, Best reward: 270.06904296486215, Avg reward: 22.855739103772944\n","Episode: 387, Steps: 1000, Reward: -84.45072120235645, Best reward: 270.06904296486215, Avg reward: 22.50792989509406\n","Episode: 388, Steps: 1000, Reward: -112.63946417358359, Best reward: 270.06904296486215, Avg reward: 21.748925754493257\n","Episode: 389, Steps: 316, Reward: -142.23248779188322, Best reward: 270.06904296486215, Avg reward: 20.88564280766317\n","Episode: 390, Steps: 1000, Reward: -102.62787200209151, Best reward: 270.06904296486215, Avg reward: 20.23000876823788\n","Episode: 391, Steps: 1000, Reward: 79.46008472055536, Best reward: 270.06904296486215, Avg reward: 21.70996293469062\n","Episode: 392, Steps: 1000, Reward: 134.6995962975376, Best reward: 270.06904296486215, Avg reward: 23.673251434508803\n","Episode: 393, Steps: 1000, Reward: 141.8053702765022, Best reward: 270.06904296486215, Avg reward: 25.791177872914645\n","Episode: 394, Steps: 1000, Reward: -154.1601657215647, Best reward: 270.06904296486215, Avg reward: 26.02620456690379\n","Episode: 395, Steps: 1000, Reward: 112.5193495178794, Best reward: 270.06904296486215, Avg reward: 26.03391350429058\n","Episode: 396, Steps: 1000, Reward: -17.936378195554532, Best reward: 270.06904296486215, Avg reward: 27.730061633144615\n","Episode: 397, Steps: 570, Reward: -138.81520071796547, Best reward: 270.06904296486215, Avg reward: 27.17533914880736\n","Episode: 398, Steps: 1000, Reward: -66.49514118616364, Best reward: 270.06904296486215, Avg reward: 28.59439277930856\n","Episode: 399, Steps: 1000, Reward: -35.02689278928362, Best reward: 270.06904296486215, Avg reward: 30.09615352029156\n","Episode: 400, Steps: 1000, Reward: -55.52941814665812, Best reward: 270.06904296486215, Avg reward: 30.90782246341934\n"]},{"name":"stderr","output_type":"stream","text":["[swscaler @ 0x6f40a00] Warning: data is not aligned! This can lead to a speed loss\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 401, Steps: 1000, Reward: 141.78072032056463, Best reward: 270.06904296486215, Avg reward: 34.150617184589585\n","Episode: 402, Steps: 1000, Reward: 96.58115462592528, Best reward: 270.06904296486215, Avg reward: 36.09515068049698\n","Episode: 403, Steps: 1000, Reward: -120.43573253874571, Best reward: 270.06904296486215, Avg reward: 34.32530026294268\n","Episode: 404, Steps: 1000, Reward: 132.80705025286952, Best reward: 270.06904296486215, Avg reward: 34.25260703522578\n","Episode: 405, Steps: 1000, Reward: -69.21535077861257, Best reward: 270.06904296486215, Avg reward: 34.942044424473885\n","Episode: 406, Steps: 1000, Reward: 120.26212925119864, Best reward: 270.06904296486215, Avg reward: 35.59193544556978\n","Episode: 407, Steps: 1000, Reward: -109.40178571040276, Best reward: 270.06904296486215, Avg reward: 33.753157257232886\n","Episode: 408, Steps: 1000, Reward: 136.08048893137993, Best reward: 270.06904296486215, Avg reward: 36.48769964036694\n","Episode: 409, Steps: 1000, Reward: 158.7095402113503, Best reward: 270.06904296486215, Avg reward: 39.87862648350385\n","Episode: 410, Steps: 779, Reward: -187.91855573365982, Best reward: 270.06904296486215, Avg reward: 39.63612687322074\n","Episode: 411, Steps: 735, Reward: -180.14200443290764, Best reward: 270.06904296486215, Avg reward: 39.620852913923386\n","Episode: 412, Steps: 617, Reward: 236.62847663937177, Best reward: 270.06904296486215, Avg reward: 44.22055108297501\n","Episode: 413, Steps: 1000, Reward: 178.45692819698817, Best reward: 270.06904296486215, Avg reward: 47.714587873406174\n","Episode: 414, Steps: 1000, Reward: -84.15943809423163, Best reward: 270.06904296486215, Avg reward: 45.28757618715943\n","Episode: 415, Steps: 1000, Reward: -45.16178883958775, Best reward: 270.06904296486215, Avg reward: 43.71976676429499\n","Episode: 416, Steps: 1000, Reward: 125.11808866324861, Best reward: 270.06904296486215, Avg reward: 44.64665544862125\n","Episode: 417, Steps: 1000, Reward: 93.60185882209467, Best reward: 270.06904296486215, Avg reward: 44.519185850457305\n","Episode: 418, Steps: 1000, Reward: 103.9545705333922, Best reward: 270.06904296486215, Avg reward: 44.30446342872109\n","Episode: 419, Steps: 1000, Reward: 122.2489817153054, Best reward: 270.06904296486215, Avg reward: 44.38127112537986\n","Episode: 420, Steps: 1000, Reward: 130.98327584557217, Best reward: 270.06904296486215, Avg reward: 44.68103213873731\n","Episode: 421, Steps: 1000, Reward: 92.81945740585455, Best reward: 270.06904296486215, Avg reward: 45.01997273605977\n","Episode: 422, Steps: 1000, Reward: 109.07705694594783, Best reward: 270.06904296486215, Avg reward: 44.58510077244574\n","Episode: 423, Steps: 1000, Reward: 190.06789927461313, Best reward: 270.06904296486215, Avg reward: 44.846048936802234\n","Episode: 424, Steps: 1000, Reward: 164.405938773333, Best reward: 270.06904296486215, Avg reward: 45.072149056368474\n","Episode: 425, Steps: 1000, Reward: -61.92863633694434, Best reward: 270.06904296486215, Avg reward: 43.08051515711671\n","Episode: 426, Steps: 1000, Reward: -17.239375295061564, Best reward: 270.06904296486215, Avg reward: 42.02312350470756\n","Episode: 427, Steps: 1000, Reward: 160.9722688538094, Best reward: 270.06904296486215, Avg reward: 43.73516128441748\n","Episode: 428, Steps: 1000, Reward: 70.72774456934918, Best reward: 270.06904296486215, Avg reward: 43.29436791342641\n","Episode: 429, Steps: 1000, Reward: 154.9501207264525, Best reward: 270.06904296486215, Avg reward: 42.61414936146463\n","Episode: 430, Steps: 1000, Reward: -26.27980299440193, Best reward: 270.06904296486215, Avg reward: 41.80711905364773\n","Episode: 431, Steps: 1000, Reward: 169.02099570993323, Best reward: 270.06904296486215, Avg reward: 42.34750262259726\n","Episode: 432, Steps: 1000, Reward: 137.3793765240774, Best reward: 270.06904296486215, Avg reward: 42.59264044880128\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/760713040.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                           \u001b[0mexplore_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network_update_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 )\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_rl_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td3_episode_rewards.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/484381088.py\u001b[0m in \u001b[0;36mtrain_rl_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network_update_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mupdate_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3018711625.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, update_actor)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mq_vals_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mq_vals_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mtarget_q_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mone_step_lookahead_vals\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mcritic_2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3018711625.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mq_vals_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mq_vals_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mtarget_q_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mone_step_lookahead_vals\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mcritic_2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7326\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7327\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7328\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0;32m-> 1072\u001b[0;31m                                                   stack(strides))\n\u001b[0m\u001b[1;32m   1073\u001b[0m       \u001b[0;31m# TODO(mdan): Instead of implicitly casting, it's better to enforce the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m       \u001b[0;31m# same dtypes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m                                          as_ref=False):\n\u001b[1;32m    342\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m    267\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 268\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gymnasium as gym\n","import time\n","import signal\n","import time\n","import sys\n","import pickle\n","import os \n","\n","env = make_env(\"LunarLanderContinuous-v2\", \"videos/\", 50)\n","print(env.action_space)\n","record = True\n","gamma = 0.99\n","alpha = 0.0001# actor lr\n","beta = 0.001# critic lr \n","batch_size = 64\n","tau = 0.01\n","soft_update = True \n","noe = 500\n","max_steps = int(1e5)\n","is_tg = True \n","tg_bot_freq_epi = 20\n","record = True \n","mem_size = int(5e6)\n","noise = 0.5\n","explore_steps = 1000\n","policy_network_update_freq = 2\n","  \n","    \n","if not os.path.exists(\"videos\"): \n","    os.mkdir(\"videos\")\n","\n","if not os.path.exists(\"test_videos\"):\n","    os.mkdir(\"test_videos\")\n","\n","\n","if __name__ == \"__main__\": \n","    \n","    try: \n","        manage_memory()\n","        trainer = Trainer(env, gamma, alpha, beta, batch_size, tau,\n","                          soft_update, noe, max_steps, is_tg,\n","                          tg_bot_freq_epi, record, mem_size, noise,\n","                          explore_steps, policy_network_update_freq\n","                )\n","        episode_rewards, avg_rewards, best_reward = trainer.train_rl_model()\n","        \n","        with open(\"td3_episode_rewards.obj\", \"wb\") as f: \n","            pickle.dump(episode_rewards, f)\n","        \n","        with open(\"td3_avg_rewards.obj\", \"wb\") as f:\n","            pickle.dump(avg_rewards, f)\n","            \n","        x = [i+1 for i in range(noe)]\n","        plot_learning_curve(x, episode_rewards, \"td3_con_mountain_car\")\n","\n","       # model_path = \"models/lunarlander_DQN_q_value/\"\n","\n","        #evaluator = Eval(env, action_space, model_path, \"vanilla_dqn_lunarlander\", 10)\n","        #evaluator.test()\n","        \n","    except Exception as error: \n","        raise error"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
