{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium[atari] --quiet\n!pip install gymnasium --quiet\n!pip install -U gymnasium[atari] --quiet\n!pip install imageio_ffmpeg --quiet\n!pip install npy_append_array --quiet\n!pip install pyTelegramBotAPI --quiet\n!pip install gymnasium[accept-rom-license] --quiet\n!pip install gymnasium[box2d] --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-17T00:34:47.234821Z","iopub.execute_input":"2023-03-17T00:34:47.235163Z","iopub.status.idle":"2023-03-17T00:38:14.146048Z","shell.execute_reply.started":"2023-03-17T00:34:47.235133Z","shell.execute_reply":"2023-03-17T00:38:14.144801Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras.layers import Dense, Conv2D, Input, Lambda\nimport numpy as np \n \nclass Actor:\n    def __init__(self, input_dims, action_dim, action_bound, std_bound, actor_lr): \n        self.input_dims = input_dims \n        self.action_dim = action_dim\n        self.action_bound = action_bound \n        self.std_bound = std_bound\n        self.actor_lr = actor_lr \n        self.actor_network = self.build_network() \n        \n    def build_network(self): \n        inputs = Input((self.input_dims, ))\n        x = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(inputs)\n        x = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(x)\n        \n        out_mu = Dense(self.action_dim, activation='tanh')(x)\n        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n        std_output = Dense(self.action_dim, activation='softplus')(x)\n        \n        model = tf.keras.Model(inputs=[inputs], outputs=[mu_output, std_output])\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.actor_lr))\n        return model\n\n    def get_action(self, state): \n        state = np.reshape(state, [1, self.input_dims])\n        mu, std = self.actor_network.predict(state, verbose=False)\n        mu, std = mu[0], std[0]\n        return np.random.normal(mu, std, size=self.action_dim)\n\n    def log_pdf(self, mu, std, action):\n        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n        var = std ** 2\n        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n            var * 2 * np.pi\n        )\n        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n    \n    \n    def compute_loss(self, mu, std, actions, advantages):\n        log_policy_pdf = self.log_pdf(mu, std, actions)\n        loss_policy = log_policy_pdf * advantages\n        return tf.reduce_sum(-loss_policy)\n    \n    \n    def train(self, states, actions, advantages): \n        \n        with tf.GradientTape() as tape: \n            mu, std = self.actor_network(states, training=True)\n            mu, std = mu[0], std[0]\n            loss = self.compute_loss(mu, std, actions, advantages)\n        \n        params = self.actor_network.trainable_variables\n        grads = tape.gradient(loss, params)\n        \n        self.actor_network.optimizer.apply_gradients(zip(grads, params))\n        return loss\n        \n    def save_models(self): \n        pass \n    \n    \n    def load_models(self):\n        pass\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:18:07.175264Z","iopub.execute_input":"2023-03-17T03:18:07.175660Z","iopub.status.idle":"2023-03-17T03:18:07.194357Z","shell.execute_reply.started":"2023-03-17T03:18:07.175627Z","shell.execute_reply":"2023-03-17T03:18:07.193176Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"class Critic:\n    def __init__(self, input_dims, action_dim, critic_lr): \n        self.input_dims = input_dims \n        self.action_dim = action_dim\n        self.critic_lr = critic_lr\n        self.critic_network = self.build_network() \n        \n    def build_network(self): \n        inputs = Input((self.input_dims, ))\n        x = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(inputs)\n        x = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(x)\n\n        output = Dense(1)(x)\n        \n        model = tf.keras.Model(inputs=[inputs], outputs=[output])\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.critic_lr))\n        return model\n    \n    def load_model(self): \n        self.model = tf.keras.models.load_model(self.fname + \"_actor_network\")\n        print(\"loaded the model\")\n        \n    def compute_loss(self, v_pred, td_targets):\n        mse = tf.keras.losses.MeanSquaredError()\n        return mse(td_targets, v_pred)\n\n    def save_model(self): \n        self.model.save(self.fname + \"_actor_network\") \n    \n    def train(self, states, td_targets): \n        with tf.GradientTape() as tape:\n            values = self.critic_network(states, training=True)\n            loss = self.compute_loss(values, td_targets)\n        \n        params = self.critic_network.trainable_variables\n        grads = tape.gradient(loss, params)\n        \n        self.critic_network.optimizer.apply_gradients(zip(grads, params))\n        return loss\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:18:07.288456Z","iopub.execute_input":"2023-03-17T03:18:07.288808Z","iopub.status.idle":"2023-03-17T03:18:07.302061Z","shell.execute_reply.started":"2023-03-17T03:18:07.288776Z","shell.execute_reply":"2023-03-17T03:18:07.300578Z"},"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"code","source":"class GlobalNetwork: \n    def __init__(self, input_dims, action_dims,  action_bound, std_bound, actor_lr, critic_lr): \n        self.actor_builder = Actor(input_dims, action_dims, action_bound, std_bound, actor_lr)\n        self.actor = self.actor_builder.actor_network\n        \n        self.critic_builder = Critic(input_dims, action_dims, critic_lr)\n        self.critic = self.critic_builder.critic_network\n        \n    def update_global_params(self, states, actions, advantages, td_targets):\n        actor_l = self.actor_builder.train(states, actions, advantages)\n        critic_l = self.critic_builder.train(states, td_targets)\n        return actor_l , critic_l\n        \n    def pull_global_params(self): \n        actor_params = self.actor.get_weights()\n        critic_params = self.critic.get_weights()\n        \n        return actor_params, critic_params        \n    \n    def save_models(self): \n        pass \n    \n    def load_models(self):\n        pass \n\nclass WorkerNetwork: \n    def __init__(self , input_dims, action_dims,  action_bound, std_bound, actor_lr, critic_lr): \n        self.actor_builder = Actor(input_dims, action_dims, action_bound, std_bound, actor_lr)        \n        self.critic_builder = Critic(input_dims, action_dims, critic_lr)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:18:07.583535Z","iopub.execute_input":"2023-03-17T03:18:07.583861Z","iopub.status.idle":"2023-03-17T03:18:07.594128Z","shell.execute_reply.started":"2023-03-17T03:18:07.583830Z","shell.execute_reply":"2023-03-17T03:18:07.592915Z"},"trusted":true},"execution_count":331,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import cpu_count\n\n# global_networks will be created in the A3C Agent\n# global_network = GlobalNetwork()\n\nCURR_EPISODE = 0\nclass A3CWorker: \n    def __init__(self, env, noe, action_bound, std_bound, gamma, \n                     update_interval, global_network, local_network, input_dims, out_dims): \n        self.env = env \n        self.noe = noe\n        self.gamma= gamma\n        self.update_interval = update_interval\n        self.action_bound = action_bound\n        self.std_bound = std_bound \n        self.input_dims = input_dims\n        \n        self.global_network = global_network\n        self.global_actor, self.global_critic = self.global_network.actor, self.global_network.critic\n        \n        self.worker_actor_builder, self.worker_critic_builder = local_network.actor_builder, local_network.critic_builder\n        \n        self.worker_actor, self.worker_critic = self.worker_actor_builder.actor_network, self.worker_critic_builder.critic_network\n        \n        actor_weights, critic_weights = global_network.pull_global_params()\n        self.worker_actor.set_weights(actor_weights)\n        self.worker_critic.set_weights(critic_weights)\n        \n        \n    def n_step_td_target(self, rewards, next_v_value, done):\n        td_targets = np.zeros_like(rewards)\n        cumulative = 0\n        if not done:\n            cumulative = next_v_value\n\n        for k in reversed(range(0, len(rewards))):\n            cumulative = self.gamma * cumulative + rewards[k]\n            td_targets[k] = cumulative\n        return td_targets\n    \n    def list_to_batch(self, list):\n        batch = list[0]\n        for elem in list[1:]:\n            batch = np.append(batch, elem, axis=0)\n        return batch\n    \n    def advatnage(self, td_targets, baselines):\n        return td_targets - baselines\n    \n    def learn(self): \n        \n        global CURR_EPISODE\n        \n        while CURR_EPISODE <= self.noe: \n            state = self.env.reset()\n            buffer_states, buffer_actions, buffer_rewards = [], [], []\n            done = False \n            episodic_reward = 0\n            \n            while not done:  \n                if type(state) == tuple: \n                    state = state[0]\n                    \n                action = self.worker_actor_builder.get_action(state)\n                action = np.clip(action, -self.action_bound, self.action_bound)\n                next_state_info = self.env.step(action)\n                    \n                next_state, reward_prob, terminated, truncated, _ = next_state_info \n                done = terminated or truncated \n                    \n                   \n                state = np.reshape(state, [1, self.input_dims])\n                action = np.reshape(action, [1, 1])\n                next_state = np.reshape(next_state, [1, self.input_dims])\n                reward_prob = np.reshape(reward_prob, [1, 1])\n                    \n                buffer_states.append(state)\n                buffer_actions.append(action)\n                buffer_rewards.append(reward_prob)\n\n                    \n                if len(buffer_states) > self.update_interval or done: \n                    states =  self.list_to_batch(buffer_states)\n                    actions = self.list_to_batch(buffer_actions)\n                    rewards = self.list_to_batch(buffer_rewards)\n                    \n                    next_v_value = self.worker_critic.predict(next_state, verbose=False)\n                    td_targets = self.n_step_td_target(rewards, next_v_value, done)\n                    advantages = td_targets - self.worker_critic.predict(states, verbose=False)\n                        \n                    actor_loss, critic_loss = self.global_network.update_global_params(states, actions, advantages, td_targets)\n                        \n                    actor_weights, critic_weights = self.global_network.pull_global_params()\n                    self.worker_actor.set_weights(actor_weights)\n                    self.worker_critic.set_weights(critic_weights)\n                        \n                    buffer_states, buffer_actions, buffer_rewards = [], [], []\n                \n                episodic_reward += reward_prob[0][0]\n                state  = next_state[0]\n            \n            print(f\"Episode: {CURR_EPISODE}, Reward: {episodic_reward}\")\n            CURR_EPISODE += 1 \n                ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:20:29.731523Z","iopub.execute_input":"2023-03-17T03:20:29.731906Z","iopub.status.idle":"2023-03-17T03:20:29.751390Z","shell.execute_reply.started":"2023-03-17T03:20:29.731872Z","shell.execute_reply":"2023-03-17T03:20:29.750094Z"},"trusted":true},"execution_count":340,"outputs":[]},{"cell_type":"code","source":"from threading import Thread \nfrom multiprocessing import cpu_count \n\nclass ActorCriticAgent:\n    def __init__(self, env, actor_lr, critic_lr, gamma, update_interval, noe, worker_count=cpu_count()):\n        self.input_dims = env.observation_space.shape[0]\n        self.out_dims = env.action_space.shape[0]\n        self.action_bound = env.action_space.high[0]\n        self.std_bound = [1e-2, 1.0]\n        self.worker_count = worker_count\n        self.noe = noe\n        self.update_interval = update_interval\n        self.env = env\n        self.gamma = gamma\n        \n        self.global_network = GlobalNetwork(self.input_dims, self.out_dims, \n                                                self.action_bound, self.std_bound, actor_lr, critic_lr)\n        self.worker_network = WorkerNetwork(self.input_dims, self.out_dims, \n                                                self.action_bound, self.std_bound, actor_lr, critic_lr)\n        \n        \n        \n    def learn(self): \n        workers = []\n        for _ in range(self.worker_count): \n            a3c_worker = A3CWorker(self.env, self.noe, self.action_bound, self.std_bound,\n                                        self.gamma, self.update_interval,  self.global_network,\n                                        self.worker_network, self.input_dims, self.out_dims\n                                    )\n            workers.append(Thread(target=a3c_worker.learn()))\n        \n        print(len(workers))\n        for worker in workers: \n            worker.start()\n        \n        for worker in workers: \n            worker.join()\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:20:31.759039Z","iopub.execute_input":"2023-03-17T03:20:31.759484Z","iopub.status.idle":"2023-03-17T03:20:31.777190Z","shell.execute_reply.started":"2023-03-17T03:20:31.759442Z","shell.execute_reply":"2023-03-17T03:20:31.776030Z"},"trusted":true},"execution_count":341,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym \n\nif __name__ == \"__main__\": \n    env = gym.make(\"MountainCarContinuous-v0\")\n    agent = ActorCriticAgent(env, 0.005, 0.005, 0.99, 30, 50)\n    agent.learn()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T03:22:44.949437Z","iopub.execute_input":"2023-03-17T03:22:44.950270Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Episode: 2, Reward: -71.76229691049484\nEpisode: 3, Reward: 39.27725910630954\nEpisode: 4, Reward: -80.58688811422674\nEpisode: 5, Reward: -80.45210806325507\nEpisode: 6, Reward: 26.283470737207907\nEpisode: 7, Reward: -75.83938066688273\nEpisode: 8, Reward: -81.64035857301405\nEpisode: 9, Reward: 49.221613840810136\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}